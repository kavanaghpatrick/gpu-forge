{"domain":"metal-compute","title":"proto1: Flash Attention baseline throughput on M4","content":"From-scratch Flash Attention kernel (Br=16, Bc=64, D=64, single head) achieves 0.16 TFLOPS at N=2048 on Apple M4. Kernel uses simdgroup_float8x8 for Q*K^T and P*V matmuls with online softmax (running max/sum). GPU time: ~6.7ms at N=2048. This is a first unoptimized implementation with one simdgroup (32 threads) per threadgroup, no shared memory prefetching, and no multi-head parallelism.","tags":["proto1","flash-attention","simdgroup-matrix","M4","tflops"],"confidence":0.85,"source":"attention-proto/proto1_flash benchmark (criterion, 50 samples, CV<1%)"}
{"domain":"gpu-perf","title":"proto1: Flash Attention TFLOPS scales with sequence length on M4","content":"Flash Attention TFLOPS scales roughly linearly with N in [256-2048] range on M4: N=256 → 0.03 TFLOPS (398us), N=512 → 0.07 TFLOPS (779us), N=1024 → 0.11 TFLOPS (2.4ms), N=2048 → 0.16 TFLOPS (6.7ms). FLOPs grow as O(N^2*D) while GPU time grows sub-quadratically, indicating better GPU utilization at larger workloads. The 5x throughput improvement from N=256 to N=2048 suggests the kernel is dispatch/launch-overhead-bound at small N and compute-bound at larger N.","tags":["proto1","flash-attention","scaling","M4","sequence-length"],"confidence":0.85,"source":"attention-proto/proto1_flash benchmark (criterion, 50 samples per N, CV<1%)"}
{"domain":"gpu-perf","title":"proto1: From-scratch Flash Attention achieves ~4-10% of MFA throughput on M4","content":"Metal-flash-attention (MFA) by Apple/philipturner achieves ~1790 GINSTRS (~1.8 TFLOPS equivalent) on M4 for optimized configurations. Our from-scratch Proto 1 kernel at 0.16 TFLOPS (N=2048, D=64) is approximately 4-10% of MFA performance. This gap is expected for a first implementation: (1) single simdgroup (32 threads) vs MFA's multi-simdgroup design, (2) no async copy or prefetching, (3) naive threadgroup memory layout, (4) no register-level optimizations. The 10-25x gap validates that significant optimization headroom exists and provides a concrete baseline for measuring improvement in later protos.","tags":["proto1","flash-attention","metal-flash-attention","mfa-comparison","M4"],"confidence":0.8,"source":"attention-proto/proto1_flash benchmark vs MFA published M4 numbers"}
{"domain":"msl-kernels","title":"proto1: 32KB threadgroup memory constraint validated for Br=16, Bc=64, D=64","content":"Flash Attention threadgroup memory layout at Br=16, Bc=64, D=64: Q_tile = Br*D*4 = 16*64*4 = 4096 bytes (4KB), K_chunk = Bc*D*4 = 64*64*4 = 16384 bytes (16KB), S_tile = Br*Bc*4 = 16*64*4 = 4096 bytes (4KB). Total = 24576 bytes (24KB), fits within Apple M4's 32KB per-threadgroup limit with 8KB headroom. Doubling Bc to 128 would require 32KB Q_tile(4KB) + K_chunk(32KB) + S_tile(8KB) = 44KB, exceeding the limit. Doubling Br to 32 would need Q_tile(8KB) + K_chunk(16KB) + S_tile(8KB) = 32KB, at the exact limit. This confirms Br=16, Bc=64 is a safe default for D=64.","tags":["proto1","flash-attention","threadgroup-memory","M4","tile-size"],"confidence":0.95,"source":"attention-proto/proto1_flash kernel analysis (deterministic calculation)"}
{"domain":"simd-wave","title":"proto1: simdgroup_float8x8 load/multiply_accumulate/store validated on M4","content":"simdgroup_float8x8 operations (simdgroup_load, simdgroup_multiply_accumulate, simdgroup_store) compile and produce correct results on Apple M4 with -std=metal3.1 and #include <metal_simdgroup_matrix>. The Flash Attention kernel uses 8x8 tiles for both Q*K^T and P*V matmuls. Correctness verified against FP64 CPU reference at N=256, D=64 with atol=5e-3, rtol=1e-2. The simdgroup_load with transpose=true flag enables direct K^T loading without explicit transpose. All 32 threads in a simdgroup participate cooperatively in each 8x8 matrix op.","tags":["proto1","simdgroup-matrix","simdgroup_float8x8","M4","metal3.1"],"confidence":0.95,"source":"attention-proto/proto1_flash correctness test (MTL_SHADER_VALIDATION=1)"}
{"domain":"gpu-perf","title":"proto1: GPU dispatch overhead dominates at small sequence lengths on M4","content":"At N=256 (FLOPS=16.8M), GPU kernel time is ~398us yielding only 0.03 TFLOPS (42 GFLOPS). At N=2048 (FLOPS=1.07B), time is ~6.7ms yielding 0.16 TFLOPS. The 8x N increase yields 64x more FLOPs but only 17x more time, indicating that at N=256 a significant fraction of time is non-compute overhead (command buffer submission, encoder setup, GPU clock ramp). Estimated dispatch overhead is ~100-200us based on extrapolation. For production attention kernels, batching multiple heads into a single dispatch (multi-head parallelism) is critical to amortize this overhead at small N.","tags":["proto1","dispatch-overhead","command-buffer","M4","small-workload"],"confidence":0.8,"source":"attention-proto/proto1_flash benchmark (extrapolated from N=256..2048 scaling)"}
{"domain":"metal-compute","title":"proto4: Function constant PSO compilation speed on M4","content":"Metal function constant PSO compilation takes 34-63us per variant on M4, enabling runtime specialization of 72+ attention variants in <5ms. Measured cold compile times: N=1 variant ~34us, N=10 ~34us/variant, N=50 ~37us/variant, N=100 ~43us/variant. Scaling is near-linear with slight per-variant increase at higher counts (cache pressure). 72 variants (full combinatorial: 3 HEAD_DIM x 4 BLOCK_R x 3 BLOCK_C x 2 VARIANT) compile in ~4.5ms total (~63us/variant). This is well under the 50ms/variant threshold for runtime dispatch viability.","tags":["proto4","function-constants","pso-compilation","M4"],"confidence":0.9,"source":"attention-proto/constant_overhead benchmark (criterion, cold_compile group)"}
{"domain":"metal-compute","title":"proto4: MTLBinaryArchive provides no speedup over cold compile on M4","content":"MTLBinaryArchive load time (4.7ms for 72 variants) equals cold compilation time on M4. Archives add 82ms creation overhead with zero runtime benefit. Archive creation is dominated by addComputePipelineFunctionsWithDescriptor_error calls (~82ms for 72 variants), not serialization. Archive load (~4.7ms) provides no speedup vs fresh cold compile (~4.5ms). The M4 Metal compiler is fast enough that pre-compiled binary archives are unnecessary for attention kernel variant counts up to at least 100. Binary archives may still benefit workloads with thousands of variants or slower GPU generations.","tags":["proto4","binary-archive","pso-compilation","M4"],"confidence":0.9,"source":"attention-proto/constant_overhead benchmark (criterion, binary_archive group)"}
{"domain":"gpu-perf","title":"proto4: PsoCache HashMap lookup is 350x faster than cold compile on M4","content":"PsoCache HashMap lookup at 178ns/variant is 350x faster than cold compilation (~63us/variant). For trait Attention<Q,K,V>, lazy compilation + cache is the recommended dispatch strategy. Total lookup time for 72 variants: ~12.8us (vs ~4.5ms cold compile). The HashMap key (PsoKey) serializes function constant values into a comparable type with negligible overhead. First access triggers cold compile (~63us), all subsequent accesses hit cache (~178ns). This validates the lazy-compile-and-cache pattern for runtime PSO management.","tags":["proto4","pso-cache","function-constants","dispatch-strategy"],"confidence":0.9,"source":"attention-proto/constant_overhead benchmark (criterion, pso_cache group)"}
{"domain":"gpu-centric-arch","title":"proto4: Function constants enable compile-time specialization for trait Attention","content":"Function constant compilation is fast enough (<63us/variant on M4) that trait Attention<Q,K,V> can use compile-time specialization via function constants rather than runtime dispatch via function stitching. This simplifies the architecture significantly: each (HEAD_DIM, BLOCK_R, BLOCK_C, VARIANT) tuple maps to a unique PSO compiled on first use and cached in PsoCache HashMap. No need for MTLLinkedFunctions, MTLVisibleFunctionTable, or binary archives. Recommended pattern: define trait variants as function constant combinations, lazy-compile PSOs on first dispatch, cache with PsoKey. Total startup cost for 72 variants: <5ms. Per-dispatch cache hit: ~178ns.","tags":["proto4","function-constants","architecture","trait-dispatch","M4"],"confidence":0.85,"source":"attention-proto/proto4 synthesis (cold_compile + binary_archive + pso_cache results)"}
{"domain":"msl-kernels","title":"proto2: MSL always_inline functions have zero overhead in flash attention inner loop on M4","content":"MSL __attribute__((always_inline)) functions in flash attention inner loop add 0.28% overhead vs monolithic kernel on M4 — within measurement noise. Compiler fully eliminates call overhead. Measured: monolithic 2.44ms vs always_inline 2.45ms at N=1024, D=64 (100 iterations, criterion). Per-call overhead: ~144ns across 48 function calls per dispatch (1024/64=16 KV blocks * 3 functions each). This validates factoring GPU kernels into inline helper functions for readability and maintainability with zero performance cost.","tags":["proto2","function-stitching","always_inline","msl","M4"],"confidence":0.9,"source":"attention-proto/function_stitch benchmark (criterion, N=1024, D=64, 100 iterations)"}
{"domain":"msl-kernels","title":"proto2: MSL noinline functions add 39% overhead in flash attention inner loop on M4","content":"MSL __attribute__((noinline)) functions in flash attention inner loop add 39.4% overhead on M4 (2.44ms -> 3.38ms at N=1024, D=64). Real function calls in GPU inner loops are prohibitively expensive. Per-call overhead: ~20us across 48 calls per dispatch (0.94ms total overhead / 48 calls). The overhead comes from function call setup/teardown, register spilling, and loss of cross-function optimization (e.g., simdgroup_matrix register reuse). This definitively shows that runtime indirect function dispatch (visible_function_table) would have similar or worse overhead in compute kernel inner loops.","tags":["proto2","function-stitching","noinline","msl","M4"],"confidence":0.9,"source":"attention-proto/function_stitch benchmark (criterion, N=1024, D=64, 100 iterations)"}
{"domain":"gpu-centric-arch","title":"proto2: Function constants are the clear dispatch strategy for trait Attention<Q,K,V>","content":"For trait Attention<Q,K,V>, use function constants (compile-time specialization, 34-63us per variant) rather than runtime function dispatch. Combined Proto 2+4 data: compile-time = 0% overhead + 63us compile; runtime noinline = 39% overhead. Function constants are the clear winner. The break-even point does not exist in any practical scenario: even if a kernel is dispatched only once, the 63us compile cost is negligible compared to the 0.94ms overhead of noinline function calls at N=1024. For the recommended lazy-compile-and-cache pattern (Proto 4), subsequent dispatches pay only ~178ns cache lookup vs 39% runtime overhead per dispatch.","tags":["proto2","proto4","function-constants","architecture","trait-dispatch","M4"],"confidence":0.95,"source":"attention-proto/proto2+proto4 synthesis (function_stitch + constant_overhead results)"}
{"domain":"metal-compute","title":"proto2: Metal function stitching not viable for compute kernel inner loops","content":"[[stitchable]] / visible_function_table dispatch in Metal compute kernel inner loops would have overhead comparable to or worse than noinline (39%+). Function stitching is designed for render pipeline composition, not compute kernel inner loops. The noinline benchmark (39% overhead at N=1024) represents a lower bound for function table dispatch, since visible_function_table adds additional indirection through a function pointer table. For compute kernels requiring runtime polymorphism, use function constants to compile specialized variants (Proto 4: 63us/variant, 0% runtime overhead) rather than function tables.","tags":["proto2","function-stitching","visible_function_table","stitchable","metal-compute","M4"],"confidence":0.8,"source":"attention-proto/proto2 noinline benchmark extrapolation (lower bound for function table overhead)"}
{"domain":"metal-compute","title":"proto3: PagedAttention V2 viable on M4 with 32KB threadgroup memory constraint","content":"PagedAttention V2 two-pass (partition + reduce) works correctly on M4 with 32KB threadgroup memory. Page_size=16 uses 13KB, page_size=32 uses 22KB. Max viable page_size=32 for D=64. The partition kernel stores Q_tile (4KB at BLOCK_R=16, D=64), one K page, one V page, and score buffer in threadgroup memory. The reduce kernel uses BLOCK_R*D threads (1024) to combine partitions via log-sum-exp. Both kernels fit comfortably within the 32KB limit at page_size<=32.","tags":["proto3","paged-attention","threadgroup-memory","M4","metal-compute"],"confidence":0.9,"source":"attention-proto/proto3_paged correctness test + threadgroup budget test"}
{"domain":"gpu-perf","title":"proto3: Page table indirection adds ~9% overhead vs contiguous KV cache on M4","content":"Page table indirection adds ~9% overhead vs contiguous KV cache at N=256-512, D=64, page_size=16 on M4. Measured: N=256 paged ~434us vs contiguous ~399us (+9%), N=512 paged ~855us vs contiguous ~782us (+9%). Acceptable for production KV cache management where fragmentation avoidance is critical for long-context inference. The overhead comes from page_table[logical_page] indirection per KV block load, which adds one extra memory access per page iteration.","tags":["proto3","paged-attention","page-table","overhead","M4"],"confidence":0.85,"source":"attention-proto/paged_attention benchmark (criterion, N=256/512, page_size=16)"}
{"domain":"msl-kernels","title":"proto3: M4 threadgroup memory budget map for PagedAttention at D=64 and D=128","content":"M4 threadgroup memory for PagedAttention at D=64: page_size=8 -> 8.5KB, page_size=16 -> 13KB, page_size=32 -> 22KB (max viable), page_size=64 -> 40KB (exceeds 32KB), page_size=128 -> 76KB. For D=128: max viable page_size=16 (22KB). Formula: Q_tile (BLOCK_R*D*4) + K_page (page_size*D*4) + V_page (page_size*D*4) + S_buf (BLOCK_R*page_size*4) with BLOCK_R=16. The 32KB constraint is the primary limiting factor for page size selection on Apple Silicon. Larger page sizes reduce page table overhead but exceed threadgroup memory limits.","tags":["proto3","paged-attention","threadgroup-memory","page-size","M4","msl-kernels"],"confidence":0.95,"source":"attention-proto/proto3 threadgroup budget test (deterministic calculation)"}
{"domain":"metal-compute","title":"proto3: PagedAttention V2 log-sum-exp reduction verified correct for single partition","content":"PagedAttention V2 log-sum-exp reduction kernel verified correct for single partition at N=64, D=64, page_size=16. The reduce kernel finds global max across partitions, rescales each partition's O and l by exp(m_p - m_global), sums, and normalizes by 1/l_total. For single partition this is a passthrough, but the mechanism is validated. Multi-partition reduce (needed for long contexts with >1 KV partition per query block) requires further validation at larger context lengths where multiple partitions are active.","tags":["proto3","paged-attention","log-sum-exp","reduce","M4"],"confidence":0.8,"source":"attention-proto/proto3_paged correctness test (MTL_SHADER_VALIDATION=1, atol=1e-3)"}
{"domain":"gpu-centric-arch","title":"proto3: PagedAttention page size recommendation for trait Attention<Q,K,V> KV cache","content":"For trait Attention<Q,K,V> KV cache management: use page_size=16 (D=64) or page_size=8 (D=128) to fit within 32KB threadgroup memory on Apple Silicon. Page table adds minimal overhead (~9%). V2 partitioned mode enables long-context support via multi-partition dispatch. Recommended configuration: page_size as function constant (Proto 4 validates <63us compile), lazy page allocation with reversed-order simulation for fragmentation testing, two-pass dispatch (partition + reduce) in single command buffer for implicit synchronization. The 32KB constraint is the binding limit — not compute overhead.","tags":["proto3","paged-attention","architecture","kv-cache","page-size","M4"],"confidence":0.85,"source":"attention-proto/proto3 synthesis (correctness + budget + benchmark results)"}
