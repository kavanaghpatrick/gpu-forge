{"domain":"metal-compute","title":"proto1: Flash Attention baseline throughput on M4","content":"From-scratch Flash Attention kernel (Br=16, Bc=64, D=64, single head) achieves 0.16 TFLOPS at N=2048 on Apple M4. Kernel uses simdgroup_float8x8 for Q*K^T and P*V matmuls with online softmax (running max/sum). GPU time: ~6.7ms at N=2048. This is a first unoptimized implementation with one simdgroup (32 threads) per threadgroup, no shared memory prefetching, and no multi-head parallelism.","tags":["proto1","flash-attention","simdgroup-matrix","M4","tflops"],"confidence":0.85,"source":"attention-proto/proto1_flash benchmark (criterion, 50 samples, CV<1%)"}
{"domain":"gpu-perf","title":"proto1: Flash Attention TFLOPS scales with sequence length on M4","content":"Flash Attention TFLOPS scales roughly linearly with N in [256-2048] range on M4: N=256 → 0.03 TFLOPS (398us), N=512 → 0.07 TFLOPS (779us), N=1024 → 0.11 TFLOPS (2.4ms), N=2048 → 0.16 TFLOPS (6.7ms). FLOPs grow as O(N^2*D) while GPU time grows sub-quadratically, indicating better GPU utilization at larger workloads. The 5x throughput improvement from N=256 to N=2048 suggests the kernel is dispatch/launch-overhead-bound at small N and compute-bound at larger N.","tags":["proto1","flash-attention","scaling","M4","sequence-length"],"confidence":0.85,"source":"attention-proto/proto1_flash benchmark (criterion, 50 samples per N, CV<1%)"}
{"domain":"gpu-perf","title":"proto1: From-scratch Flash Attention achieves ~4-10% of MFA throughput on M4","content":"Metal-flash-attention (MFA) by Apple/philipturner achieves ~1790 GINSTRS (~1.8 TFLOPS equivalent) on M4 for optimized configurations. Our from-scratch Proto 1 kernel at 0.16 TFLOPS (N=2048, D=64) is approximately 4-10% of MFA performance. This gap is expected for a first implementation: (1) single simdgroup (32 threads) vs MFA's multi-simdgroup design, (2) no async copy or prefetching, (3) naive threadgroup memory layout, (4) no register-level optimizations. The 10-25x gap validates that significant optimization headroom exists and provides a concrete baseline for measuring improvement in later protos.","tags":["proto1","flash-attention","metal-flash-attention","mfa-comparison","M4"],"confidence":0.8,"source":"attention-proto/proto1_flash benchmark vs MFA published M4 numbers"}
{"domain":"msl-kernels","title":"proto1: 32KB threadgroup memory constraint validated for Br=16, Bc=64, D=64","content":"Flash Attention threadgroup memory layout at Br=16, Bc=64, D=64: Q_tile = Br*D*4 = 16*64*4 = 4096 bytes (4KB), K_chunk = Bc*D*4 = 64*64*4 = 16384 bytes (16KB), S_tile = Br*Bc*4 = 16*64*4 = 4096 bytes (4KB). Total = 24576 bytes (24KB), fits within Apple M4's 32KB per-threadgroup limit with 8KB headroom. Doubling Bc to 128 would require 32KB Q_tile(4KB) + K_chunk(32KB) + S_tile(8KB) = 44KB, exceeding the limit. Doubling Br to 32 would need Q_tile(8KB) + K_chunk(16KB) + S_tile(8KB) = 32KB, at the exact limit. This confirms Br=16, Bc=64 is a safe default for D=64.","tags":["proto1","flash-attention","threadgroup-memory","M4","tile-size"],"confidence":0.95,"source":"attention-proto/proto1_flash kernel analysis (deterministic calculation)"}
{"domain":"simd-wave","title":"proto1: simdgroup_float8x8 load/multiply_accumulate/store validated on M4","content":"simdgroup_float8x8 operations (simdgroup_load, simdgroup_multiply_accumulate, simdgroup_store) compile and produce correct results on Apple M4 with -std=metal3.1 and #include <metal_simdgroup_matrix>. The Flash Attention kernel uses 8x8 tiles for both Q*K^T and P*V matmuls. Correctness verified against FP64 CPU reference at N=256, D=64 with atol=5e-3, rtol=1e-2. The simdgroup_load with transpose=true flag enables direct K^T loading without explicit transpose. All 32 threads in a simdgroup participate cooperatively in each 8x8 matrix op.","tags":["proto1","simdgroup-matrix","simdgroup_float8x8","M4","metal3.1"],"confidence":0.95,"source":"attention-proto/proto1_flash correctness test (MTL_SHADER_VALIDATION=1)"}
{"domain":"gpu-perf","title":"proto1: GPU dispatch overhead dominates at small sequence lengths on M4","content":"At N=256 (FLOPS=16.8M), GPU kernel time is ~398us yielding only 0.03 TFLOPS (42 GFLOPS). At N=2048 (FLOPS=1.07B), time is ~6.7ms yielding 0.16 TFLOPS. The 8x N increase yields 64x more FLOPs but only 17x more time, indicating that at N=256 a significant fraction of time is non-compute overhead (command buffer submission, encoder setup, GPU clock ramp). Estimated dispatch overhead is ~100-200us based on extrapolation. For production attention kernels, batching multiple heads into a single dispatch (multi-head parallelism) is critical to amortize this overhead at small N.","tags":["proto1","dispatch-overhead","command-buffer","M4","small-workload"],"confidence":0.8,"source":"attention-proto/proto1_flash benchmark (extrapolated from N=256..2048 scaling)"}
