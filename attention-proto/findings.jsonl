{"domain":"metal-compute","title":"proto1: Flash Attention baseline throughput on M4","content":"From-scratch Flash Attention kernel (Br=16, Bc=64, D=64, single head) achieves 0.16 TFLOPS at N=2048 on Apple M4. Kernel uses simdgroup_float8x8 for Q*K^T and P*V matmuls with online softmax (running max/sum). GPU time: ~6.7ms at N=2048. This is a first unoptimized implementation with one simdgroup (32 threads) per threadgroup, no shared memory prefetching, and no multi-head parallelism.","tags":["proto1","flash-attention","simdgroup-matrix","M4","tflops"],"confidence":0.85,"source":"attention-proto/proto1_flash benchmark (criterion, 50 samples, CV<1%)"}
{"domain":"gpu-perf","title":"proto1: Flash Attention TFLOPS scales with sequence length on M4","content":"Flash Attention TFLOPS scales roughly linearly with N in [256-2048] range on M4: N=256 → 0.03 TFLOPS (398us), N=512 → 0.07 TFLOPS (779us), N=1024 → 0.11 TFLOPS (2.4ms), N=2048 → 0.16 TFLOPS (6.7ms). FLOPs grow as O(N^2*D) while GPU time grows sub-quadratically, indicating better GPU utilization at larger workloads. The 5x throughput improvement from N=256 to N=2048 suggests the kernel is dispatch/launch-overhead-bound at small N and compute-bound at larger N.","tags":["proto1","flash-attention","scaling","M4","sequence-length"],"confidence":0.85,"source":"attention-proto/proto1_flash benchmark (criterion, 50 samples per N, CV<1%)"}
{"domain":"gpu-perf","title":"proto1: From-scratch Flash Attention achieves ~4-10% of MFA throughput on M4","content":"Metal-flash-attention (MFA) by Apple/philipturner achieves ~1790 GINSTRS (~1.8 TFLOPS equivalent) on M4 for optimized configurations. Our from-scratch Proto 1 kernel at 0.16 TFLOPS (N=2048, D=64) is approximately 4-10% of MFA performance. This gap is expected for a first implementation: (1) single simdgroup (32 threads) vs MFA's multi-simdgroup design, (2) no async copy or prefetching, (3) naive threadgroup memory layout, (4) no register-level optimizations. The 10-25x gap validates that significant optimization headroom exists and provides a concrete baseline for measuring improvement in later protos.","tags":["proto1","flash-attention","metal-flash-attention","mfa-comparison","M4"],"confidence":0.8,"source":"attention-proto/proto1_flash benchmark vs MFA published M4 numbers"}
{"domain":"msl-kernels","title":"proto1: 32KB threadgroup memory constraint validated for Br=16, Bc=64, D=64","content":"Flash Attention threadgroup memory layout at Br=16, Bc=64, D=64: Q_tile = Br*D*4 = 16*64*4 = 4096 bytes (4KB), K_chunk = Bc*D*4 = 64*64*4 = 16384 bytes (16KB), S_tile = Br*Bc*4 = 16*64*4 = 4096 bytes (4KB). Total = 24576 bytes (24KB), fits within Apple M4's 32KB per-threadgroup limit with 8KB headroom. Doubling Bc to 128 would require 32KB Q_tile(4KB) + K_chunk(32KB) + S_tile(8KB) = 44KB, exceeding the limit. Doubling Br to 32 would need Q_tile(8KB) + K_chunk(16KB) + S_tile(8KB) = 32KB, at the exact limit. This confirms Br=16, Bc=64 is a safe default for D=64.","tags":["proto1","flash-attention","threadgroup-memory","M4","tile-size"],"confidence":0.95,"source":"attention-proto/proto1_flash kernel analysis (deterministic calculation)"}
{"domain":"simd-wave","title":"proto1: simdgroup_float8x8 load/multiply_accumulate/store validated on M4","content":"simdgroup_float8x8 operations (simdgroup_load, simdgroup_multiply_accumulate, simdgroup_store) compile and produce correct results on Apple M4 with -std=metal3.1 and #include <metal_simdgroup_matrix>. The Flash Attention kernel uses 8x8 tiles for both Q*K^T and P*V matmuls. Correctness verified against FP64 CPU reference at N=256, D=64 with atol=5e-3, rtol=1e-2. The simdgroup_load with transpose=true flag enables direct K^T loading without explicit transpose. All 32 threads in a simdgroup participate cooperatively in each 8x8 matrix op.","tags":["proto1","simdgroup-matrix","simdgroup_float8x8","M4","metal3.1"],"confidence":0.95,"source":"attention-proto/proto1_flash correctness test (MTL_SHADER_VALIDATION=1)"}
{"domain":"gpu-perf","title":"proto1: GPU dispatch overhead dominates at small sequence lengths on M4","content":"At N=256 (FLOPS=16.8M), GPU kernel time is ~398us yielding only 0.03 TFLOPS (42 GFLOPS). At N=2048 (FLOPS=1.07B), time is ~6.7ms yielding 0.16 TFLOPS. The 8x N increase yields 64x more FLOPs but only 17x more time, indicating that at N=256 a significant fraction of time is non-compute overhead (command buffer submission, encoder setup, GPU clock ramp). Estimated dispatch overhead is ~100-200us based on extrapolation. For production attention kernels, batching multiple heads into a single dispatch (multi-head parallelism) is critical to amortize this overhead at small N.","tags":["proto1","dispatch-overhead","command-buffer","M4","small-workload"],"confidence":0.8,"source":"attention-proto/proto1_flash benchmark (extrapolated from N=256..2048 scaling)"}
{"domain":"metal-compute","title":"proto4: Function constant PSO compilation speed on M4","content":"Metal function constant PSO compilation takes 34-63us per variant on M4, enabling runtime specialization of 72+ attention variants in <5ms. Measured cold compile times: N=1 variant ~34us, N=10 ~34us/variant, N=50 ~37us/variant, N=100 ~43us/variant. Scaling is near-linear with slight per-variant increase at higher counts (cache pressure). 72 variants (full combinatorial: 3 HEAD_DIM x 4 BLOCK_R x 3 BLOCK_C x 2 VARIANT) compile in ~4.5ms total (~63us/variant). This is well under the 50ms/variant threshold for runtime dispatch viability.","tags":["proto4","function-constants","pso-compilation","M4"],"confidence":0.9,"source":"attention-proto/constant_overhead benchmark (criterion, cold_compile group)"}
{"domain":"metal-compute","title":"proto4: MTLBinaryArchive provides no speedup over cold compile on M4","content":"MTLBinaryArchive load time (4.7ms for 72 variants) equals cold compilation time on M4. Archives add 82ms creation overhead with zero runtime benefit. Archive creation is dominated by addComputePipelineFunctionsWithDescriptor_error calls (~82ms for 72 variants), not serialization. Archive load (~4.7ms) provides no speedup vs fresh cold compile (~4.5ms). The M4 Metal compiler is fast enough that pre-compiled binary archives are unnecessary for attention kernel variant counts up to at least 100. Binary archives may still benefit workloads with thousands of variants or slower GPU generations.","tags":["proto4","binary-archive","pso-compilation","M4"],"confidence":0.9,"source":"attention-proto/constant_overhead benchmark (criterion, binary_archive group)"}
{"domain":"gpu-perf","title":"proto4: PsoCache HashMap lookup is 350x faster than cold compile on M4","content":"PsoCache HashMap lookup at 178ns/variant is 350x faster than cold compilation (~63us/variant). For trait Attention<Q,K,V>, lazy compilation + cache is the recommended dispatch strategy. Total lookup time for 72 variants: ~12.8us (vs ~4.5ms cold compile). The HashMap key (PsoKey) serializes function constant values into a comparable type with negligible overhead. First access triggers cold compile (~63us), all subsequent accesses hit cache (~178ns). This validates the lazy-compile-and-cache pattern for runtime PSO management.","tags":["proto4","pso-cache","function-constants","dispatch-strategy"],"confidence":0.9,"source":"attention-proto/constant_overhead benchmark (criterion, pso_cache group)"}
{"domain":"gpu-centric-arch","title":"proto4: Function constants enable compile-time specialization for trait Attention","content":"Function constant compilation is fast enough (<63us/variant on M4) that trait Attention<Q,K,V> can use compile-time specialization via function constants rather than runtime dispatch via function stitching. This simplifies the architecture significantly: each (HEAD_DIM, BLOCK_R, BLOCK_C, VARIANT) tuple maps to a unique PSO compiled on first use and cached in PsoCache HashMap. No need for MTLLinkedFunctions, MTLVisibleFunctionTable, or binary archives. Recommended pattern: define trait variants as function constant combinations, lazy-compile PSOs on first dispatch, cache with PsoKey. Total startup cost for 72 variants: <5ms. Per-dispatch cache hit: ~178ns.","tags":["proto4","function-constants","architecture","trait-dispatch","M4"],"confidence":0.85,"source":"attention-proto/proto4 synthesis (cold_compile + binary_archive + pso_cache results)"}
{"domain":"msl-kernels","title":"proto2: MSL always_inline functions have zero overhead in flash attention inner loop on M4","content":"MSL __attribute__((always_inline)) functions in flash attention inner loop add 0.28% overhead vs monolithic kernel on M4 — within measurement noise. Compiler fully eliminates call overhead. Measured: monolithic 2.44ms vs always_inline 2.45ms at N=1024, D=64 (100 iterations, criterion). Per-call overhead: ~144ns across 48 function calls per dispatch (1024/64=16 KV blocks * 3 functions each). This validates factoring GPU kernels into inline helper functions for readability and maintainability with zero performance cost.","tags":["proto2","function-stitching","always_inline","msl","M4"],"confidence":0.9,"source":"attention-proto/function_stitch benchmark (criterion, N=1024, D=64, 100 iterations)"}
{"domain":"msl-kernels","title":"proto2: MSL noinline functions add 39% overhead in flash attention inner loop on M4","content":"MSL __attribute__((noinline)) functions in flash attention inner loop add 39.4% overhead on M4 (2.44ms -> 3.38ms at N=1024, D=64). Real function calls in GPU inner loops are prohibitively expensive. Per-call overhead: ~20us across 48 calls per dispatch (0.94ms total overhead / 48 calls). The overhead comes from function call setup/teardown, register spilling, and loss of cross-function optimization (e.g., simdgroup_matrix register reuse). This definitively shows that runtime indirect function dispatch (visible_function_table) would have similar or worse overhead in compute kernel inner loops.","tags":["proto2","function-stitching","noinline","msl","M4"],"confidence":0.9,"source":"attention-proto/function_stitch benchmark (criterion, N=1024, D=64, 100 iterations)"}
{"domain":"gpu-centric-arch","title":"proto2: Function constants are the clear dispatch strategy for trait Attention<Q,K,V>","content":"For trait Attention<Q,K,V>, use function constants (compile-time specialization, 34-63us per variant) rather than runtime function dispatch. Combined Proto 2+4 data: compile-time = 0% overhead + 63us compile; runtime noinline = 39% overhead. Function constants are the clear winner. The break-even point does not exist in any practical scenario: even if a kernel is dispatched only once, the 63us compile cost is negligible compared to the 0.94ms overhead of noinline function calls at N=1024. For the recommended lazy-compile-and-cache pattern (Proto 4), subsequent dispatches pay only ~178ns cache lookup vs 39% runtime overhead per dispatch.","tags":["proto2","proto4","function-constants","architecture","trait-dispatch","M4"],"confidence":0.95,"source":"attention-proto/proto2+proto4 synthesis (function_stitch + constant_overhead results)"}
{"domain":"metal-compute","title":"proto2: Metal function stitching not viable for compute kernel inner loops","content":"[[stitchable]] / visible_function_table dispatch in Metal compute kernel inner loops would have overhead comparable to or worse than noinline (39%+). Function stitching is designed for render pipeline composition, not compute kernel inner loops. The noinline benchmark (39% overhead at N=1024) represents a lower bound for function table dispatch, since visible_function_table adds additional indirection through a function pointer table. For compute kernels requiring runtime polymorphism, use function constants to compile specialized variants (Proto 4: 63us/variant, 0% runtime overhead) rather than function tables.","tags":["proto2","function-stitching","visible_function_table","stitchable","metal-compute","M4"],"confidence":0.8,"source":"attention-proto/proto2 noinline benchmark extrapolation (lower bound for function table overhead)"}
{"domain":"metal-compute","title":"proto3: PagedAttention V2 viable on M4 with 32KB threadgroup memory constraint","content":"PagedAttention V2 two-pass (partition + reduce) works correctly on M4 with 32KB threadgroup memory. Page_size=16 uses 13KB, page_size=32 uses 22KB. Max viable page_size=32 for D=64. The partition kernel stores Q_tile (4KB at BLOCK_R=16, D=64), one K page, one V page, and score buffer in threadgroup memory. The reduce kernel uses BLOCK_R*D threads (1024) to combine partitions via log-sum-exp. Both kernels fit comfortably within the 32KB limit at page_size<=32.","tags":["proto3","paged-attention","threadgroup-memory","M4","metal-compute"],"confidence":0.9,"source":"attention-proto/proto3_paged correctness test + threadgroup budget test"}
{"domain":"gpu-perf","title":"proto3: Page table indirection adds ~9% overhead vs contiguous KV cache on M4","content":"Page table indirection adds ~9% overhead vs contiguous KV cache at N=256-512, D=64, page_size=16 on M4. Measured: N=256 paged ~434us vs contiguous ~399us (+9%), N=512 paged ~855us vs contiguous ~782us (+9%). Acceptable for production KV cache management where fragmentation avoidance is critical for long-context inference. The overhead comes from page_table[logical_page] indirection per KV block load, which adds one extra memory access per page iteration.","tags":["proto3","paged-attention","page-table","overhead","M4"],"confidence":0.85,"source":"attention-proto/paged_attention benchmark (criterion, N=256/512, page_size=16)"}
{"domain":"msl-kernels","title":"proto3: M4 threadgroup memory budget map for PagedAttention at D=64 and D=128","content":"M4 threadgroup memory for PagedAttention at D=64: page_size=8 -> 8.5KB, page_size=16 -> 13KB, page_size=32 -> 22KB (max viable), page_size=64 -> 40KB (exceeds 32KB), page_size=128 -> 76KB. For D=128: max viable page_size=16 (22KB). Formula: Q_tile (BLOCK_R*D*4) + K_page (page_size*D*4) + V_page (page_size*D*4) + S_buf (BLOCK_R*page_size*4) with BLOCK_R=16. The 32KB constraint is the primary limiting factor for page size selection on Apple Silicon. Larger page sizes reduce page table overhead but exceed threadgroup memory limits.","tags":["proto3","paged-attention","threadgroup-memory","page-size","M4","msl-kernels"],"confidence":0.95,"source":"attention-proto/proto3 threadgroup budget test (deterministic calculation)"}
{"domain":"metal-compute","title":"proto3: PagedAttention V2 log-sum-exp reduction verified correct for single partition","content":"PagedAttention V2 log-sum-exp reduction kernel verified correct for single partition at N=64, D=64, page_size=16. The reduce kernel finds global max across partitions, rescales each partition's O and l by exp(m_p - m_global), sums, and normalizes by 1/l_total. For single partition this is a passthrough, but the mechanism is validated. Multi-partition reduce (needed for long contexts with >1 KV partition per query block) requires further validation at larger context lengths where multiple partitions are active.","tags":["proto3","paged-attention","log-sum-exp","reduce","M4"],"confidence":0.8,"source":"attention-proto/proto3_paged correctness test (MTL_SHADER_VALIDATION=1, atol=1e-3)"}
{"domain":"gpu-centric-arch","title":"proto3: PagedAttention page size recommendation for trait Attention<Q,K,V> KV cache","content":"For trait Attention<Q,K,V> KV cache management: use page_size=16 (D=64) or page_size=8 (D=128) to fit within 32KB threadgroup memory on Apple Silicon. Page table adds minimal overhead (~9%). V2 partitioned mode enables long-context support via multi-partition dispatch. Recommended configuration: page_size as function constant (Proto 4 validates <63us compile), lazy page allocation with reversed-order simulation for fragmentation testing, two-pass dispatch (partition + reduce) in single command buffer for implicit synchronization. The 32KB constraint is the binding limit — not compute overhead.","tags":["proto3","paged-attention","architecture","kv-cache","page-size","M4"],"confidence":0.85,"source":"attention-proto/proto3 synthesis (correctness + budget + benchmark results)"}
{"domain":"gpu-perf","title":"proto6: FLA chunk_h+chunk_o GPU kernel time near-constant ~35us on M4","content":"FLA chunk_h+chunk_o GPU kernel time is ~35us on M4 for N=256-1024, D=64, chunk=32. O(N*D^2) complexity confirmed — kernel time scales linearly with N, not quadratically. At D=64 the D^2 term dominates, making kernel time nearly constant across tested sequence lengths. GPU TFLOPS: 0.011 (N=256) to 0.040 (N=1024) using 4*N*D^2 FLOPs formula.","tags":["proto6","linear-attention","fla","gpu-kernel-time","M4"],"confidence":0.9,"source":"attention-proto/linear_attention benchmark (criterion, GPU timing, N=256/512/1024)"}
{"domain":"gpu-perf","title":"proto6: Linear attention beats softmax flash attention at all tested seq_lens on M4","content":"Linear attention (chunk-based, D=64) is faster than flash attention at all tested seq_lens on M4: 0.45x at N=256, 0.26x at N=512, 0.13x at N=1024. Crossover point is below N=256. Wall-clock: linear ~347-417us (nearly constant, dominated by CPU prefix sum) vs flash 821us-3164us (quadratic growth). Even with CPU-side prefix sum overhead, O(N) linear scaling decisively beats O(N^2) softmax at all tested lengths for D=64.","tags":["proto6","linear-attention","flash-attention","crossover","M4"],"confidence":0.85,"source":"attention-proto/linear_attention benchmark (criterion, wall-clock, N=256/512/1024)"}
{"domain":"metal-compute","title":"proto6: CPU prefix sum dominates linear attention wall-clock time on M4","content":"CPU-side prefix sum of D x D hidden state matrices takes 300-380us, dominating total wall-clock time. Includes buffer readback (waitUntilCompleted + read_buffer_slice), prefix sum computation over num_chunks D x D matrices, and buffer re-upload (alloc_buffer_with_data). A GPU prefix sum kernel would reduce this to <50us, further improving linear attention throughput. With GPU prefix sum, total linear attention time would be ~35us (kernel only) vs flash's hundreds-to-thousands us.","tags":["proto6","linear-attention","prefix-sum","cpu-bottleneck","metal-compute"],"confidence":0.8,"source":"attention-proto/linear_attention benchmark (wall-clock vs GPU timing comparison)"}
{"domain":"msl-kernels","title":"proto6: FLA chunk_size=32 at D=64 uses 16KB threadgroup memory on M4","content":"FLA chunk_size=32 at D=64 uses 16KB threadgroup memory (K_chunk 8KB + V_chunk 8KB). chunk_size=64 would use 32KB (at limit). Recommend chunk_size=32 for D=64, chunk_size=16 for D=128. chunk_o kernel uses 24KB (Q_chunk 8KB + H_tile 16KB). Both kernels fit within 32KB Apple Silicon threadgroup memory limit. Function constant CHUNK_SIZE (index 4) enables runtime selection of optimal chunk size per head dimension.","tags":["proto6","linear-attention","chunk-size","threadgroup-memory","msl-kernels"],"confidence":0.9,"source":"attention-proto/proto6_fla kernel analysis (deterministic calculation)"}
{"domain":"gpu-centric-arch","title":"proto6: Chunk-based linear attention viable as Metal compute alternative to softmax","content":"Chunk-based linear attention ports successfully from Triton to Metal. The 2-kernel approach (chunk_h + chunk_o) maps naturally to Metal compute with simdgroup-compatible tile sizes. Viable as alternative attention mechanism in trait hierarchy. Key advantages: O(N*D^2) vs O(N^2*D) scaling, near-constant GPU kernel time, decisive win at N>=256 for D=64. Key limitation: CPU prefix sum adds ~300us overhead (solvable with GPU scan kernel). Recommended as long-context attention backend for trait Attention<Q,K,V> when N >= 256 and D <= 128.","tags":["proto6","linear-attention","fla","architecture","trait-dispatch","metal-compute"],"confidence":0.85,"source":"attention-proto/proto6 synthesis (correctness + benchmark + crossover results)"}
{"domain":"gpu-perf","title":"proto7: RoPE standalone kernel overhead ~10us per head on M4 — negligible vs attention","content":"RoPE apply_rope kernel takes ~9.7us for one head at N=2048, D=64 on M4. For 32-head attention taking ~204ms total GPU time, RoPE adds <0.01% overhead per head (~0.3ms total for all heads). RoPE uses a 2D grid (seq_len, D/2) with cos/sin rotation per (token, dim_pair). Expected 2-5% overhead confirmed to be well below 1%. Can be applied as a pre-processing step before attention without measurable impact on end-to-end latency.","tags":["proto7","rope","rotary-position-embedding","gpu-perf","M4"],"confidence":0.9,"source":"attention-proto/variant_overhead benchmark (criterion, GPU timing, 32 heads, N=2048, D=64)"}
{"domain":"gpu-perf","title":"proto7: ALiBi fused into flash attention via function constant adds zero measurable overhead on M4","content":"ALiBi bias fused into flash_attention.metal via ALIBI_ENABLED function_constant(4) takes ~201ms vs base ~204ms for 32 heads at N=2048, D=64 — within noise (-2.3%). Metal compiler dead-code eliminates the ALiBi branch when ALIBI_ENABLED=false, so the disabled path has zero cost. When enabled, the ALiBi math (one multiply + one add per score element: bias = -slope * |pos_q - pos_k|) is negligible relative to simdgroup_matrix attention compute. Confirms <1% overhead target. Function constant specialization is the correct strategy for optional attention biases.","tags":["proto7","alibi","function-constant","zero-overhead","M4"],"confidence":0.92,"source":"attention-proto/variant_overhead benchmark (criterion, GPU timing, 32 heads, N=2048, D=64)"}
{"domain":"gpu-perf","title":"proto7: GQA head remap kernel overhead <0.1% of attention compute on M4","content":"GQA gqa_remap kernel expands KV heads to Q head count via pure memory copy. Measured on M4 at N=2048, D=64, 32 Q heads: group_size=1 (32 KV heads, full copy) ~184us = 0.1%, group_size=2 (16 KV heads) ~112us, group_size=4 (8 KV heads) ~80us, group_size=8 (4 KV heads) ~73us. All well below 1% of base flash attention ~204ms. Cost scales with output size (num_heads * seq_len * head_dim elements), not input KV head count. Exact match (atol=1e-6) with CPU reference — pure copy has no numerical error. GQA remap can be applied as a pre-processing step with negligible overhead.","tags":["proto7","gqa","grouped-query-attention","head-remap","gpu-perf","M4"],"confidence":0.9,"source":"attention-proto/variant_overhead benchmark (criterion, GPU timing, group_size=1/2/4/8)"}
{"domain":"metal-compute","title":"proto7: All attention variants (RoPE, ALiBi, GQA) amenable to function constant specialization on M4","content":"All three Proto 7 attention variants work with Metal function constants for zero-overhead dispatch. ALiBi: ALIBI_ENABLED bool function_constant(4) enables compiler dead-code elimination — confirmed zero overhead when disabled and negligible overhead when enabled. RoPE: standalone kernel with no function constants needed (always applied identically), ~10us per head. GQA: uses AttentionParams.group_size at runtime (pure memory copy, no branching to specialize). For trait Attention<Q,K,V>, the recommended dispatch strategy is: (1) function constants for attention kernel variants (ALiBi, causal mask, etc.), (2) standalone pre/post-processing kernels for RoPE and GQA remap. Combined with Proto 4 findings (~63us cold compile, ~178ns cache hit), variant selection adds <1ms total overhead to any attention configuration.","tags":["proto7","function-constant","trait-attention","dispatch-strategy","metal-compute"],"confidence":0.88,"source":"attention-proto/proto7_variants analysis (combined benchmark + correctness data)"}
{"domain":"msl-kernels","title":"proto7: Attention variant correctness tolerances — RoPE 1e-4, ALiBi 5e-3, GQA 1e-6 on M4","content":"GPU vs CPU FP64 reference correctness tolerances on M4: RoPE matches at atol=1e-4 (tight — element-wise trig ops have good FP32 agreement). ALiBi matches at atol=5e-3 (wider — accumulated softmax error from FP32 attention compute, same as base flash attention). GQA remap matches at atol=1e-6 (exact — pure memory copy with no arithmetic). These tolerances establish baselines for regression testing in trait Attention<Q,K,V> implementations. The ALiBi tolerance is dominated by flash attention FP32 softmax accumulation, not the ALiBi bias computation itself.","tags":["proto7","correctness","numerical-tolerance","rope","alibi","gqa"],"confidence":0.92,"source":"attention-proto/proto7_variants correctness tests (GPU vs CPU FP64 reference)"}
{"domain":"cubecl-ecosystem","title":"proto5: CubeCL-generated MSL produces 4.3x fewer AIR instructions than hand-written FlashAttention kernel","content":"Assembly comparison via xcrun metal -S -std=metal3.1 -O2: hand-written Proto 1 flash_attention.metal compiles to 528 AIR lines (383 code lines) while a CubeCL-equivalent naive matmul compiles to 122 AIR lines (50 code lines). The hand-written kernel is ~7.7x more code reflecting algorithmic complexity: full FlashAttention-2 with tiled matmul, online softmax, fused O accumulation, simdgroup_matrix cooperative ops, threadgroup memory tiling, and function constant specialization. CubeCL produces a simple scalar fmul+fadd loop with no GPU-specific optimizations. The instruction ratio reflects capability gap, not codegen inefficiency — CubeCL's MSL codegen is architecturally sound but the #[cube] API abstraction prevents expressing GPU-specific patterns.","tags":["proto5","cubecl","air-assembly","instruction-count","msl-quality"],"confidence":0.95,"source":"attention-proto/proto5_cubecl analysis (xcrun metal -S, hand-written 528 vs CubeCL 122 AIR lines)"}
{"domain":"cubecl-ecosystem","title":"proto5: CubeCL has internal simdgroup_matrix (WMMA) support but it is inaccessible through user #[cube] kernels","content":"CubeCL's cubecl-cpp MslDialect implements DialectWmmaCompiler with full simdgroup_float8x8 codegen: make_filled_simdgroup_matrix, simdgroup_load, simdgroup_multiply_accumulate, simdgroup_store. MetalArchitecture::is_wmma_capable() returns true. However, this WMMA path is only accessible through CubeCL's internal cubecl-linalg matmul algorithms, not user-facing #[cube] Array<f32> kernels. User kernels produce scalar code with 0 simdgroup_matrix ops, 0 threadgroup memory accesses, and 0 function constants. The hand-written Proto 1 kernel uses 9 simdgroup_matrix intrinsics, 24 threadgroup accesses, 4 function constants, and 7 barriers — none of which can be expressed in #[cube] syntax. For trait Attention<Q,K,V>, simdgroup_matrix is essential for competitive throughput.","tags":["proto5","cubecl","simdgroup-matrix","wmma","limitation"],"confidence":0.95,"source":"attention-proto/proto5_cubecl analysis (CubeCL source inspection + AIR assembly diff)"}
{"domain":"gpu-perf","title":"proto5: CubeCL scalar kernel achieves 58-70% of hand-written MSL throughput on M4","content":"Side-by-side benchmark (criterion, 20 samples, D=64) comparing CubeCL naive Q*K^T matmul vs hand-written Proto 1 full flash attention, each measured against their own FLOP count (2*N^2*D for matmul, 4*N^2*D for full attention): N=256: CubeCL 0.017 TFLOPS vs hand-written 0.024 TFLOPS (ratio 0.70x). N=512: CubeCL 0.035 vs 0.061 TFLOPS (ratio 0.58x). N=1024: CubeCL 0.063 vs 0.103 TFLOPS (ratio 0.61x). The 30-42% throughput gap is caused by: (1) no simdgroup_matrix cooperative matmul, (2) no threadgroup memory tiling (no data reuse), (3) wgpu abstraction overhead, (4) scalar per-thread computation vs cooperative 32-thread simdgroup ops. Wall-clock CubeCL is faster (0.71-0.86x of hand-written time) because it computes only Q*K^T (~1/4 of full attention FLOPs).","tags":["proto5","cubecl","tflops","benchmark","gpu-perf","M4"],"confidence":0.9,"source":"attention-proto/cubecl_comparison benchmark (criterion, 20 samples, D=64, N=256/512/1024)"}
{"domain":"cubecl-ecosystem","title":"proto5: CubeCL not viable for high-performance attention kernels — hand-written MSL required","content":"CubeCL 0.9 evaluation for trait Attention<Q,K,V> on Apple M4 concludes hand-written MSL is the only viable path for competitive performance. Three blocking limitations: (1) No simdgroup_matrix access through #[cube] user API — internal WMMA support exists but is inaccessible, resulting in scalar kernels with 0 cooperative matrix ops. (2) No function constant support through wgpu — all parameters are runtime buffer values, preventing compile-time specialization and dead-code elimination (Proto 4 proved function constants have 0% runtime overhead vs 39% for runtime dispatch). (3) No threadgroup memory control — CubeCL's dynamic shared memory uses untyped uchar[] with reinterpret_cast, losing compiler optimization opportunities vs static typed arrays. CubeCL is viable for simple, non-performance-critical compute kernels (correctness validated at 2.4e-7 max diff vs FP64 reference). For attention: hand-written MSL with simdgroup_matrix, function constants, and explicit tiling delivers 1.5-1.7x better throughput.","tags":["proto5","cubecl","recommendation","attention","viability"],"confidence":0.95,"source":"attention-proto/proto5_cubecl (synthesis of assembly analysis, benchmark, and API inspection)"}
{"domain":"cubecl-ecosystem","title":"proto5: CubeCL adds ~350 crate dependencies via wgpu stack — heavyweight for Metal-only targets","content":"CubeCL 0.9 Metal support requires cubecl = { features = [\"wgpu\", \"wgpu-msl\"] } + cubecl-cpp = { features = [\"metal\"] }, pulling in ~350 additional crates including wgpu, naga (shader translator), ash (Vulkan bindings unused on macOS), and the full cubecl stack (cubecl-core, cubecl-runtime, cubecl-linalg, cubecl-cpp, cubecl-wgpu). No standalone cubecl-metal crate exists — Metal support is exclusively through wgpu. For a Metal-only target like M4 Apple Silicon, the wgpu/naga/ash dependencies are dead weight. Compare: direct objc2-metal approach uses ~20 crates (objc2, objc2-metal, objc2-foundation, block2). The ~17x dependency multiplier increases compile times, binary size, and supply chain attack surface. Recommendation: use CubeCL only if cross-platform GPU support is required; for Apple Silicon exclusive targets, direct objc2-metal is strongly preferred.","tags":["proto5","cubecl","dependencies","build-cost","ecosystem"],"confidence":0.9,"source":"attention-proto/Cargo.lock analysis (cubecl feature-gated dependency count)"}
{"domain":"metal-compute","title":"proto8: AttentionBackend supertrait pattern viable without forking Burn","content":"Defining `trait AttentionBackend: Backend` with a `flash_attention(q, k, v, mask) -> Tensor<Self, 3>` method compiles and integrates cleanly with Burn 0.20.1's type system. The orphan rule is solved via a newtype wrapper `MetalAttentionBackend<B: Backend>(PhantomData<B>)` with Clone+Default+Debug derives. The bridge function `metal_flash_attention_bridge<B: Backend>()` is generic over any Burn Backend, extracting data via into_data()/to_vec::<f32>() and reconstructing via TensorData::new()/Tensor::from_data(). No fork of Burn is needed — the pattern is purely additive. The remaining production task is Backend delegation (forwarding 7+ op traits with hundreds of methods), which is mechanical boilerplate solvable with proc-macros or the ambassador crate. Verdict: YES — the supertrait pattern is the correct approach for adding custom GPU attention to Burn's trait hierarchy.","tags":["proto8","burn","trait-extension","attention-backend","supertrait","orphan-rule"],"confidence":0.95,"source":"attention-proto/proto8_burn (compile tests + integration test, Burn 0.20.1 NdArray)"}
{"domain":"gpu-perf","title":"proto8: Burn bridge dispatch overhead 2-17us on M4 — dominated by tensor copy cost","content":"The metal_flash_attention_bridge function adds 2-17us overhead vs direct Proto 1 Metal dispatch, measured via criterion (20 samples each) at D=64: N=64 ~2us, N=128 ~5us, N=256 ~8us, N=512 ~17us. Overhead source: Vec::clone for 3 input tensors + into_data() + to_vec::<f32>() extraction + TensorData::new() + Tensor::from_data() re-wrapping. Grows linearly with tensor data size (3 * N * D * 4 bytes per dispatch). For N=512 D=64, the bridge copies 3 * 512 * 64 * 4 = 384KB total. This is the unavoidable cost of bridging two memory models without a shared-memory backend. A native Burn Metal backend sharing MTLBuffer pointers would eliminate this overhead entirely. For production trait Attention, the 2-17us bridge overhead is negligible relative to attention compute (277-1065us for N=64-512).","tags":["proto8","burn","dispatch-overhead","bridge-pattern","gpu-perf","M4"],"confidence":0.9,"source":"attention-proto/burn_extension benchmark (criterion, 20 samples, N=64/128/256/512, D=64)"}
{"domain":"metal-compute","title":"proto8: Burn 0.20.1 compatible — NdArray backend for testing, ~15 crate footprint","content":"Burn 0.20.1 compiles successfully as an optional dependency behind the `burn-ext` feature flag. NdArray backend used for testing (CPU-only, no GPU dependency from Burn side). Burn dependency footprint is ~15 additional crates (burn-backend, burn-tensor, burn-core, burn-nn, burn-optim, burn-std, burn-derive, ahash, bincode, uuid, rand_distr, etc.) — significantly lighter than CubeCL's ~350 crates. Backend trait path is `burn::tensor::backend::Backend` (not `burn::backend::Backend`). TensorData API uses `TensorData::new(Vec<f32>, shape)` for creation and `.to_vec::<f32>()` (returns Result) for extraction. Tensor API uses `Tensor::from_data(data, &device)` and `.into_data()` for CPU round-trip. Burn 0.20 introduced CubeK (CubeCL-based kernels) but the extension trait pattern does not depend on CubeK — works with any backend including NdArray.","tags":["proto8","burn","burn-0.20","dependency-footprint","compatibility"],"confidence":0.95,"source":"attention-proto/proto8_burn (Cargo.toml dependency resolution, compile test)"}
{"domain":"metal-compute","title":"proto8: Burn extension trait requires ~150 lines boilerplate, zero unsafe blocks","content":"The complete Proto 8 implementation is ~240 lines of Rust: ~55 lines for AttentionBackend trait definition + MetalAttentionBackend newtype, ~70 lines for metal_flash_attention_bridge function, ~60 lines for tests, ~55 lines for documentation. Zero unsafe blocks are needed — the bridge goes through safe Burn tensor APIs (into_data, to_vec, from_data) and safe Metal host APIs (run_flash_attention). The only remaining boilerplate for production use is Backend delegation: forwarding FloatTensorOps, IntTensorOps, BoolTensorOps, ModuleOps, ActivationOps, QTensorOps, TransactionOps + Clone + Default + Sized + Send + Sync + Debug + 'static (7 op traits with hundreds of methods total). Solutions: (a) proc-macro derive generating forwarding impls, (b) ambassador crate for delegation, (c) manual forwarding. The architectural pattern is proven; only mechanical boilerplate remains.","tags":["proto8","burn","code-complexity","boilerplate","no-unsafe","backend-delegation"],"confidence":0.92,"source":"attention-proto/proto8_burn.rs (source analysis, line count, unsafe audit)"}
