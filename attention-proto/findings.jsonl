{"domain":"metal-compute","title":"proto1: Flash Attention baseline throughput on M4","content":"From-scratch Flash Attention kernel (Br=16, Bc=64, D=64, single head) achieves 0.16 TFLOPS at N=2048 on Apple M4. Kernel uses simdgroup_float8x8 for Q*K^T and P*V matmuls with online softmax (running max/sum). GPU time: ~6.7ms at N=2048. This is a first unoptimized implementation with one simdgroup (32 threads) per threadgroup, no shared memory prefetching, and no multi-head parallelism.","tags":["proto1","flash-attention","simdgroup-matrix","M4","tflops"],"confidence":0.85,"source":"attention-proto/proto1_flash benchmark (criterion, 50 samples, CV<1%)"}
{"domain":"gpu-perf","title":"proto1: Flash Attention TFLOPS scales with sequence length on M4","content":"Flash Attention TFLOPS scales roughly linearly with N in [256-2048] range on M4: N=256 → 0.03 TFLOPS (398us), N=512 → 0.07 TFLOPS (779us), N=1024 → 0.11 TFLOPS (2.4ms), N=2048 → 0.16 TFLOPS (6.7ms). FLOPs grow as O(N^2*D) while GPU time grows sub-quadratically, indicating better GPU utilization at larger workloads. The 5x throughput improvement from N=256 to N=2048 suggests the kernel is dispatch/launch-overhead-bound at small N and compute-bound at larger N.","tags":["proto1","flash-attention","scaling","M4","sequence-length"],"confidence":0.85,"source":"attention-proto/proto1_flash benchmark (criterion, 50 samples per N, CV<1%)"}
{"domain":"gpu-perf","title":"proto1: From-scratch Flash Attention achieves ~4-10% of MFA throughput on M4","content":"Metal-flash-attention (MFA) by Apple/philipturner achieves ~1790 GINSTRS (~1.8 TFLOPS equivalent) on M4 for optimized configurations. Our from-scratch Proto 1 kernel at 0.16 TFLOPS (N=2048, D=64) is approximately 4-10% of MFA performance. This gap is expected for a first implementation: (1) single simdgroup (32 threads) vs MFA's multi-simdgroup design, (2) no async copy or prefetching, (3) naive threadgroup memory layout, (4) no register-level optimizations. The 10-25x gap validates that significant optimization headroom exists and provides a concrete baseline for measuring improvement in later protos.","tags":["proto1","flash-attention","metal-flash-attention","mfa-comparison","M4"],"confidence":0.8,"source":"attention-proto/proto1_flash benchmark vs MFA published M4 numbers"}
{"domain":"msl-kernels","title":"proto1: 32KB threadgroup memory constraint validated for Br=16, Bc=64, D=64","content":"Flash Attention threadgroup memory layout at Br=16, Bc=64, D=64: Q_tile = Br*D*4 = 16*64*4 = 4096 bytes (4KB), K_chunk = Bc*D*4 = 64*64*4 = 16384 bytes (16KB), S_tile = Br*Bc*4 = 16*64*4 = 4096 bytes (4KB). Total = 24576 bytes (24KB), fits within Apple M4's 32KB per-threadgroup limit with 8KB headroom. Doubling Bc to 128 would require 32KB Q_tile(4KB) + K_chunk(32KB) + S_tile(8KB) = 44KB, exceeding the limit. Doubling Br to 32 would need Q_tile(8KB) + K_chunk(16KB) + S_tile(8KB) = 32KB, at the exact limit. This confirms Br=16, Bc=64 is a safe default for D=64.","tags":["proto1","flash-attention","threadgroup-memory","M4","tile-size"],"confidence":0.95,"source":"attention-proto/proto1_flash kernel analysis (deterministic calculation)"}
{"domain":"simd-wave","title":"proto1: simdgroup_float8x8 load/multiply_accumulate/store validated on M4","content":"simdgroup_float8x8 operations (simdgroup_load, simdgroup_multiply_accumulate, simdgroup_store) compile and produce correct results on Apple M4 with -std=metal3.1 and #include <metal_simdgroup_matrix>. The Flash Attention kernel uses 8x8 tiles for both Q*K^T and P*V matmuls. Correctness verified against FP64 CPU reference at N=256, D=64 with atol=5e-3, rtol=1e-2. The simdgroup_load with transpose=true flag enables direct K^T loading without explicit transpose. All 32 threads in a simdgroup participate cooperatively in each 8x8 matrix op.","tags":["proto1","simdgroup-matrix","simdgroup_float8x8","M4","metal3.1"],"confidence":0.95,"source":"attention-proto/proto1_flash correctness test (MTL_SHADER_VALIDATION=1)"}
{"domain":"gpu-perf","title":"proto1: GPU dispatch overhead dominates at small sequence lengths on M4","content":"At N=256 (FLOPS=16.8M), GPU kernel time is ~398us yielding only 0.03 TFLOPS (42 GFLOPS). At N=2048 (FLOPS=1.07B), time is ~6.7ms yielding 0.16 TFLOPS. The 8x N increase yields 64x more FLOPs but only 17x more time, indicating that at N=256 a significant fraction of time is non-compute overhead (command buffer submission, encoder setup, GPU clock ramp). Estimated dispatch overhead is ~100-200us based on extrapolation. For production attention kernels, batching multiple heads into a single dispatch (multi-head parallelism) is critical to amortize this overhead at small N.","tags":["proto1","dispatch-overhead","command-buffer","M4","small-workload"],"confidence":0.8,"source":"attention-proto/proto1_flash benchmark (extrapolated from N=256..2048 scaling)"}
{"domain":"metal-compute","title":"proto4: Function constant PSO compilation speed on M4","content":"Metal function constant PSO compilation takes 34-63us per variant on M4, enabling runtime specialization of 72+ attention variants in <5ms. Measured cold compile times: N=1 variant ~34us, N=10 ~34us/variant, N=50 ~37us/variant, N=100 ~43us/variant. Scaling is near-linear with slight per-variant increase at higher counts (cache pressure). 72 variants (full combinatorial: 3 HEAD_DIM x 4 BLOCK_R x 3 BLOCK_C x 2 VARIANT) compile in ~4.5ms total (~63us/variant). This is well under the 50ms/variant threshold for runtime dispatch viability.","tags":["proto4","function-constants","pso-compilation","M4"],"confidence":0.9,"source":"attention-proto/constant_overhead benchmark (criterion, cold_compile group)"}
{"domain":"metal-compute","title":"proto4: MTLBinaryArchive provides no speedup over cold compile on M4","content":"MTLBinaryArchive load time (4.7ms for 72 variants) equals cold compilation time on M4. Archives add 82ms creation overhead with zero runtime benefit. Archive creation is dominated by addComputePipelineFunctionsWithDescriptor_error calls (~82ms for 72 variants), not serialization. Archive load (~4.7ms) provides no speedup vs fresh cold compile (~4.5ms). The M4 Metal compiler is fast enough that pre-compiled binary archives are unnecessary for attention kernel variant counts up to at least 100. Binary archives may still benefit workloads with thousands of variants or slower GPU generations.","tags":["proto4","binary-archive","pso-compilation","M4"],"confidence":0.9,"source":"attention-proto/constant_overhead benchmark (criterion, binary_archive group)"}
{"domain":"gpu-perf","title":"proto4: PsoCache HashMap lookup is 350x faster than cold compile on M4","content":"PsoCache HashMap lookup at 178ns/variant is 350x faster than cold compilation (~63us/variant). For trait Attention<Q,K,V>, lazy compilation + cache is the recommended dispatch strategy. Total lookup time for 72 variants: ~12.8us (vs ~4.5ms cold compile). The HashMap key (PsoKey) serializes function constant values into a comparable type with negligible overhead. First access triggers cold compile (~63us), all subsequent accesses hit cache (~178ns). This validates the lazy-compile-and-cache pattern for runtime PSO management.","tags":["proto4","pso-cache","function-constants","dispatch-strategy"],"confidence":0.9,"source":"attention-proto/constant_overhead benchmark (criterion, pso_cache group)"}
{"domain":"gpu-centric-arch","title":"proto4: Function constants enable compile-time specialization for trait Attention","content":"Function constant compilation is fast enough (<63us/variant on M4) that trait Attention<Q,K,V> can use compile-time specialization via function constants rather than runtime dispatch via function stitching. This simplifies the architecture significantly: each (HEAD_DIM, BLOCK_R, BLOCK_C, VARIANT) tuple maps to a unique PSO compiled on first use and cached in PsoCache HashMap. No need for MTLLinkedFunctions, MTLVisibleFunctionTable, or binary archives. Recommended pattern: define trait variants as function constant combinations, lazy-compile PSOs on first dispatch, cache with PsoKey. Total startup cost for 72 variants: <5ms. Per-dispatch cache hit: ~178ns.","tags":["proto4","function-constants","architecture","trait-dispatch","M4"],"confidence":0.85,"source":"attention-proto/proto4 synthesis (cold_compile + binary_archive + pso_cache results)"}
