## Reality Check (BEFORE)

**Goal type**: Fix
**Problem**: gpu-search is 25-34x slower than GPUripgrep (10.4s vs <200ms for "kolbey" across 21GB)
**Root causes identified**: 3 (chunk cap, CPU resolve, metadata rebuild)

**Key metrics to improve**:
- GPU dispatch: 6.6s (64 serial batches) -> target <200ms (1 batch)
- CPU resolve: 3.6s (String::from_utf8_lossy per match) -> target 0ms (GPU line numbers)
- Metadata build: 20ms per keystroke -> target 0ms (pre-built in GCIX)

## Learnings

- ContentSearchEngine has TWO GpuMatchResult structs: one in content.rs (GpuMatchResult, internal to engine) and one in gpu/types.rs (public GpuMatchResult). The content.rs one maps directly to the Metal MatchResult struct. The types.rs one is used by StreamingSearchEngine. Must keep both in sync.
- The zerocopy kernel already has access to `raw_data` and `meta.buffer_offset` -- scanning from file start to match position for newlines is straightforward.
- GCIX format uses explicit byte-level serialization (not struct reinterpret_cast) -- safe to add fields to header as long as offsets and CRC are updated.
- `search_zerocopy()` batch loop exists because metadata_buffer is sized to max_chunks. Dynamic resize eliminates the need for batching.
- The `content::ContentMatch` (file_index + line_number + column + byte_offset) is different from `types::ContentMatch` (path + line_number + line_content + context). The orchestrator resolves from one to the other.
- `build_chunk_metadata()` is a standalone function in content_store.rs, not a method on ContentStore. Easy to call at save time and cache result.
- ChunkMetadata.buffer_offset is the key to zerocopy -- it gives the absolute position in the contiguous buffer, allowing the kernel to read directly without padding.
