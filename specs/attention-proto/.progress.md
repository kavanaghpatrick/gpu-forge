---
spec: attention-proto
basePath: specs/attention-proto
phase: tasks
task: 0/43
updated: 2026-02-12T08:35:00Z
---

# Progress: attention-proto

## Original Goal

Build 8 targeted GPU kernel prototypes producing 40-60 empirical KB findings to close ~30% knowledge gap for implementing `trait Attention<Q,K,V>` on Apple Silicon Metal. Focus: empirical verification of Flash Attention tile sizes, function dispatch strategies, PagedAttention memory constraints, linear attention viability, and ecosystem tooling (CubeCL, Burn).

**Prototypes**:
1. Metal Flash Attention — simdgroup_matrix tiled attention, tile size sweep, TFLOPS baseline
2. Function Stitching — [[stitchable]] inner loop overhead measurement
3. PagedAttention V2 — partitioned mode with block table, 32KB threadgroup constraint
4. Function Constants — compilation overhead for 72 variants, binary archive testing
5. CubeCL MSL Quality — compare generated vs hand-written MSL
6. FLA chunk_h/chunk_o — port linear attention kernels to Metal
7. RoPE/ALiBi/GQA — empirical per-variant timing on M4
8. Burn Extension Trait — AttentionBackend supertrait viability

**Tech stack**: Rust + objc2-metal 0.3 + MSL + criterion 0.5
**Target**: M4 Apple Silicon only
**Location**: `gpu_kernel/attention-proto/`
**Reference codebases**: gpu-query, particle-system

## Completed Tasks

### Spec Synthesis (Phase 0)
- [x] Read foreman analysis files (PM.md, TECH.md, QA.md, PM_QA.md)
- [x] Generate research.md — feasibility analysis, codebase exploration, external references
- [x] Generate requirements.md — 8 user stories, 18 FRs, 12 NFRs, acceptance criteria
- [x] Generate design.md — architecture, 8 prototype designs, shared infrastructure, technical decisions
- [x] Generate tasks.md — 43 POC-first tasks across 6 phases

### Phase 1: Foundation
- [x] 1.1 Create project structure and Cargo manifest
- [x] 1.2 Implement build.rs for Metal shader compilation
- [x] 1.3 Implement GpuDevice singleton
- [x] 1.4 Implement PsoCache with function constants
- [x] 1.5 Implement compute encoder helpers
- [x] 1.6 Implement GPU timing infrastructure
- [x] 1.7 Implement KB finding output
- [x] 1.8 Define AttentionParams #[repr(C)] type
- [x] 1.9 Foundation checkpoint — all 15 tests pass, zero clippy warnings

### Phase 2: Proto 1 — Flash Attention
- [x] 2.1 Implement naive CPU attention reference (FP64)
- [x] 2.2 Write basic flash_attention.metal kernel
- [x] 2.3 Implement proto1_flash host code and correctness test
- [x] 2.4 Implement tile size sweep benchmark
- [x] 2.5 Generate KB findings for Proto 1
- [x] 2.6 Proto 1 checkpoint — all tests pass, benchmark runs, 6 findings generated

## Proto 1 Summary
- **Tests**: 6 pass (5 unit + 1 integration), Metal shader validation enabled
- **Baseline TFLOPS** (D=64, Br=16, Bc=64): N=256 → 0.029, N=512 → 0.049, N=1024 → 0.071, N=2048 → 0.148
- **Criterion timing** (wall-clock): N=256 → 397us, N=512 → 777us, N=1024 → 2.4ms, N=2048 → 6.7ms
- **KB Findings**: 6 findings emitted (baseline throughput, scaling, MFA comparison, threadgroup memory, simdgroup_matrix validation, dispatch overhead)
- **Throughput scaling**: quadratic with N as expected, utilization improves at larger N

### Phase 3: Proto 4 — Function Constants
- [x] 3.1 Implement PSO compilation benchmark
- [x] 3.2 Implement binary archive benchmark

## Proto 4 Results
- **Cold compile per variant**: ~34us (N=1), ~34us/variant (N=10), ~37us/variant (N=50), ~43us/variant (N=100)
- **Scaling**: Near-linear — 100 variants compile in ~4.3ms total
- **Binary archive creation**: ~82ms for 72 variants (compile + serialize)
- **Archive load**: ~4.7ms for 72 variants (similar to cold compile — no speedup)
- **Cold compile 72**: ~4.5ms total (~63us/variant)
- **PsoCache hit**: ~12.8us for 72 lookups (~178ns/variant)
- **Verdict**: Function constants are extremely cheap to compile on M4 (<50ms/variant threshold easily met)
- **Verdict**: Binary archives provide no speedup over cold compile for attention variants — M4 Metal compiler is fast enough
- **Verdict**: Use PsoCache (HashMap) for runtime dispatch — ~178ns cache hit vs ~63us cold compile

## Current Task

Awaiting next task (3.3: Generate KB findings for Proto 4)

## Learnings

- Foreman PM, Tech Architect, and QA agents completed 3-day deep analysis with web research
- 1,124 findings already in KB from 45 prior investigations
- Key decisions: M4-only target, write Flash Attention from scratch (learn internals), fine-grained findings (40-60 for KB searchability)
- Critical dependencies: Proto 1 (Flash Attention) is foundation for all comparisons, Proto 4 (Function Constants) informs Proto 2 (Stitching)
- Spec synthesis: Research focuses on MFA reference, CubeCL/Burn ecosystem, FLA port difficulty, PagedAttention V2 memory constraints
- Design: Single library crate, 8 prototype modules, shared Metal infrastructure (device, pipeline, encode, timing, kb), criterion benchmarks
- Tasks: 43 tasks across 6 phases — Foundation (9 tasks) → Proto 1 (6) → Proto 4 (3) → Parallel Protos 2/3/6/7 (18) → Ecosystem 5/8 (4) → Quality Gates (6)
- Estimated duration: 14 working days (3 weeks)
- attention-proto is a standalone crate (not a workspace member), same pattern as gpu-query and particle-system
- criterion 0.5 with html_reports used for benchmarks; 8 bench targets with required-features for cubecl/burn-ext gated ones
- build.rs uses -std=metal3.1 for simdgroup_matrix, -O2 for release, stub metallib when no .metal files exist (adapted from gpu-query pattern)
- GpuDevice singleton uses OnceLock (not lazy_static), Send+Sync unsafe impl required for Retained<ProtocolObject<dyn MTL*>> types
- newLibraryWithFile_error is deprecated but works; uses #[allow(deprecated)] — future migration to newLibraryWithURL_error
- find_metallib() searches OUT_DIR first (works during cargo test), then exe-relative build dirs at runtime
- Stub metallib from build.rs is sufficient for device init tests (contains _stub kernel)
- PsoCache takes Retained<ProtocolObject<dyn MTLLibrary>> (not a reference) — use gpu.library.clone()
- PsoKey builder pattern: simple() + with_uint/float/bool by index or with_named_uint/float/bool by name
- Binary archive save/load uses NSURL::fileURLWithPath_isDirectory (not URLWithString) for file paths
- NSArray::from_slice(&[&ref]) works for passing protocol object references to Metal API
- MTLComputePipelineDescriptor + setBinaryArchives for archive-accelerated PSO compilation
- newComputePipelineStateWithDescriptor_options_reflection_error needed for descriptor-based PSO creation
- encode.rs: dispatchThreads (not dispatchThreadgroups) used for 1D dispatch — Metal handles non-uniform grids automatically
- dispatch_2d uses 16x16 threadgroup (256 threads) default, suitable for attention grid patterns (seq_len x heads)
- alloc_buffer_with_data needs NonNull wrapping of the data pointer for newBufferWithBytes_length_options
- timing.rs: MTLCommandBuffer trait's GPUStartTime()/GPUEndTime() return CFTimeInterval (f64 seconds since boot) — subtract for GPU kernel duration
- timing.rs: Must import MTLComputeCommandEncoder, MTLCommandEncoder, MTLCommandBuffer traits into scope for method resolution on ProtocolObject
- The _stub kernel takes no buffer arguments — can dispatch with empty buffer slice for warmup/timing tests
- PsoCache.get_or_compile takes &PsoKey (not device+name+option), returns &ProtocolObject<dyn MTLComputePipelineState>
- kb.rs: env!("CARGO_MANIFEST_DIR") resolves to attention-proto/ at compile time — use for findings.jsonl path
- KbFinding uses serde::Serialize, emit_finding uses serde_json::to_string (not pretty) for JSON-lines format
- OpenOptions append+create mode for idempotent multi-finding writes
- AttentionParams: 16 x u32/f32 fields = 64 bytes, 4-byte aligned, #[repr(C)] matches MSL struct exactly
- std::mem::offset_of! macro (stable since Rust 1.77) works for verifying C struct layout without unsafe code
- Convenience constructors (flash, gqa, paged) simplify prototype setup — scale auto-computed from head_dim
- Foundation checkpoint: 15 tests across 6 modules (device=1, pipeline=4, encode=1, timing=2, kb=1, types=6), zero clippy warnings, ready for Proto 1
- cpu_attention_f64: QA.md pattern works directly — FP64 accumulation, safe softmax with max subtraction, truncate to FP32 output
- assert_allclose: fails only when BOTH atol AND rtol are exceeded (OR logic matches numpy/torch allclose semantics)
- 5 tests for proto1_flash: identity Q + uniform K → mean(V), small data softmax sums to 1, single token → output equals V, assert_allclose pass/fail
- MSL kernel input attributes must all have the same vector dimensionality (all uint, all uint2, or all uint3) — cannot mix scalar and vector
- MSL function constants cannot be used in threadgroup array size expressions (multiplication of function constants not treated as compile-time constant) — use #define for fixed tile sizes, function constants for loop bounds
- simdgroup_float8x8 + simdgroup_load/store/multiply_accumulate compiles with -std=metal3.1 and #include <metal_simdgroup_matrix>
- simdgroup_load with transpose=true loads K^T directly for S = Q * K^T matmul — avoids explicit transpose step
- Flash attention kernel uses 24KB threadgroup memory (4KB Q_tile + 16KB K_chunk + 4KB S_tile) within 32KB limit for D=64, Br=16, Bc=64
- Threadgroup size of 32 (one simdgroup) sufficient for initial version — simdgroup_matrix ops use all 32 threads cooperatively
- Host code: dispatch_2d helper uses dispatchThreads (non-uniform grid), but flash attention kernel uses threadgroup_position_in_grid (tg_pos) which requires dispatchThreadgroups — must dispatch manually
- dispatchThreadgroups_threadsPerThreadgroup: grid = (ceil(seq_len/BLOCK_R), num_heads), threadgroup = (32, 1) for one simdgroup
- Must import MTLCommandQueue trait for commandBuffer() method on Retained<ProtocolObject<dyn MTLCommandQueue>>
- Flash attention GPU kernel produces correct results vs CPU FP64 reference at N=256, D=64 within atol=5e-3, rtol=1e-2 — passes with MTL_SHADER_VALIDATION=1
- Deterministic pseudo-random test data via LCG (seed-based) avoids dependency on rand in integration tests while maintaining reproducibility
- Criterion 0.5 does NOT support --output-format json flag (dropped from 0.4). JSON results are saved to target/criterion/ automatically. Benchmarks output to stderr/stdout in human-readable format.
- Flash attention benchmark: iter_custom with manual GPU timing (GPUStartTime/GPUEndTime) via dispatchThreadgroups (not dispatchThreads) since kernel uses threadgroup_position_in_grid
- Benchmark results (M4, D=64, Br=16, Bc=64): N=256 ~0.03 TFLOPS (398us), N=512 ~0.07 TFLOPS (779us), N=1024 ~0.11 TFLOPS (2.4ms), N=2048 ~0.16 TFLOPS (6.7ms)
- Criterion reports sub-1% confidence intervals — GPU timing is very stable across 50 samples with 5s warmup
- TFLOPS scales roughly with N^2 as expected (quadratic attention), but throughput increases with N due to better GPU utilization at larger workloads
- Must import MTLCommandQueue, MTLCommandBuffer, MTLCommandEncoder, MTLComputeCommandEncoder traits in bench files for method resolution on ProtocolObject types
- ProtocolObject type params require `dyn` keyword in Rust 2021 edition (e.g., ProtocolObject<dyn MTLBuffer> not ProtocolObject<MTLBuffer>)
- emit_proto1_findings() is a public function in proto1_flash.rs; also callable via #[ignore] test generate_proto1_findings
- kb::test_emit_finding calls clear_findings() at cleanup — running it concurrently with finding generation will delete findings.jsonl; always run generate test alone
- 6 findings emitted for Proto 1: baseline throughput, scaling, MFA comparison, threadgroup memory, simdgroup_matrix validation, dispatch overhead
- Cold PSO compilation benchmark: bypass PsoCache, use MTLFunctionConstantValues + newFunctionWithName_constantValues_error + newComputePipelineStateWithFunction_error directly
- NonNull wrapping of local u32 variables for setConstantValue_type_atIndex — same pattern as pipeline.rs PsoCache::build_constant_values
- M4 Metal compiler is extremely fast: ~34us per function constant variant cold compile, near-linear scaling to 100 variants (~4.3ms total)
- For N > 36 realistic combos (3 HEAD_DIM * 4 BLOCK_R * 3 BLOCK_C), use arbitrary uint values — Metal compiler still specializes each unique combination
- iter_custom with Instant::now() (not GPU timing) for measuring CPU-side compilation overhead
- Binary archive benchmark: archive creation (~82ms for 72 variants) is dominated by addComputePipelineFunctionsWithDescriptor_error calls, not serialization
- Binary archive load provides no speedup over cold compile for 72 attention variants on M4 (~4.7ms vs ~4.5ms) — Metal compiler is too fast for archives to help
- PsoCache HashMap lookup at ~178ns/variant is 350x faster than cold compile — use cache for runtime dispatch, skip binary archives entirely
- MTLBinaryArchive API works correctly in objc2-metal 0.3 but is unnecessary for this workload scale

## Blockers

- None currently

## Next

Task 3.3: Generate KB findings for Proto 4
