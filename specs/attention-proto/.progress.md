---
spec: attention-proto
basePath: specs/attention-proto
phase: tasks
task: 0/43
updated: 2026-02-12T08:35:00Z
---

# Progress: attention-proto

## Original Goal

Build 8 targeted GPU kernel prototypes producing 40-60 empirical KB findings to close ~30% knowledge gap for implementing `trait Attention<Q,K,V>` on Apple Silicon Metal. Focus: empirical verification of Flash Attention tile sizes, function dispatch strategies, PagedAttention memory constraints, linear attention viability, and ecosystem tooling (CubeCL, Burn).

**Prototypes**:
1. Metal Flash Attention — simdgroup_matrix tiled attention, tile size sweep, TFLOPS baseline
2. Function Stitching — [[stitchable]] inner loop overhead measurement
3. PagedAttention V2 — partitioned mode with block table, 32KB threadgroup constraint
4. Function Constants — compilation overhead for 72 variants, binary archive testing
5. CubeCL MSL Quality — compare generated vs hand-written MSL
6. FLA chunk_h/chunk_o — port linear attention kernels to Metal
7. RoPE/ALiBi/GQA — empirical per-variant timing on M4
8. Burn Extension Trait — AttentionBackend supertrait viability

**Tech stack**: Rust + objc2-metal 0.3 + MSL + criterion 0.5
**Target**: M4 Apple Silicon only
**Location**: `gpu_kernel/attention-proto/`
**Reference codebases**: gpu-query, particle-system

## Completed Tasks

### Spec Synthesis (Phase 0)
- [x] Read foreman analysis files (PM.md, TECH.md, QA.md, PM_QA.md)
- [x] Generate research.md — feasibility analysis, codebase exploration, external references
- [x] Generate requirements.md — 8 user stories, 18 FRs, 12 NFRs, acceptance criteria
- [x] Generate design.md — architecture, 8 prototype designs, shared infrastructure, technical decisions
- [x] Generate tasks.md — 43 POC-first tasks across 6 phases

### Phase 1: Foundation
- [x] 1.1 Create project structure and Cargo manifest
- [x] 1.2 Implement build.rs for Metal shader compilation
- [x] 1.3 Implement GpuDevice singleton
- [x] 1.4 Implement PsoCache with function constants
- [x] 1.5 Implement compute encoder helpers
- [x] 1.6 Implement GPU timing infrastructure
- [x] 1.7 Implement KB finding output
- [x] 1.8 Define AttentionParams #[repr(C)] type
- [x] 1.9 Foundation checkpoint — all 15 tests pass, zero clippy warnings

### Phase 2: Proto 1 — Flash Attention
- [x] 2.1 Implement naive CPU attention reference (FP64)
- [x] 2.2 Write basic flash_attention.metal kernel
- [x] 2.3 Implement proto1_flash host code and correctness test
- [x] 2.4 Implement tile size sweep benchmark
- [x] 2.5 Generate KB findings for Proto 1
- [x] 2.6 Proto 1 checkpoint — all tests pass, benchmark runs, 6 findings generated

## Proto 1 Summary
- **Tests**: 6 pass (5 unit + 1 integration), Metal shader validation enabled
- **Baseline TFLOPS** (D=64, Br=16, Bc=64): N=256 → 0.029, N=512 → 0.049, N=1024 → 0.071, N=2048 → 0.148
- **Criterion timing** (wall-clock): N=256 → 397us, N=512 → 777us, N=1024 → 2.4ms, N=2048 → 6.7ms
- **KB Findings**: 6 findings emitted (baseline throughput, scaling, MFA comparison, threadgroup memory, simdgroup_matrix validation, dispatch overhead)
- **Throughput scaling**: quadratic with N as expected, utilization improves at larger N

### Phase 3: Proto 4 — Function Constants
- [x] 3.1 Implement PSO compilation benchmark
- [x] 3.2 Implement binary archive benchmark
- [x] 3.3 Generate KB findings for Proto 4

## Proto 4 Results
- **Cold compile per variant**: ~34us (N=1), ~34us/variant (N=10), ~37us/variant (N=50), ~43us/variant (N=100)
- **Scaling**: Near-linear — 100 variants compile in ~4.3ms total
- **Binary archive creation**: ~82ms for 72 variants (compile + serialize)
- **Archive load**: ~4.7ms for 72 variants (similar to cold compile — no speedup)
- **Cold compile 72**: ~4.5ms total (~63us/variant)
- **PsoCache hit**: ~12.8us for 72 lookups (~178ns/variant)
- **Verdict**: Function constants are extremely cheap to compile on M4 (<50ms/variant threshold easily met)
- **Verdict**: Binary archives provide no speedup over cold compile for attention variants — M4 Metal compiler is fast enough
- **Verdict**: Use PsoCache (HashMap) for runtime dispatch — ~178ns cache hit vs ~63us cold compile

### Phase 4: Parallel Prototypes
- [x] 4.1 Implement flash_attention_stitched.metal
- [x] 4.2 Implement function stitching benchmark
- [x] 4.3 Generate KB findings for Proto 2
- [x] 4.4 Implement paged_attention.metal (phase 1 kernel)

## Proto 2 Results
- **Monolithic (mode 0)**: 2.44ms mean, 0.110 TFLOPS (N=1024, D=64)
- **Always_inline (mode 1)**: 2.45ms mean, 0.110 TFLOPS (+0.28% overhead, within noise)
- **Noinline (mode 2)**: 3.38ms mean, 0.079 TFLOPS (+39.4% overhead, SIGNIFICANT)
- **Per-call overhead**: always_inline ~144ns/call (noise), noinline ~20us/call (48 calls/dispatch)
- **Verdict**: always_inline has zero measurable overhead — safe to factor kernels into inline functions for readability
- **Verdict**: noinline has ~39% overhead — real function calls are expensive on M4 GPU, avoid for hot inner loops

- [x] 4.5 Implement paged_reduce.metal (phase 2 kernel)
- [x] 4.6 Implement proto3_paged host code and correctness test

## Current Task

Awaiting next task

## Proto 3 Notes
- paged_attention.metal phase 1 kernel compiles successfully with -std=metal3.1 -O2
- KV_cache layout: [num_pages, 2, page_size, head_dim] — K and V interleaved per page for locality
- Threadgroup memory: 13KB total (Q_tile 4KB + K_page 4KB + V_page 4KB + S_buf 1KB) — well within 32KB
- Simple scalar dot products for Q*K^T (no simdgroup_matrix) — sufficient for prototype paging overhead measurement
- Online softmax per row with running max/sum, same algorithm as flash_attention.metal
- Page table indirection: page_table[logical_page] -> physical_page index for KV_cache lookup
- Partitioning: each threadgroup processes a contiguous range of pages; partial O/m/l written for reduce kernel
- Function constants: HEAD_DIM and PAGE_SIZE set at PSO compile time

## Proto 2 KB Findings
- 4 findings emitted: always_inline zero overhead, noinline 39% overhead, architecture recommendation (function constants over stitching), function stitching not viable for compute inner loops
- Combined Proto 2+4 verdict: function constants are the clear dispatch strategy for trait Attention<Q,K,V>

## Learnings

- Foreman PM, Tech Architect, and QA agents completed 3-day deep analysis with web research
- 1,124 findings already in KB from 45 prior investigations
- Key decisions: M4-only target, write Flash Attention from scratch (learn internals), fine-grained findings (40-60 for KB searchability)
- Critical dependencies: Proto 1 (Flash Attention) is foundation for all comparisons, Proto 4 (Function Constants) informs Proto 2 (Stitching)
- Spec synthesis: Research focuses on MFA reference, CubeCL/Burn ecosystem, FLA port difficulty, PagedAttention V2 memory constraints
- Design: Single library crate, 8 prototype modules, shared Metal infrastructure (device, pipeline, encode, timing, kb), criterion benchmarks
- Tasks: 43 tasks across 6 phases — Foundation (9 tasks) → Proto 1 (6) → Proto 4 (3) → Parallel Protos 2/3/6/7 (18) → Ecosystem 5/8 (4) → Quality Gates (6)
- Estimated duration: 14 working days (3 weeks)
- attention-proto is a standalone crate (not a workspace member), same pattern as gpu-query and particle-system
- criterion 0.5 with html_reports used for benchmarks; 8 bench targets with required-features for cubecl/burn-ext gated ones
- build.rs uses -std=metal3.1 for simdgroup_matrix, -O2 for release, stub metallib when no .metal files exist (adapted from gpu-query pattern)
- GpuDevice singleton uses OnceLock (not lazy_static), Send+Sync unsafe impl required for Retained<ProtocolObject<dyn MTL*>> types
- newLibraryWithFile_error is deprecated but works; uses #[allow(deprecated)] — future migration to newLibraryWithURL_error
- find_metallib() searches OUT_DIR first (works during cargo test), then exe-relative build dirs at runtime
- Stub metallib from build.rs is sufficient for device init tests (contains _stub kernel)
- PsoCache takes Retained<ProtocolObject<dyn MTLLibrary>> (not a reference) — use gpu.library.clone()
- PsoKey builder pattern: simple() + with_uint/float/bool by index or with_named_uint/float/bool by name
- Binary archive save/load uses NSURL::fileURLWithPath_isDirectory (not URLWithString) for file paths
- NSArray::from_slice(&[&ref]) works for passing protocol object references to Metal API
- MTLComputePipelineDescriptor + setBinaryArchives for archive-accelerated PSO compilation
- newComputePipelineStateWithDescriptor_options_reflection_error needed for descriptor-based PSO creation
- encode.rs: dispatchThreads (not dispatchThreadgroups) used for 1D dispatch — Metal handles non-uniform grids automatically
- dispatch_2d uses 16x16 threadgroup (256 threads) default, suitable for attention grid patterns (seq_len x heads)
- alloc_buffer_with_data needs NonNull wrapping of the data pointer for newBufferWithBytes_length_options
- timing.rs: MTLCommandBuffer trait's GPUStartTime()/GPUEndTime() return CFTimeInterval (f64 seconds since boot) — subtract for GPU kernel duration
- timing.rs: Must import MTLComputeCommandEncoder, MTLCommandEncoder, MTLCommandBuffer traits into scope for method resolution on ProtocolObject
- The _stub kernel takes no buffer arguments — can dispatch with empty buffer slice for warmup/timing tests
- PsoCache.get_or_compile takes &PsoKey (not device+name+option), returns &ProtocolObject<dyn MTLComputePipelineState>
- kb.rs: env!("CARGO_MANIFEST_DIR") resolves to attention-proto/ at compile time — use for findings.jsonl path
- KbFinding uses serde::Serialize, emit_finding uses serde_json::to_string (not pretty) for JSON-lines format
- OpenOptions append+create mode for idempotent multi-finding writes
- AttentionParams: 16 x u32/f32 fields = 64 bytes, 4-byte aligned, #[repr(C)] matches MSL struct exactly
- std::mem::offset_of! macro (stable since Rust 1.77) works for verifying C struct layout without unsafe code
- Convenience constructors (flash, gqa, paged) simplify prototype setup — scale auto-computed from head_dim
- Foundation checkpoint: 15 tests across 6 modules (device=1, pipeline=4, encode=1, timing=2, kb=1, types=6), zero clippy warnings, ready for Proto 1
- cpu_attention_f64: QA.md pattern works directly — FP64 accumulation, safe softmax with max subtraction, truncate to FP32 output
- assert_allclose: fails only when BOTH atol AND rtol are exceeded (OR logic matches numpy/torch allclose semantics)
- 5 tests for proto1_flash: identity Q + uniform K → mean(V), small data softmax sums to 1, single token → output equals V, assert_allclose pass/fail
- MSL kernel input attributes must all have the same vector dimensionality (all uint, all uint2, or all uint3) — cannot mix scalar and vector
- MSL function constants cannot be used in threadgroup array size expressions (multiplication of function constants not treated as compile-time constant) — use #define for fixed tile sizes, function constants for loop bounds
- simdgroup_float8x8 + simdgroup_load/store/multiply_accumulate compiles with -std=metal3.1 and #include <metal_simdgroup_matrix>
- simdgroup_load with transpose=true loads K^T directly for S = Q * K^T matmul — avoids explicit transpose step
- Flash attention kernel uses 24KB threadgroup memory (4KB Q_tile + 16KB K_chunk + 4KB S_tile) within 32KB limit for D=64, Br=16, Bc=64
- Threadgroup size of 32 (one simdgroup) sufficient for initial version — simdgroup_matrix ops use all 32 threads cooperatively
- Host code: dispatch_2d helper uses dispatchThreads (non-uniform grid), but flash attention kernel uses threadgroup_position_in_grid (tg_pos) which requires dispatchThreadgroups — must dispatch manually
- dispatchThreadgroups_threadsPerThreadgroup: grid = (ceil(seq_len/BLOCK_R), num_heads), threadgroup = (32, 1) for one simdgroup
- Must import MTLCommandQueue trait for commandBuffer() method on Retained<ProtocolObject<dyn MTLCommandQueue>>
- Flash attention GPU kernel produces correct results vs CPU FP64 reference at N=256, D=64 within atol=5e-3, rtol=1e-2 — passes with MTL_SHADER_VALIDATION=1
- Deterministic pseudo-random test data via LCG (seed-based) avoids dependency on rand in integration tests while maintaining reproducibility
- Criterion 0.5 does NOT support --output-format json flag (dropped from 0.4). JSON results are saved to target/criterion/ automatically. Benchmarks output to stderr/stdout in human-readable format.
- Flash attention benchmark: iter_custom with manual GPU timing (GPUStartTime/GPUEndTime) via dispatchThreadgroups (not dispatchThreads) since kernel uses threadgroup_position_in_grid
- Benchmark results (M4, D=64, Br=16, Bc=64): N=256 ~0.03 TFLOPS (398us), N=512 ~0.07 TFLOPS (779us), N=1024 ~0.11 TFLOPS (2.4ms), N=2048 ~0.16 TFLOPS (6.7ms)
- Criterion reports sub-1% confidence intervals — GPU timing is very stable across 50 samples with 5s warmup
- TFLOPS scales roughly with N^2 as expected (quadratic attention), but throughput increases with N due to better GPU utilization at larger workloads
- Must import MTLCommandQueue, MTLCommandBuffer, MTLCommandEncoder, MTLComputeCommandEncoder traits in bench files for method resolution on ProtocolObject types
- ProtocolObject type params require `dyn` keyword in Rust 2021 edition (e.g., ProtocolObject<dyn MTLBuffer> not ProtocolObject<MTLBuffer>)
- emit_proto1_findings() is a public function in proto1_flash.rs; also callable via #[ignore] test generate_proto1_findings
- kb::test_emit_finding calls clear_findings() at cleanup — running it concurrently with finding generation will delete findings.jsonl; always run generate test alone
- 6 findings emitted for Proto 1: baseline throughput, scaling, MFA comparison, threadgroup memory, simdgroup_matrix validation, dispatch overhead
- Cold PSO compilation benchmark: bypass PsoCache, use MTLFunctionConstantValues + newFunctionWithName_constantValues_error + newComputePipelineStateWithFunction_error directly
- NonNull wrapping of local u32 variables for setConstantValue_type_atIndex — same pattern as pipeline.rs PsoCache::build_constant_values
- M4 Metal compiler is extremely fast: ~34us per function constant variant cold compile, near-linear scaling to 100 variants (~4.3ms total)
- For N > 36 realistic combos (3 HEAD_DIM * 4 BLOCK_R * 3 BLOCK_C), use arbitrary uint values — Metal compiler still specializes each unique combination
- iter_custom with Instant::now() (not GPU timing) for measuring CPU-side compilation overhead
- Binary archive benchmark: archive creation (~82ms for 72 variants) is dominated by addComputePipelineFunctionsWithDescriptor_error calls, not serialization
- Binary archive load provides no speedup over cold compile for 72 attention variants on M4 (~4.7ms vs ~4.5ms) — Metal compiler is too fast for archives to help
- PsoCache HashMap lookup at ~178ns/variant is 350x faster than cold compile — use cache for runtime dispatch, skip binary archives entirely
- MTLBinaryArchive API works correctly in objc2-metal 0.3 but is unnecessary for this workload scale
- Proto 4 findings: 4 findings emitted (compilation speed, binary archive no-speedup, PsoCache dispatch strategy, architecture recommendation)
- proto4_constants.rs follows same pattern as proto1_flash.rs: emit_proto4_findings() function + #[ignore] test for manual generation
- [[stitchable]] attribute is for visible_function_table (runtime indirect calls) in compute kernels — not suitable for this benchmark. Instead use __attribute__((noinline)) to force real function calls vs __attribute__((always_inline)) for guaranteed inlining. This tests the same overhead question more directly.
- flash_attention_stitched.metal uses STITCH_MODE function_constant(3) with 3 modes: 0=monolithic (all in kernel body), 1=always_inline functions, 2=noinline functions. Metal compiler eliminates dead branches at PSO compile time.
- Factored inner loop into 3 helper functions: compute_scores (simdgroup_matrix S=Q*K^T), scale_scores (scale + mask), softmax_accumulate (online softmax + O += P*V)
- Each helper function has inline and noinline variants — both use identical logic, only linkage differs
- For benchmark files needing multiple PSOs, compile directly with MTLFunctionConstantValues + newComputePipelineStateWithFunction_error returning owned Retained<> — avoids PsoCache borrow checker issues with closures
- Function stitching benchmark: always_inline has zero overhead vs monolithic (within noise at 0.28%), noinline has ~39% overhead (~20us/call) — M4 GPU function call mechanism is expensive
- 48 function calls per dispatch at N=1024, Bc=64 (1024/64=16 KV blocks * 3 functions each)
- After cargo clean, build.rs recompiles all .metal files including new stitched shader — previously cached metallib may not include newly added shaders
- proto2_stitch.rs follows same pattern as proto4_constants.rs: emit_proto2_findings() function + #[ignore] test for manual generation
- Proto 2 KB findings: 4 findings (always_inline 0% overhead, noinline 39% overhead, function constants > stitching recommendation, function stitching not viable for compute inner loops)
- Combined Proto 2+4 definitive answer: function constants (0% runtime overhead, 63us compile) beat runtime dispatch (39%+ overhead) in all scenarios
- PagedAttention kernel uses interleaved KV layout [num_pages, 2, page_size, head_dim] for better spatial locality when loading K then V for the same page
- Scalar dot products sufficient for prototype — measures paging overhead without simdgroup_matrix complexity
- Threadgroup memory 13KB (TILE_Q=16, TILE_PAGE=16, TILE_D=64) leaves 19KB headroom for larger page sizes if needed
- Partition output indexing: (block_row * num_parts + partition) as linear index for O_partial/m_partial/l_partial arrays
- Reduce kernel thread layout: tid = row * TILE_D + d_col — each thread handles one (row, dim) pair, iterates over all partitions
- Log-sum-exp reduction: find global max across partitions, rescale each partition's O and l by exp(m_p - m_global), sum, normalize by 1/l_total
- Reduce kernel threadgroup size: TILE_Q * TILE_D = 16 * 64 = 1024 threads — matches one (row, dim) pair per thread for full parallelism
- When needing two PSOs from PsoCache in the same scope, use separate PsoCache instances to avoid double mutable borrow — each cache clones the library Retained
- PagedAttention host code: reversed page_table order for deterministic fragmentation simulation — simpler than random shuffle and fully reproducible
- Paged attention correctness passes at atol=1e-3, rtol=1e-2 with N=64, D=64, page_size=16, 1 partition — matches CPU FP64 reference well despite scalar dot products and online softmax

## Proto 3 Host Code Notes
- PagedKVCache struct: page_size, head_dim, num_physical_pages, kv_data (flat [num_pages, 2, page_size, head_dim]), page_table (logical->physical)
- create_paged_kv_cache: splits K/V into pages, reverses physical order for deterministic fragmentation simulation
- run_paged_attention: two-pass dispatch — partition kernel (32 threads, one simdgroup) then reduce kernel (TILE_Q*TILE_D=1024 threads)
- Separate PsoCache instances for partition and reduce PSOs to avoid borrow checker issues
- Correctness validated: N=64, D=64, page_size=16, 1 partition, atol=1e-3, rtol=1e-2 vs CPU FP64 reference
- Metal shader validation enabled — no GPU-side errors

## Blockers

- None currently

## Next

Task 4.7: Implement threadgroup memory budget test
