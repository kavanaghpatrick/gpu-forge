---
spec: attention-proto
basePath: specs/attention-proto
phase: tasks
task: 0/43
updated: 2026-02-12T08:35:00Z
---

# Progress: attention-proto

## Original Goal

Build 8 targeted GPU kernel prototypes producing 40-60 empirical KB findings to close ~30% knowledge gap for implementing `trait Attention<Q,K,V>` on Apple Silicon Metal. Focus: empirical verification of Flash Attention tile sizes, function dispatch strategies, PagedAttention memory constraints, linear attention viability, and ecosystem tooling (CubeCL, Burn).

**Prototypes**:
1. Metal Flash Attention — simdgroup_matrix tiled attention, tile size sweep, TFLOPS baseline
2. Function Stitching — [[stitchable]] inner loop overhead measurement
3. PagedAttention V2 — partitioned mode with block table, 32KB threadgroup constraint
4. Function Constants — compilation overhead for 72 variants, binary archive testing
5. CubeCL MSL Quality — compare generated vs hand-written MSL
6. FLA chunk_h/chunk_o — port linear attention kernels to Metal
7. RoPE/ALiBi/GQA — empirical per-variant timing on M4
8. Burn Extension Trait — AttentionBackend supertrait viability

**Tech stack**: Rust + objc2-metal 0.3 + MSL + criterion 0.5
**Target**: M4 Apple Silicon only
**Location**: `gpu_kernel/attention-proto/`
**Reference codebases**: gpu-query, particle-system

## Completed Tasks

### Spec Synthesis (Phase 0)
- [x] Read foreman analysis files (PM.md, TECH.md, QA.md, PM_QA.md)
- [x] Generate research.md — feasibility analysis, codebase exploration, external references
- [x] Generate requirements.md — 8 user stories, 18 FRs, 12 NFRs, acceptance criteria
- [x] Generate design.md — architecture, 8 prototype designs, shared infrastructure, technical decisions
- [x] Generate tasks.md — 43 POC-first tasks across 6 phases

### Phase 1: Foundation
- [x] 1.1 Create project structure and Cargo manifest
- [x] 1.2 Implement build.rs for Metal shader compilation
- [x] 1.3 Implement GpuDevice singleton
- [x] 1.4 Implement PsoCache with function constants
- [x] 1.5 Implement compute encoder helpers
- [x] 1.6 Implement GPU timing infrastructure
- [x] 1.7 Implement KB finding output
- [x] 1.8 Define AttentionParams #[repr(C)] type
- [x] 1.9 Foundation checkpoint — all 15 tests pass, zero clippy warnings

### Phase 2: Proto 1 — Flash Attention
- [x] 2.1 Implement naive CPU attention reference (FP64)

## Current Task

Awaiting next task

## Learnings

- Foreman PM, Tech Architect, and QA agents completed 3-day deep analysis with web research
- 1,124 findings already in KB from 45 prior investigations
- Key decisions: M4-only target, write Flash Attention from scratch (learn internals), fine-grained findings (40-60 for KB searchability)
- Critical dependencies: Proto 1 (Flash Attention) is foundation for all comparisons, Proto 4 (Function Constants) informs Proto 2 (Stitching)
- Spec synthesis: Research focuses on MFA reference, CubeCL/Burn ecosystem, FLA port difficulty, PagedAttention V2 memory constraints
- Design: Single library crate, 8 prototype modules, shared Metal infrastructure (device, pipeline, encode, timing, kb), criterion benchmarks
- Tasks: 43 tasks across 6 phases — Foundation (9 tasks) → Proto 1 (6) → Proto 4 (3) → Parallel Protos 2/3/6/7 (18) → Ecosystem 5/8 (4) → Quality Gates (6)
- Estimated duration: 14 working days (3 weeks)
- attention-proto is a standalone crate (not a workspace member), same pattern as gpu-query and particle-system
- criterion 0.5 with html_reports used for benchmarks; 8 bench targets with required-features for cubecl/burn-ext gated ones
- build.rs uses -std=metal3.1 for simdgroup_matrix, -O2 for release, stub metallib when no .metal files exist (adapted from gpu-query pattern)
- GpuDevice singleton uses OnceLock (not lazy_static), Send+Sync unsafe impl required for Retained<ProtocolObject<dyn MTL*>> types
- newLibraryWithFile_error is deprecated but works; uses #[allow(deprecated)] — future migration to newLibraryWithURL_error
- find_metallib() searches OUT_DIR first (works during cargo test), then exe-relative build dirs at runtime
- Stub metallib from build.rs is sufficient for device init tests (contains _stub kernel)
- PsoCache takes Retained<ProtocolObject<dyn MTLLibrary>> (not a reference) — use gpu.library.clone()
- PsoKey builder pattern: simple() + with_uint/float/bool by index or with_named_uint/float/bool by name
- Binary archive save/load uses NSURL::fileURLWithPath_isDirectory (not URLWithString) for file paths
- NSArray::from_slice(&[&ref]) works for passing protocol object references to Metal API
- MTLComputePipelineDescriptor + setBinaryArchives for archive-accelerated PSO compilation
- newComputePipelineStateWithDescriptor_options_reflection_error needed for descriptor-based PSO creation
- encode.rs: dispatchThreads (not dispatchThreadgroups) used for 1D dispatch — Metal handles non-uniform grids automatically
- dispatch_2d uses 16x16 threadgroup (256 threads) default, suitable for attention grid patterns (seq_len x heads)
- alloc_buffer_with_data needs NonNull wrapping of the data pointer for newBufferWithBytes_length_options
- timing.rs: MTLCommandBuffer trait's GPUStartTime()/GPUEndTime() return CFTimeInterval (f64 seconds since boot) — subtract for GPU kernel duration
- timing.rs: Must import MTLComputeCommandEncoder, MTLCommandEncoder, MTLCommandBuffer traits into scope for method resolution on ProtocolObject
- The _stub kernel takes no buffer arguments — can dispatch with empty buffer slice for warmup/timing tests
- PsoCache.get_or_compile takes &PsoKey (not device+name+option), returns &ProtocolObject<dyn MTLComputePipelineState>
- kb.rs: env!("CARGO_MANIFEST_DIR") resolves to attention-proto/ at compile time — use for findings.jsonl path
- KbFinding uses serde::Serialize, emit_finding uses serde_json::to_string (not pretty) for JSON-lines format
- OpenOptions append+create mode for idempotent multi-finding writes
- AttentionParams: 16 x u32/f32 fields = 64 bytes, 4-byte aligned, #[repr(C)] matches MSL struct exactly
- std::mem::offset_of! macro (stable since Rust 1.77) works for verifying C struct layout without unsafe code
- Convenience constructors (flash, gqa, paged) simplify prototype setup — scale auto-computed from head_dim
- Foundation checkpoint: 15 tests across 6 modules (device=1, pipeline=4, encode=1, timing=2, kb=1, types=6), zero clippy warnings, ready for Proto 1
- cpu_attention_f64: QA.md pattern works directly — FP64 accumulation, safe softmax with max subtraction, truncate to FP32 output
- assert_allclose: fails only when BOTH atol AND rtol are exceeded (OR logic matches numpy/torch allclose semantics)
- 5 tests for proto1_flash: identity Q + uniform K → mean(V), small data softmax sums to 1, single token → output equals V, assert_allclose pass/fail

## Blockers

- None currently

## Next

Task 2.2: Write basic flash_attention.metal kernel
