---
spec: mistral-7b-perf
basePath: ./specs/mistral-7b-perf
phase: tasks
task: 1/24
updated: 2026-02-16
---

# Progress: mistral-7b-perf

## Original Goal

Close the 42 → 100+ tok/s gap in metal-attention GPU inference on Mistral-7B Q4_0 M4 Pro with THREE immediate optimizations plus speculative decoding prep. Foreman analysis complete at metal-attention/ai/tasks/spec/ (PM.md, TECH.md, UX.md, QA.md, OVERVIEW.md). THREE CHANGES: (1) Q6_K Metal matvec kernel — lm_head is 512MB F32 dequantized from Q6_K. Native Q6_K matvec reads only 108MB (4.9x less). Q6_K super-blocks are 210 bytes: ql[128]+qh[64]+scales[16]+d[2] for 256 values. 256 threads, 8 simdgroups, 8 rows/TG matching Q4_0 pattern. (2) Page-aligned weight buffers — WeightBuffer{buffer, offset} struct, makeBuffer(bytesNoCopy:), zero memcpy. (3) Batch prefill for Mistral-7B GQA — extend forward_prompt to 32Q/8KV, hidden=4096, target 500+ tok/s. PLUS (4) Speculative decoding scaffolding — SpeculativeDecoder, draft model, --draft CLI flag. BLOCKER: vocab mismatch SmolLM 49K ≠ Mistral 32K.

## Completed Tasks
- [x] 1.1 Create matvec_q6_k.metal shader

## Current Task
Awaiting next task

## Learnings
- Q6_K super-block is 210 bytes: ql[128] + qh[64] + scales[16] + d(half) = 128+64+16+2 = 210
- Q6_K has 256 values per super-block (vs 32 for Q4_0/Q8_0), so n_blocks = in_dim/256
- ql stores 2 values per byte (low/high nibbles = low 4 bits each), qh stores 4 values per byte (2 bits each)
- Reconstruct: q6 = ql_nibble | (qh_2bits << 4), dequant = d * scales[sb] * (q6 - 32)
- Followed Q8_0 pattern: 256 threads, 8 simdgroups, ROWS_PER_TG=8, threadgroups = ceil(out_dim/8)
- Build system auto-discovers .metal files in shaders/ dir, need cargo clean -p to pick up new shaders

## Blockers

- Tokenizer mismatch: SmolLM (49K vocab) ≠ Mistral (32K vocab) blocks naive speculative decoding

## Next
Task 1.2: Create multi_token_matvec_q6_k.metal shader (batched variant)
