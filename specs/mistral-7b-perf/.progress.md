---
spec: mistral-7b-perf
basePath: ./specs/mistral-7b-perf
phase: tasks
task: 5/24
updated: 2026-02-16
---

# Progress: mistral-7b-perf

## Original Goal

Close the 42 → 100+ tok/s gap in metal-attention GPU inference on Mistral-7B Q4_0 M4 Pro with THREE immediate optimizations plus speculative decoding prep. Foreman analysis complete at metal-attention/ai/tasks/spec/ (PM.md, TECH.md, UX.md, QA.md, OVERVIEW.md). THREE CHANGES: (1) Q6_K Metal matvec kernel — lm_head is 512MB F32 dequantized from Q6_K. Native Q6_K matvec reads only 108MB (4.9x less). Q6_K super-blocks are 210 bytes: ql[128]+qh[64]+scales[16]+d[2] for 256 values. 256 threads, 8 simdgroups, 8 rows/TG matching Q4_0 pattern. (2) Page-aligned weight buffers — WeightBuffer{buffer, offset} struct, makeBuffer(bytesNoCopy:), zero memcpy. (3) Batch prefill for Mistral-7B GQA — extend forward_prompt to 32Q/8KV, hidden=4096, target 500+ tok/s. PLUS (4) Speculative decoding scaffolding — SpeculativeDecoder, draft model, --draft CLI flag. BLOCKER: vocab mismatch SmolLM 49K ≠ Mistral 32K.

## Completed Tasks
- [x] 1.1 Create matvec_q6_k.metal shader
- [x] 1.2 Create multi_token_matvec_q6_k.metal shader
- [x] 1.3 Add lm_head_q6k field to GpuWeightStore
- [x] 1.4 Add encode_matvec_q6_k to GpuForwardPass
- [x] 1.5 Q6_K kernel correctness test

## Current Task
Awaiting next task

## Learnings
- Q6_K super-block is 210 bytes: ql[128] + qh[64] + scales[16] + d(half) = 128+64+16+2 = 210
- Q6_K has 256 values per super-block (vs 32 for Q4_0/Q8_0), so n_blocks = in_dim/256
- ql stores 2 values per byte (low/high nibbles = low 4 bits each), qh stores 4 values per byte (2 bits each)
- Reconstruct: q6 = ql_nibble | (qh_2bits << 4), dequant = d * scales[sb] * (q6 - 32)
- Followed Q8_0 pattern: 256 threads, 8 simdgroups, ROWS_PER_TG=8, threadgroups = ceil(out_dim/8)
- Build system auto-discovers .metal files in shaders/ dir, need cargo clean -p to pick up new shaders
- Batched multi-token pattern: outer loop over tokens, weight pointers same (SLC cache reuse), input/output indexed by tok*dim
- Two kernel variants needed: overwrite (=) and accumulate (+=) for residual connections
- Accumulate variant uses _accumulate suffix matching Q4_0 convention
- Q6_K GPU vs CPU correctness test: synthetic blocks with random ql/qh/scales + known d scale, tolerance 5e-2 sufficient for both (256,256) and (4096,32000)
- Test needs `-p metal-attention` flag since gpu_correctness.rs is in crates/metal-attention/tests/ not workspace root
- Pre-existing build bug: GgufType::Q5_K doesn't exist, should be Q5_K_S | Q5_K_M — fixed in gpu_weight_store.rs
- CPU reference uses f64 accumulation to avoid confounding GPU vs CPU FP ordering differences with reference errors

## Blockers

- Tokenizer mismatch: SmolLM (49K vocab) != Mistral (32K vocab) blocks naive speculative decoding

## Next
Task 1.6: POC Checkpoint - Q6_K end-to-end validation
