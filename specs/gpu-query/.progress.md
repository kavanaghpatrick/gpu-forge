# Progress: gpu-query

## Original Goal

Build a GPU-native local data analytics engine ("gpu-query") for Apple Silicon M4 laptops. The tool lets users point at a directory of files (JSON, CSV, logs, Parquet, plain text) and query them like a database in milliseconds — BigQuery-class performance on local files with zero data copying. Core architecture: files are mmap'd directly as Metal buffers (zero-copy via makeBuffer bytesNoCopy), GPU persistent compute kernels parse/index/query data at SSD bandwidth, columnar SoA storage in GPU memory, SQL-like query interface with GPU-compiled filters and simdgroup aggregations, and a full dashboard TUI with gradients for interactive results. The CPU does nothing except present results.

## Source Context

Detailed foreman-spec analysis available at:
- ai/tasks/spec/PM.md (584 lines, 26 KB findings)
- ai/tasks/spec/UX.md (1024 lines, 21 KB findings)
- ai/tasks/spec/TECH.md (1673 lines, 50+ KB findings)
- ai/tasks/spec/QA.md (1190 lines, 16 KB findings)
- ai/tasks/spec/OVERVIEW.md (executive summaries)

## Key Decisions (from Foreman Q&A)

- Target audience: Data engineers
- File formats: Parquet + CSV + JSON
- Query planner: Custom minimal MVP, DataFusion Phase 2
- Performance: Tiered (100M M4, 1B M4 Pro, 10B M4 Max)
- Default mode: Full Dashboard TUI
- CPU comparison: Always shown
- Theme: Gradients — "make it look sick"
- Autocomplete: Rich from start
- Shaders: AOT via build.rs
- Strings: Adaptive dictionary encoding
- Metal API: Metal 3 baseline + Metal 4 opt-in
- Oracle: Golden files for MVP
- Perf gate: 15% blocks, 5% warns
- Fuzzing: Parser-only (CSV + JSON)

## Phase Status

- [x] Research (via foreman-spec PM + TECH agents, synthesized to ralph-specum format)
- [x] Requirements (via foreman-spec PM + UX agents, synthesized to ralph-specum format)
- [x] Design (via foreman-spec TECH agent, synthesized to ralph-specum format)
- [x] Tasks (40 tasks across 5 phases, POC-first structure)
- [ ] Execution

## Completed Tasks
- [x] 1.1 Scaffold project and build.rs
- [x] 1.2 Metal device init and GPU types
- [x] 1.3 mmap + zero-copy Metal buffer
- [x] 1.4 CSV metadata reader (CPU-side)
- [x] 1.5 GPU CSV parser kernel (newline detection + field extraction)
- [x] 1.6 GPU column filter kernel (WHERE clause)
- [x] 1.7 GPU aggregation kernel (COUNT/SUM)
- [x] 1.8 SQL parser integration
- [x] 1.9 GPU execution engine (end-to-end query)
- [x] 1.10 POC Checkpoint
- [x] 2.1 Parquet reader (CPU metadata + GPU decode)

## Current Task
Awaiting next task

## Learnings

- Comprehensive foreman-spec analysis already completed: PM.md (584 lines), UX.md (1024 lines), TECH.md (1673 lines), QA.md (1190 lines), OVERVIEW.md covering complete architecture with 110+ KB citations
- particle-system codebase provides direct reuse patterns: build.rs AOT compilation, #[repr(C)] struct layout tests, buffer allocation (alloc_buffer helper), indirect dispatch (DispatchArgs), compute encoder helpers (per-pass pattern)
- All analytics kernels are memory-bandwidth-bound (operational intensity well below M4 ridge point of ~24 FLOPS/byte) -- GPU advantage comes from massive bandwidth (100-546 GB/s) vs CPU (~60 GB/s)
- mmap+bytesNoCopy is HIGH risk (undocumented by Apple) but validated by MLX and llama.cpp -- fallback copy path needed (~1ms/GB)
- SSD bandwidth is the real cold-query bottleneck (3-7 GB/s), not GPU. Warm queries at memory bandwidth are 10-100x faster
- Parquet is the easiest format (already columnar), CSV is medium (GPU row detection), JSON is hardest (structural indexing ala GpJSON)
- Function constant specialization [KB #210, #202] eliminates branches at compile time -- 84% instruction reduction -- key to query-specific kernel performance
- No persistent kernels on Metal [KB #440] -- must use batched dispatches chained in command buffers, with 1GB batch limit for watchdog safety [KB #441]
- POC-first task structure: 10 tasks to prove end-to-end GPU query pipeline, then 11 core engine tasks, then 10 TUI tasks, then 7 test tasks, then 2 PR tasks
- build.rs handles zero .metal files gracefully by generating a stub shader; future tasks adding .metal files will automatically be compiled
- gpu-query is a standalone crate (no workspace), same as particle-system
- parquet crate v54 with arrow feature pulls in significant deps (~160 packages total); build takes ~41s cold
- Shared MSL types.h defines: FilterParams (40B), AggParams (16B), SortParams (16B), CsvParseParams (24B), DispatchArgs (12B), ColumnSchema (8B)
- Rust FilterParams uses actual types.h field names (compare_value_int, compare_value_float, row_count, column_stride, null_bitmap_present, compare_value_int_hi) not the task description names which were approximate
- End-to-end GPU query pipeline works: SQL parse → LogicalPlan → PhysicalPlan → GPU CSV parse → GPU filter → GPU aggregate → formatted result
- QueryExecutor connects all stages: ScanResult (mmap+CSV parse), FilterResult (bitmask+match_count), QueryResult (columns+rows)
- CPU-side schema inference from first data row (i64, f64, varchar detection) works for type-routing to correct GPU kernels
- All-ones bitmask builder for unfiltered aggregation avoids special-casing in aggregate kernels
- End-to-end verified: SELECT count(*), sum(amount) FROM sales WHERE amount > 100 → 5, 1600 (correct)
- GpuDevice is compute-only (no CAMetalLayer, no render pipeline) unlike particle-system's GpuState
- 16 layout tests verify all 5 struct sizes, alignments, and byte-level field offsets
- bytesNoCopy works on Apple Silicon with 16KB page-aligned mmap regions -- zero-copy confirmed (buffer.contents() points to same mmap'd data)
- newBufferWithBytesNoCopy requires NonNull<c_void>, mapped_len (page-aligned), MTLResourceOptions, and optional deallocator block -- pass None for deallocator when MmapFile owns the mapping
- tempfile crate added as dev-dependency for test temp files
- CSV delimiter detection uses frequency analysis across comma/tab/pipe candidates -- highest count wins, defaults to comma
- format_detect priority: PAR1 magic > JSON content ({/[) > extension-based > CSV default
- catalog.scan_directory is non-recursive (immediate children only), skips Unknown formats, sorts by table name
- 41 tests total for io::csv (17), io::format_detect (17), io::catalog (7)
- Metal Shading Language does NOT support `double` (64-bit float) -- must use `float` (32-bit) for GPU-side float columns. types.h FilterParams.compare_value_float changed to compare_value_float_bits (long) to avoid double in MSL
- GPU CSV parser is two-pass: (1) csv_detect_newlines scans all bytes in parallel for '\n', stores offsets via atomic_fetch_add; (2) csv_parse_fields processes one row per thread, splits by delimiter, parses int/float fields into SoA buffers
- Atomic newline offsets are unordered -- must sort on CPU between pass 1 and pass 2
- SoA stride in shader MUST match host-side ColumnarBatch.max_rows -- initially had a bug where shader used clamped num_data_rows but host allocated with larger max_rows. Fixed by using params.max_rows (= batch.max_rows) as soa_stride unconditionally
- GpuDevice.find_metallib() needed fix: test binaries run from target/debug/deps/ but build output is in target/debug/build/. Added parent directory search
- objc2-metal 0.3 requires explicit trait imports (MTLDevice, MTLBuffer, MTLCommandBuffer, MTLCommandEncoder, MTLComputeCommandEncoder, MTLComputePipelineState) for methods to be in scope
- MTLSize uses NSUInteger (= usize on 64-bit), not u64
- setBuffer_offset_atIndex is unsafe in objc2-metal 0.3, but dispatchThreadgroups_threadsPerThreadgroup is safe
- encode.rs helpers: make_pipeline, make_command_buffer, dispatch_1d, dispatch_threads_1d, alloc_buffer, alloc_buffer_with_data, read_buffer, read_buffer_slice
- storage module: RuntimeSchema (column names + types), ColumnarBatch (Metal buffers for SoA int/float columns), DataType enum (Int64/Float64/Varchar/Bool/Date)
- 75 tests total: 69 lib + 6 GPU CSV integration tests
- Metal function constants compile into metallib correctly via AOT (build.rs xcrun metal -c), but newFunctionWithName_constantValues_error must be used at runtime to specialize -- newFunctionWithName alone will find the function but without constants resolved
- MTLFunctionConstantValues.setConstantValue_type_atIndex takes NonNull<c_void> for value, MTLDataType for type, and NSUInteger for index; needs MTLDataType::UInt for uint constants and MTLDataType::Bool for bool constants
- After adding new .metal files, cargo clean may be needed if build.rs doesn't detect the new file (rerun-if-changed=shaders/ should catch it but incremental builds may cache the old metallib)
- PsoCache pattern: HashMap<PsoKey, Retained<ProtocolObject<dyn MTLComputePipelineState>>> keyed by function name + serialized constant values
- Bitmask output: 1-bit per row stored as uint32 words; atomic_fetch_or_explicit for thread-safe bit setting; simd_sum + atomic_fetch_add for efficient match counting
- 95 tests total: 73 lib + 6 GPU CSV integration + 16 GPU filter integration
- Metal simd_shuffle_down does NOT support long/int64 -- must split into int2 (lo/hi) halves, reduce each with simd_shuffle_down, and track carry for proper 64-bit addition
- aggregate_count: one thread per bitmask word, popcount + simd_sum + threadgroup reduce + atomic_fetch_add (all uint32 = simple)
- aggregate_sum_int64: one thread per row, 64-bit SIMD reduction via int2 halves with carry, threadgroup reduction via lo/hi uint arrays, global atomic via split lo/hi with carry propagation
- Metal lacks atomic_long, so 64-bit atomic accumulation requires split into two atomic_uint (lo + hi) with carry from lo overflow propagated to hi
- 115 tests total: 73 lib + 6 GPU CSV integration + 16 GPU filter integration + 20 GPU aggregate integration
- sqlparser 0.53: FunctionArg has Unnamed, Named, and ExprNamed variants; GroupByExpr::Expressions takes (exprs, modifiers); OrderBy has exprs field directly; no OrderByKind enum
- SQL module: 49 tests covering types (8), logical_plan (6), physical_plan (12), parser (23 including parse-to-physical round trips)
- PhysicalPlan has GpuCompoundFilter for AND/OR predicates -- both sides share same input scan, executor combines bitmasks
- 164 tests total: 122 lib + 6 GPU CSV integration + 16 GPU filter integration + 20 GPU aggregate integration
- POC CHECKPOINT PASSED: 1M-row GPU query verified against CPU reference -- SELECT count(*), sum(amount) FROM sales_1m WHERE amount > 500 → count=499403, sum=374769525 (exact match)
- POC performance: ~50ms wall time for 1M-row filtered aggregate query (12MB CSV) including process startup, Metal init, mmap, CSV parse, filter, aggregate
- Full pipeline proven: SQL string → sqlparser → LogicalPlan → PhysicalPlan → mmap zero-copy → GPU CSV parse (2-pass) → GPU filter (function constant specialization) → GPU aggregate (hierarchical SIMD reduction) → formatted result
- Phase 1 complete: 10/10 POC tasks done, 164 tests passing, end-to-end GPU query pipeline verified at scale
- parquet crate v54 writer API: SerializedColumnWriter has .typed::<Int64Type>().write_batch() and .close() -- no close_column on row group writer
- parquet crate v54 reader API: read_records takes &mut Vec<T> (not pre-allocated slices); Vec is extended internally by the reader
- Parquet reader approach: CPU reads metadata+column data via parquet crate, uploads to Metal buffers directly (no GPU decode kernels needed for plain encoding since CPU reader handles decompression)
- Column pruning works: read_columns accepts optional needed_columns list, only reads and returns requested columns
- execute_parquet_scan: reads all columns, writes directly to ColumnarBatch int_buffer/float_buffer at correct SoA offsets, reuses existing filter/aggregate GPU pipeline
- Metal shader parquet_decode.metal compiled but not yet used at runtime -- CPU-side decoding is sufficient for POC/Phase 2; GPU decode kernels ready for future optimization
- 177 tests total: 125 lib + 6 GPU CSV + 16 GPU filter + 20 GPU aggregate + 10 GPU Parquet
