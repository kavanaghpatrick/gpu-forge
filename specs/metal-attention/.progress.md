# Progress: metal-attention

## Completed Tasks
- [x] 1.1 Create workspace skeleton and crate structure - (pending commit)
- [x] 1.2 Define core trait hierarchy in traits crate
- [x] 1.3 Port Metal build system and shader compilation
- [x] 1.4 Port GPU device, PsoCache, buffer helpers from proto
- [x] 1.6 Wire flash + linear attention dispatch with GPU correctness tests
- [x] 1.7 Implement RMSNorm, FFN, embedding, matmul kernels

- [x] 2.1 Implement GGUF parser with mmap
- [x] 2.2 Implement GGUF tokenizer + dequantization kernels
- [x] 2.4 RWKV-7 model implementation with LinearSequenceModel trait
- [x] 2.5 Inference pipeline + sampling engine
- [x] 2.6 CLI run command with streaming output

- [x] 3.2 Llama/Mistral pure transformer model
- [x] 3.4 HybridModel dispatch + Jamba model
- [x] 3.5 PagedAttention V2 + GPU prefix sum

- [x] 4.1 CLI bench and info commands

- [x] 5.1 Property-based tests with proptest
- [x] 5.2 GPU correctness test sweep
- [x] 5.3 Integration + E2E tests - 587bcf4

## Current Task
Awaiting next task

## Learnings
- Proto has 8 validated prototypes with working shaders, device.rs, pipeline.rs, types.rs -- direct port targets
- Proto build.rs handles stub metallib when no shaders present -- useful pattern to carry forward
- Proto Cargo.toml uses objc2 0.6, objc2-metal 0.3, block2 0.6 -- exact versions to match
- Proto GpuDevice uses OnceLock singleton with multi-path metallib search (OUT_DIR, build dirs, exe-relative)
- Proto AttentionParams is exactly 64 bytes with 4-byte alignment, 16 u32/f32 fields -- must match types.h
- Proto has cpu_attention_f64 and cpu_linear_attention_f64 in proto1_flash.rs and proto6_fla.rs -- port to test utils
- Proto encode.rs has buffer allocation helpers (alloc_buffer_with_data, readback) -- port to buffer.rs
- Proto pipeline.rs has PsoCache with function constant specialization -- port + add prewarm()
- Proto is NOT in workspace (standalone crate) -- must NOT be added as workspace member
- GGUF parser must handle: 4-byte magic, version u32, tensor_count u64, metadata_kv_count u64, little-endian
- GGUF quantization: Q4_0 (32 elements/block), Q4_K_M, Q8_0, F16, F32 -- block sizes vary
- RWKV-7 GGUF uses `rwkv.*` prefix for metadata, `blk.N.channel_mixing.*` / `blk.N.time_mix_*` for tensors
- Jamba GGUF uses `jamba.ssm.*` and `jamba.attention.*` prefixes
- Flash attention at 0.16 TFLOPS with single simdgroup -- multi-simdgroup needed for >1 TFLOPS target
- Linear attention 50-70x faster than softmax at N=1024 -- critical advantage for hybrid models
- GPU prefix sum eliminates ~300us CPU bottleneck in linear attention path
- 32KB threadgroup memory hard limit across all M-series -- constrains tile sizes
- Function constants: 0% GPU overhead, 178ns cache hit, 34-63us cold compile
- MTL_SHADER_VALIDATION=1 required for all test runs, --test-threads=1 for GPU tests
- Task count: 27 tasks across 6 phases (within 25-35 target)
- Workspace root Cargo.toml is also the CLI binary crate (metal-attention-cli) -- proto/ excluded via workspace.exclude not needed since proto has its own Cargo.toml and isn't a workspace member
- Traits crate: TECH.md uses BlockConfig param (not GpuContext) for SequenceBlock methods -- keeps traits pure Rust
- DType::Q4_K_M needs #[allow(non_camel_case_types)] since the name matches GGUF quantization format convention
- build.rs compiles 12 .metal shaders to single shaders.metallib via xcrun metal + xcrun metallib
- METALLIB_PATH exported as cargo:rustc-env for runtime metallib discovery
- Debug builds get -gline-tables-only -frecord-sources for GPU debugger support
- types.h extended with LayerParams (transformer block params) and SSMParams (Mamba/Jamba params), each 64 bytes/4-byte aligned matching AttentionParams pattern
- Q4_0 dequantization: 18 bytes/block = 2 (half scale) + 16 (32 nibbles), values centered at -8
- dispatch crate v0.2 provides GCD Semaphore; its new() takes u32 not isize
- MTLCommandBufferHandler is *mut DynBlock<dyn Fn(NonNull<ProtocolObject<dyn MTLCommandBuffer>>)> -- use RcBlock::as_ptr() to pass
- PSO cache tests must use real kernel names from shaders.metallib (e.g. "matmul"), not "_stub" since shaders/ has real .metal files
- BufferPool uses power-of-two size buckets with 4KB minimum (Metal page alignment)
- env!("METALLIB_PATH") works for compile-time metallib discovery set by build.rs rustc-env
- Flash attention dispatch: PsoKey indices 0=HEAD_DIM, 1=BLOCK_R, 2=BLOCK_C, 4=ALIBI_ENABLED; 32 threads per threadgroup (1 simdgroup)
- Linear attention dispatch: chunk_h index 0=HEAD_DIM, 4=CHUNK_SIZE; chunk_o same; CPU prefix sum between passes
- MTLCommandQueue trait must be imported for .commandBuffer() method to be available on Retained<ProtocolObject<dyn MTLCommandQueue>>
- GPU flash attention correctness: atol=5e-3 passes at N=64 and N=256 with D=64
- GPU linear attention correctness: atol=1e-3 passes at N=64 and N=256 with D=64, chunk_size=32
- All 5 dispatch modules (norm, ffn, embed, matmul, dequant) follow flash.rs pattern: alloc buffers, set PSO, encode, dispatch, readback
- MTLCommandQueue and MTLComputePipelineState traits must be imported in each dispatch module for .commandBuffer() and .maxTotalThreadsPerThreadgroup()
- Matmul uses function constants (MAT_M/MAT_N/MAT_K at indices 0/1/2), other kernels use LayerParams buffer
- FFN silu kernel takes gate+up buffers (input buffer unused placeholder), output = silu(gate) * up
- half crate v2 added as dev-dependency for Q4_0 dequantization CPU reference in tests
- GPU correctness: rmsnorm atol=1e-4, ffn_silu atol=1e-5, embedding exact, matmul atol=1e-4, dequant atol=1e-3
- GGUF from_bytes() needs unique temp file per thread (atomic counter + PID) to avoid race conditions in parallel tests
- GgufBuilder produces valid GGUF v2/v3 binary format: magic + version + tensor_count(u64) + metadata_kv_count(u64) + KV pairs + tensor infos + aligned data section
- GGUF string format: u64 length + bytes (NOT null-terminated)
- GgufType block sizes: Q4_0=32/18B, Q8_0=32/34B, Q4_K_M=256/144B, Q6_K=256/210B
- Architecture detection: primary from general.architecture metadata, fallback from tensor name patterns (attn_q -> Llama, time_mix_ -> RWKV, ssm_ + attn_ -> Jamba, rglru_ -> Griffin)
- GgufTokenizer: BPE from GGUF metadata -- char-level split then iterative lowest-rank merge pair
- GgufBuilder.add_f32_array() and GgufMetadata.get_array_f32() added for tokenizer score arrays
- Q8_0 dequantization: 34 bytes/block = 2 (half scale) + 32 (int8 values), value = q * scale
- Metal `char` type is signed int8 -- use `device const char*` cast for Q8_0 quants in shader
- RWKV WKV kernel uses sequential token processing (recurrent) with per-element state update + output dot product
- WKV shader uses buffer(6)/buffer(7) for scalar params (seq_len, head_dim) instead of AttentionParams struct
- GPU WKV correctness: atol=1e-3 for output and state vs CPU reference at D=16, seq_len=4
- Adding new .metal shader requires `cargo clean -p metal-attention-kernels` to rebuild metallib (build.rs caches)
- Rwkv7Block uses CPU matvec for projections + GPU rwkv_wkv kernel for state update -- full GPU path in later phase
- Model registry maps ModelArchitecture::Rwkv -> Rwkv7Block construction
- HybridModel uses enum-based ModelLayer dispatch (not dyn SequenceBlock) because associated types prevent object safety
- Inference loop: embed -> per-layer process_token -> rmsnorm -> lm_head matvec -> sample
- SimpleRng (xorshift64) avoids external rand dependency for deterministic sampling
- Sampling: softmax uses max-subtraction for numerical stability; repetition penalty divides positive logits, multiplies negative
- 32 tests in metal-attention crate: config(1) + sampling(14) + model(7) + inference(9), all GPU-validated
- CLI binary is metal-attention-cli (root Cargo.toml package name) -- not metal-attention
- metal-attention-gguf and metal-attention-models added as direct CLI deps for GGUF loading + registry access
- GGUF metadata keys: rwkv.embedding_size, rwkv.num_heads, rwkv.block_count for RWKV model config extraction
- LlamaLayer wraps FlashAttentionLayer + SwiGLU FFN + 2x RMSNorm -- forward is norm->attn->residual->norm->ffn->residual
- ModelConfig now includes num_kv_heads field for GQA support (Llama/Mistral use grouped-query attention)
- ModelLayer::Llama variant added to enum dispatch in HybridModel -- let-else pattern for state matching
- SwiGLU FFN: gate/up projections to intermediate_size (4x hidden), silu activation on gate, element-wise multiply with up, down projection back
- 8 new llama tests: forward decode, prefill, weight mapping, SwiGLU, trait compliance, residual connections, registry create/wrong-arch
- main.rs extracts num_kv_heads from GGUF metadata key "general.num_kv_heads" with fallback to num_heads
- SSM scan kernel: h[m][s] = A[m]*h[m][s] + B[s]*x[m], y[m] = sum_s(C[s]*h[m][s]) + D*x[m] -- matches Mamba selective scan
- SSM scan GPU uses 10 buffers (x,A,B,C,D,state,output,seq_len,d_model,d_state) -- no function constants needed
- GPU SSM scan correctness: atol=1e-3 for output and state vs CPU reference at d_model=16, d_state=4, seq_len=4
- MambaBlock: input_proj -> SiLU gate -> element-wise multiply -> SSM scan -> output_proj (conv1d skipped for POC)
- Mamba d_inner = 2*hidden_size (standard expansion), A uses sigmoid of learned log-space values for (0,1) decay
- JambaLayer enum: Mamba{block,moe} or Attention(LlamaLayer) -- determined by LayerSchedule::periodic(n,7)
- MoE routing: top-2 expert selection with softmax gating over router logits, weighted sum of 2 SwiGLU expert outputs
- Jamba 16 layers: 14 Mamba + 2 Attention (at indices 7,15) -- verified in test_jamba_layer_schedule_7_1
- ModelLayer enum extended with Mamba and Jamba variants; LayerState extended with Mamba and Jamba variants
- SequenceBlock trait must be imported (use metal_attention_traits::sequence::SequenceBlock) in jamba.rs for init_state() method resolution
- All 176 workspace tests pass with MTL_SHADER_VALIDATION=1 after adding ssm_scan.metal shader + mamba + jamba modules
- PagedKVCache: page pool pre-allocated, free list stack, block_table maps logical->physical pages, append auto-allocates new pages
- Paged attention shader uses interleaved KV layout: [phys_page, 2, page_size, head_dim] with K then V per page
- interleave_kv_pages() helper converts separate k_pages/v_pages to interleaved format for shader
- Prefix sum shader: each thread handles one D*D element position, sequential scan along N dimension (simple but correct)
- GPU paged attention matches CPU reference (atol=5e-3), paged vs dense GPU match (atol=1e-2)
- 47 unit + 21 integration = 68 tests pass in kernels crate after adding paged + prefix_sum modules

- Info command parses GGUF metadata only (no model load), reports arch/layers/hidden/heads/quant/vocab/memory
- Bench command uses generate_streaming with timing -- measures time-to-first-token as prefill proxy
- Bench --synthetic flag creates HybridModel::random without needing GGUF file
- detect_dominant_quant() finds most common tensor type excluding F32 (norms/biases)
- Criterion bench stubs in benches/attention.rs and benches/inference.rs with harness=false in Cargo.toml
- CLI binary verify: `metal-attention-cli info --help` and `metal-attention-cli bench --help` (not `metal-attention`)
- Linear attention GPU kernel requires chunk_size=32 (matches existing tests); chunk_size=64 produces incorrect results due to shader function constant expectations
- Flash attention sweep passes at N=64,128,256,512 with D=64, atol=5e-3
- Linear attention sweep passes at N=64,128,256 with D=64, chunk_size=32, atol=1e-3
- Tolerance constants: FLASH_ATOL=5e-3, LINEAR_ATOL=1e-3, ROPE_ATOL=1e-4, GQA_ATOL=1e-6 (from QA spec)
- 36 total correctness tests (was 22): +4 flash sweep, +3 linear sweep, +5 edge cases, +2 finiteness, +1 tolerance consistency

- proptest workspace dep already in root Cargo.toml; added to kernels + metal-attention dev-deps
- Property tests run CPU-only: 10 kernel tests (flash, linear, rmsnorm, embedding, rope) + 7 sampling tests
- prop_assert_eq! macro doesn't support inline {var} format captures -- must use positional {} args
- RoPE at position 0 is identity: angle=0 for all pairs, cos(0)=1, sin(0)=0
- RoPE preserves L2 norm (rotation is unitary): verified within 1e-3 tolerance

- Integration tests cover GGUF parsing (minimal, metadata types, multiple tensors), architecture detection (explicit + tensor patterns), tensor-to-buffer mapping, and error handling (bad magic, unsupported version, truncated data)
- E2E tests validate full inference pipeline: prefill+decode loop, deterministic generation, streaming callbacks, memory stability (50 iterations), all sampling strategies (greedy, temperature, top-k, top-p, repetition penalty)
- Tested architectures: RWKV-7 (pure linear), Llama (pure transformer), hybrid (2 RWKV + 2 Llama layers)
- All 28 integration + e2e tests pass with MTL_SHADER_VALIDATION=1
- Test uses synthetic GGUF models (via GgufBuilder) since no real GGUF models available
- Memory stability test: 50 inference iterations, all tokens valid, no crashes
- Dev-dependency metal-attention-traits added to root Cargo.toml for BlockConfig access in tests

## Next
Task 5.4 or next assigned task
