# KB Impact Audit: gpu-particle-system

**Auditor**: Claude Opus 4.6
**Date**: 2026-02-10
**Method**: Cross-referenced every KB ID cited in `research.md` against final source code in `particle-system/`

---

## Summary

The research.md cites **28 distinct KB IDs** plus several skill-level ranges. Of these:

| Rating | Count | % |
|--------|-------|---|
| CRITICAL | 4 | 14% |
| USEFUL | 10 | 36% |
| BACKGROUND | 11 | 39% |
| MISLEADING | 3 | 11% |

**Bottom line**: ~50% of cited findings materially influenced the implementation (CRITICAL + USEFUL). The other half was either general context or inapplicable to what was actually built. The most impactful findings were about atomic counter layout (KB 190), unified memory storage mode (KB 153), threadgroup sizing (KB 259), and the ping-pong pattern (KB 232). The most notable miss was KB 232 (triple buffering) -- cited as a key recommendation but the implementation uses single buffering with a semaphore count of 1.

---

## Per-Finding Analysis

### KB 190 — Append buffer lock-free pattern: atomic_uint at buffer[0], 16-byte aligned

**Claim**: Embed `atomic_uint` at buffer[0] with 16-byte alignment to avoid serialization bottlenecks in lock-free append buffers.

**Verdict**: **CRITICAL**

**Evidence**: This is the exact pattern used across every buffer in the system. The `CounterHeader` struct (types.rs:141-147, types.h:55-63) places a `uint` counter at offset 0 with 12 bytes of padding to achieve 16-byte alignment. The `COUNTER_HEADER_UINTS = 4` constant offsets all index access by 4 uints (16 bytes). Every shader (emission.metal:55-56, update.metal:147-153, grid.metal:83) casts `buffer[0]` to `device atomic_uint*` and uses `atomic_fetch_add_explicit` / `atomic_fetch_sub_explicit` with `memory_order_relaxed`.

Without this finding, the naive approach would be a separate atomic counter buffer or misaligned atomics causing performance issues. This finding directly determined the buffer layout that is the structural backbone of the entire system.

---

### KB 150 — ICB kernel chaining requires separate encoder passes

**Claim**: Indirect Command Buffers still require separate encoder passes on Metal. True persistent kernels are not feasible.

**Verdict**: **USEFUL**

**Evidence**: The implementation uses separate `computeCommandEncoder()` calls for each kernel (main.rs:213, 245, 263, 286, 318) followed by `endEncoding()`, all within a single command buffer. This matches the "separate encoder passes within one command buffer" pattern exactly. However, the system doesn't use ICBs at all -- it uses a simple `sync_indirect_args` compute kernel (render.metal:101-112) to copy the alive count into draw args, then calls `drawPrimitives_indirectBuffer_indirectBufferOffset`. This is simpler than full ICB chaining.

The finding confirmed that the "one command buffer, multiple encoders" approach was correct, but the team would likely have discovered this anyway since it's the standard Metal pattern.

---

### KB 232 — Triple buffering: 3 pre-allocated buffers with dispatch_semaphore(3)

**Claim**: Canonical Metal pattern -- 3 buffers in ring with `dispatch_semaphore(3)`. Two buffers can cause stalls; three balances parallelism, memory, and latency.

**Verdict**: **MISLEADING**

**Evidence**: The implementation explicitly **rejected** triple buffering. `frame.rs:19` sets `MAX_FRAMES_IN_FLIGHT = 1` with the comment: "Using 1 (single buffering) because particle SoA buffers and alive/dead lists are shared (not per-frame). Multiple in-flight frames would race on these buffers."

The research.md recommendation #3 says "Use triple buffering with dispatch_semaphore(3)" -- but this was impossible without tripling all particle buffers (positions, velocities, lifetimes, colors, sizes, dead list, alive lists A+B = 9 buffers, each up to 120MB at 10M particles). At 10M particles, triple buffering would require ~3x the memory budget (1GB+ on a 16GB machine). The semaphore infrastructure exists (FrameRing uses `DispatchSemaphore`) but with count 1, not 3.

This KB finding was correctly cited but led to a recommendation that was architecturally wrong for this specific use case. The shared mutable state across all SoA buffers makes triple buffering impractical without partitioned or per-frame buffer copies.

---

### KB 279 — waitUntilCompleted() anti-pattern

**Claim**: `waitUntilCompleted()` blocks CPU, creates GPU bubbles. Use completion handlers or semaphore signaling.

**Verdict**: **USEFUL**

**Evidence**: The implementation uses completion handler signaling (frame.rs:89-101). `register_completion_handler` creates an `RcBlock` that calls `semaphore.signal()` on GPU completion, and `acquire()` blocks on `semaphore.wait()`. Zero instances of `waitUntilCompleted()` anywhere in the codebase.

This is a correct application of the finding. However, `waitUntilCompleted()` is such a well-documented anti-pattern that any Metal developer would avoid it. Rated USEFUL rather than CRITICAL because the completion handler + semaphore pattern is standard.

---

### KB 283 — Excessive global atomics bottleneck scales with core count

**Claim**: Atomic bottleneck scales with GPU core count (especially M4 Pro 20 cores). Use threadgroup atomics + reduction pattern.

**Verdict**: **USEFUL** (partially followed)

**Evidence**: The system uses global atomics extensively:
- Dead list: `atomic_fetch_sub_explicit` in emission.metal:56
- Alive list push: `atomic_fetch_add_explicit` in emission.metal:123, update.metal:148,153
- Grid density: `atomic_fetch_add_explicit` in grid.metal:83

The grid.metal shader has an extensive design note (lines 29-47) that directly analyzes whether threadgroup atomics would help. It concludes that the 64^3 = 262K bin count makes threadgroup histograms impossible (would require 1MB threadgroup memory vs ~32KB limit). For the alive/dead list atomics, the system uses a single global counter per list, which is a serialization point -- but the contention is bounded (one atomic per thread, not multiple).

The finding was consulted and thoughtfully rejected for the grid case. For the alive/dead lists, it was not mitigated -- each thread still does a single global atomic. Whether this matters depends on particle count and core count. The finding prompted a real engineering analysis (visible in grid.metal comments) even though the conclusion was "single-phase is fine here."

---

### KB 121 — Memory barriers after fragment stage are extremely expensive

**Claim**: Place barriers between compute passes, not after fragment stage.

**Verdict**: **BACKGROUND**

**Evidence**: The implementation has no explicit memory barriers at all. Metal handles inter-encoder ordering implicitly when using separate encoder passes within a single command buffer (which is what the code does). The pipeline order is: emission compute -> grid clear compute -> grid populate compute -> physics update compute -> sync_indirect compute -> render. All barriers are implicit via encoder boundaries.

This finding didn't change anything because the architecture naturally avoided the anti-pattern by using the standard "separate encoders, one command buffer" approach.

---

### KB 181 — Avoid dynamic indexing in thread address space; use constant or device

**Claim**: Dynamic indexing in thread address space should be avoided; use `constant` or `device` address space.

**Verdict**: **BACKGROUND**

**Evidence**: All shader buffer parameters use `device` or `constant` address space exclusively:
- `constant Uniforms&` for read-only uniforms
- `device uint*`, `device packed_float3*`, `device half2*`, etc. for read-write particle data
- `device const uint*` for read-only alive lists in update.metal

No thread-address-space dynamic indexing exists. However, the particle system's data access pattern (index via `particle_idx` into SoA arrays) naturally requires `device` address space. There's no scenario where thread-space indexing would have been tempting. This finding is correct but irrelevant to the actual architecture.

---

### KB 134 — Empty kernel round-trip: ~150-250us baseline overhead per dispatch

**Claim**: Each dispatch incurs ~150-250us baseline overhead even for an empty kernel.

**Verdict**: **USEFUL**

**Evidence**: The pipeline has 6 dispatches per frame: emission, grid_clear, grid_populate, update_physics, sync_indirect, render. At ~200us per dispatch overhead, that's ~1.2ms of fixed overhead per frame (7.2% of a 16.6ms frame budget at 60fps).

This finding is directly relevant to the `sync_indirect_args` kernel (render.metal:101-112), which dispatches 1 threadgroup of 1 thread just to copy a single uint. Knowing the baseline cost means we know this kernel costs ~200us regardless of its trivial workload. This influenced the decision to accept 6 passes rather than trying to merge kernels further. It also explains why the grid_clear kernel is kept separate rather than doing a memset from CPU (the overhead is already paid).

---

### KB 259 — Register budget: <=104 registers -> 2 SIMD groups/core; <=52 -> 4 (max occupancy)

**Claim**: Register count determines occupancy. <=52 registers gets 4 SIMD groups per core (maximum).

**Verdict**: **USEFUL**

**Evidence**: The update_physics kernel is the most register-heavy (multiple float3s, loop variables, atomic operations). The research recommended keeping register count low, and the implementation uses mixed precision (FP16 for lifetime/color/size, FP32 for position/velocity) which reduces register pressure. The types.rs:222-228 comment explicitly calculates "38B per particle (vs 96B all-FP32) = 60% bandwidth reduction" -- but this also means fewer registers needed for FP16 values (half vs float).

However, there's no evidence of actual register profiling (no Metal GPU Profiler output). The kernel was never empirically tuned against this threshold. The finding informed the mixed-precision design rationale but was never verified against the actual compiled kernel register count.

---

### KB 310 — SIMD width: 32 threads per simdgroup

**Claim**: Apple Silicon SIMD width is 32 threads.

**Verdict**: **BACKGROUND**

**Evidence**: Every compute dispatch uses threadgroup size 256 (main.rs:232, 253, 273, 307), which is 8 SIMD groups of 32. This is a sensible choice, but the code never uses SIMD-specific intrinsics (no `simd_shuffle`, `simd_sum`, `simd_prefix_exclusive_sum`, etc.). The threadgroup size of 256 is a standard choice that any Metal developer would use regardless of knowing the exact SIMD width.

---

### KB 314 — SIMD shuffle bandwidth: 256 bytes/cycle per core (2x NVIDIA)

**Claim**: Apple Silicon has 2x NVIDIA's SIMD shuffle bandwidth.

**Verdict**: **BACKGROUND**

**Evidence**: No SIMD shuffle operations are used anywhere in the shaders. The system relies entirely on global atomics for inter-thread communication (dead/alive list management) and independent per-particle computation. This bandwidth advantage is completely irrelevant to the current implementation.

---

### KB 265 — Divergence penalty: ~70 cycles for branch divergence

**Claim**: Branch divergence costs ~70 cycles on Apple Silicon.

**Verdict**: **USEFUL**

**Evidence**: The update kernel has several branches: `if (tid >= alive_count) return` (early exit), the 3x3x3 grid neighborhood loop with boundary checks, mouse attraction radius check, boundary bounce (6 branches), and the final `if (age >= max_age)` dead/alive split. The emission kernel has divergent `is_burst` branching (emission.metal:73-75).

Research recommendation #6 suggested "Use function constants for kernel variants (emit vs. update vs. cull) -- avoids divergence." This was NOT implemented. All kernels use runtime branching. The emission kernel's burst vs. normal path is particularly divergent since burst threads (first N) take a different code path than normal threads.

The finding was cited but the recommendation to use function constants was ignored. Given the system works fine at 1M+ particles, the divergence penalty may not be significant enough to justify the complexity of multiple kernel variants.

---

### KB 263 — 16-bit types: half/ushort use 2x fewer registers -> 2x occupancy boost

**Claim**: FP16 types halve register usage and double occupancy.

**Verdict**: **CRITICAL**

**Evidence**: This finding directly shaped the mixed-precision SoA architecture, which is arguably the most distinctive design decision in the system:
- `half2` for lifetimes (4B vs 8B) -- update.metal:70, emission.metal:108
- `half4` for colors (8B vs 16B) -- update.metal:142, emission.metal:114, render.metal:15
- `half` for sizes (2B vs 4B) -- emission.metal:119, render.metal:51
- FP32 only for positions/velocities (physics precision)

The update.metal comment block (lines 33-44) explicitly cites the dual-issue FP16+FP32 capability: "On Apple Silicon Family 9 (M3/M4), FP16 and FP32 ALU ops can dual-issue in parallel, so half-precision intermediates for color/size interpolation run 'for free' alongside float physics computations."

The render.metal VertexOut uses `half4 color` instead of `float4`, and the fragment shader returns `half4`. The types.rs BufferSizes comment documents the savings: "38B per particle (vs 96B all-FP32) = 60% bandwidth reduction."

This finding fundamentally shaped the data architecture and is embedded throughout every shader and the Rust buffer allocation code.

---

### KB 266 — Cache line: 128 bytes

**Claim**: Apple Silicon cache line is 128 bytes.

**Verdict**: **BACKGROUND**

**Evidence**: No code explicitly accounts for 128-byte cache line alignment. The SoA layout naturally benefits from sequential access within each attribute array, but there's no padding to cache line boundaries, no explicit alignment directives, and no mention of 128-byte alignment in the code.

---

### KB 268 — No severe penalty for uncoalesced access on Apple Silicon

**Claim**: Unlike NVIDIA/AMD, Apple Silicon tolerates uncoalesced memory access without severe penalty, though coalesced is still better.

**Verdict**: **USEFUL**

**Evidence**: The SoA layout was chosen (research recommendation #1: "Lock particle struct layout early -- SoA with FP16 where possible to minimize bandwidth"), and the code uses SoA throughout. However, the access pattern is inherently scattered: each thread reads `positions[particle_idx]` where `particle_idx` comes from the alive list, so indices are not sequential. This means memory access is fundamentally uncoalesced.

This finding validated that the scattered-index-into-SoA pattern would still perform acceptably on Apple Silicon. On NVIDIA, this access pattern would be devastating. On Apple Silicon, it works. This finding gave confidence to proceed with the alive-list-indexed SoA layout without adding a compaction/sort pass to improve coalescing.

---

### KB 269 — Threadgroup memory: 32 independent banks with classical bank conflict patterns

**Claim**: Apple Silicon threadgroup memory has 32 banks with classical bank conflicts.

**Verdict**: **BACKGROUND**

**Evidence**: No threadgroup memory is used in any shader. All data flows through global `device` memory. The grid.metal comment (lines 29-47) explicitly considered using threadgroup memory for histogram bins but rejected it due to the 262K bin count exceeding the ~32KB limit. Since no threadgroup memory is used, bank conflicts are moot.

---

### KB 114 — Threadgroup memory: ~60KB per core tile SRAM, purely on-chip

**Claim**: ~60KB threadgroup memory per core, on-chip SRAM.

**Verdict**: **USEFUL**

**Evidence**: The grid.metal design note (line 34) references this constraint: "262K x 4B = 1 MB of threadgroup memory per threadgroup -- far exceeding Apple Silicon's ~32 KB threadgroup memory limit." The actual finding says ~60KB, but the code conservatively cites ~32KB. Either way, this finding was actively used in the decision analysis for grid.metal's single-phase atomic approach. It prevented an architectural mistake (attempting threadgroup histograms that wouldn't fit).

---

### KB 153 — StorageModeShared: zero-copy CPU<->GPU access with hardware cache coherence

**Claim**: StorageModeShared provides zero-copy CPU/GPU access on unified memory Apple Silicon.

**Verdict**: **CRITICAL**

**Evidence**: Every buffer allocation in the system uses `MTLResourceOptions::StorageModeShared` (buffers.rs:66). This enables:
1. CPU-side dead list initialization (buffers.rs:134-157)
2. CPU-side alive list counter reading for HUD display (buffers.rs:311-315)
3. CPU-side uniforms writing every frame (main.rs:140-166)
4. CPU-side pool growing with memcpy (buffers.rs:229-292)
5. GPU-side atomic operations on the same buffers

The entire system architecture depends on StorageModeShared for CPU+GPU shared access without explicit copies. If this finding were wrong (or StorageModeShared had hidden performance penalties), the system would need a completely different buffer management strategy with explicit upload/readback buffers.

---

### KB 125 — All memory coherent between CPU and GPU without explicit cache flushes

**Claim**: Apple Silicon unified memory is coherent without manual cache flushes.

**Verdict**: **USEFUL**

**Evidence**: The system writes uniforms on CPU (main.rs:140-166) and reads them immediately on GPU in the same frame. The alive count is written by GPU atomics and read by CPU for HUD display (main.rs:383). No cache flush calls, no `didModifyRange`, no synchronization primitives between CPU write and GPU read (beyond the semaphore for frame pacing).

This finding enabled the simple "write on CPU, dispatch on GPU, read back on CPU" flow without intermediate synchronization. Combined with KB 153, these two findings are the foundation of the zero-copy architecture.

---

### KB 113 — SLC bandwidth: ~2x DRAM bandwidth per core

**Claim**: System Level Cache gives ~2x DRAM bandwidth when data is hot.

**Verdict**: **BACKGROUND**

**Evidence**: No code explicitly tries to optimize for SLC residency. The particle data at 10M particles (380MB+ per frame read+write) is far too large for SLC (M4 SLC is ~16-32MB estimated). At 1M particles (~38MB), partial SLC residency might help, but there's no explicit tuning for it. The code doesn't batch work to improve cache locality or stage data to fit in SLC.

---

### KB 101 — STREAM benchmark: M4 GPU achieves ~100 GB/s (83% of theoretical 120 GB/s peak)

**Claim**: M4 achieves ~100 GB/s sustained memory bandwidth.

**Verdict**: **USEFUL**

**Evidence**: The bandwidth budget analysis in research.md (lines 147-152) uses exactly this number: "10M x 64B x 2 = 1.28 GB per frame, at 60fps = 76.8 GB/s sustained, M4 @ 100 GB/s = feasible with ~23% headroom."

This finding was the basis for the entire feasibility assessment. It confirmed that 10M particles at 60fps was achievable on the base M4. Without this number, the project might have targeted a lower particle count or required M4 Pro minimum. The implementation starts at 1M and allows scaling to 10M via keyboard (input.rs:232-238: digit keys 1/2/5/0 -> 1M/2M/5M/10M), which aligns with the "start at 1M, scale to 10M" recommendation from research.

---

### KB 141 — Metal Synchronization Hierarchy: MTLFence, threadgroup_barrier, mem_flags

**Claim**: Three-level synchronization: MTLFence (cross-encoder), threadgroup_barrier (within threadgroup), mem_flags.

**Verdict**: **BACKGROUND**

**Evidence**: The implementation uses none of these explicitly:
- No `MTLFence` (Metal handles ordering implicitly between encoders in the same command buffer)
- No `threadgroup_barrier()` (no threadgroup-shared computation)
- No explicit `mem_flags` on barriers

The system relies entirely on implicit ordering from separate encoder passes and `memory_order_relaxed` on atomics. The synchronization hierarchy is theoretically important but was never needed because the architecture avoids all the patterns that require explicit synchronization.

---

### KB 167 — Metal 4 Barriers: Producer/Consumer barriers between passes

**Claim**: Metal 4 introduces explicit producer/consumer barriers.

**Verdict**: **BACKGROUND**

**Evidence**: The implementation uses Metal 3 APIs (standard `computeCommandEncoder()`, `renderCommandEncoderWithDescriptor()`, `endEncoding()`). No Metal 4 APIs are used. The research.md explicitly noted (line 121): "Some newer Metal 4 APIs may not yet have bindings" in objc2-metal. Metal 4 barriers were irrelevant to the implementation.

---

### KB 137 — Submit 1 command buffer per frame (max 2)

**Claim**: Best practice is 1 command buffer per frame, containing multiple encoder passes.

**Verdict**: **CRITICAL**

**Evidence**: The implementation uses exactly 1 command buffer per frame (main.rs:199-206: `gpu.command_queue.commandBuffer()`) containing 5 compute encoder passes + 1 render encoder pass, followed by a single `commit()` (main.rs:369). This is the exact pattern recommended.

This finding prevented a common mistake: creating separate command buffers per kernel dispatch. With 6 passes per frame, using 6 command buffers would incur ~6x the command buffer creation/commit overhead plus lose implicit inter-encoder ordering guarantees. The 1-CB-per-frame pattern is foundational to the frame pipeline's correctness and performance.

---

### KB 259-265 (range) — Occupancy findings

**Claim**: Various occupancy-related findings for Apple Silicon.

**Verdict**: **BACKGROUND** (as a range)

**Evidence**: Beyond KB 259 (register budget, rated USEFUL above) and KB 263 (FP16 occupancy, rated CRITICAL above) and KB 265 (divergence, rated USEFUL above), the remaining findings in this range (260-262, 264) are not individually cited and provided general occupancy context. No occupancy profiling was performed on the actual compiled kernels.

---

### KB 266-270 (range) — Memory access findings

**Claim**: Cache lines, coalescing, bank conflicts, access patterns.

**Verdict**: **BACKGROUND** (as a range)

**Evidence**: Beyond KB 266 (128B cache lines, BACKGROUND), KB 268 (uncoalesced tolerance, USEFUL), and KB 269 (bank conflicts, BACKGROUND), KB 267 and 270 are not individually cited and had no visible impact.

---

### KB 281 — Bandwidth benchmarks

**Claim**: GPU bandwidth benchmark data.

**Verdict**: **BACKGROUND**

**Evidence**: Not individually cited beyond the range mention. The feasibility analysis used KB 101 (STREAM benchmark) instead for the specific 100 GB/s number.

---

## Findings Not Cited But Relevant

The following patterns appear in the code but have no KB citation trail:

1. **PCG hash PRNG** (prng.metal): No KB finding cited for the choice of PCG hash. This is a standard GPU PRNG from the broader community, not from the gpu-forge KB.

2. **Billboard quad rendering** (render.metal): The camera-facing billboard approach (extracting right/up from view matrix) has no KB citation. Standard graphics technique.

3. **Semi-implicit Euler integration** (update.metal:118): No KB guidance on integrator choice. Standard game physics.

4. **Pool growth via CPU memcpy** (buffers.rs:207-308): No KB finding on dynamic pool resizing. The grow() implementation is a straightforward CPU-side buffer reallocation.

---

## Impact Summary by Category

### CRITICAL (4 findings)
| KB ID | Finding | How It Shaped the Code |
|-------|---------|----------------------|
| 190 | Atomic counter at buffer[0], 16-byte aligned | Entire buffer layout architecture (CounterHeader, COUNTER_HEADER_UINTS) |
| 263 | FP16 types halve registers, double occupancy | Mixed-precision SoA: half2/half4/half for visuals, float3 for physics |
| 153 | StorageModeShared = zero-copy CPU<->GPU | Every buffer uses SharedStorage; enables CPU init, readback, grow() |
| 137 | 1 command buffer per frame, multiple encoders | Frame pipeline: 1 CB with 6 encoder passes, single commit() |

### USEFUL (10 findings)
| KB ID | Finding | How It Helped |
|-------|---------|---------------|
| 150 | ICB requires separate encoders | Confirmed multi-encoder-in-one-CB approach |
| 279 | waitUntilCompleted anti-pattern | Drove completion handler + semaphore pattern |
| 283 | Global atomics bottleneck at scale | Prompted grid.metal design analysis (kept single-phase with justification) |
| 134 | ~200us per empty dispatch | Informed 6-pass overhead budget, justified keeping sync_indirect_args |
| 259 | Register budget -> occupancy | Informed FP16 decision for register savings |
| 265 | ~70 cycle divergence penalty | Cited but function constants NOT implemented; minor concern |
| 268 | No severe uncoalesced penalty on Apple Silicon | Validated scattered-index SoA access pattern |
| 114 | ~60KB threadgroup memory limit | Prevented impossible threadgroup histogram in grid.metal |
| 125 | Coherent memory without flushes | Enabled simple CPU write -> GPU read flow |
| 101 | M4 ~100 GB/s sustained bandwidth | Basis for 10M particle feasibility calculation |

### BACKGROUND (11 findings)
| KB ID | Finding | Why It Didn't Matter |
|-------|---------|---------------------|
| 121 | Fragment barrier expensive | No explicit barriers used at all |
| 181 | Avoid thread-space dynamic indexing | All buffers naturally use device/constant |
| 310 | SIMD width = 32 | No SIMD intrinsics used; 256 threadgroup size is standard |
| 314 | SIMD shuffle 2x NVIDIA | No SIMD shuffles used |
| 266 | 128B cache lines | No cache-line-aware alignment |
| 269 | 32 threadgroup memory banks | No threadgroup memory used |
| 113 | SLC ~2x DRAM bandwidth | No SLC-aware optimization |
| 141 | Sync hierarchy (fence/barrier/mem_flags) | Implicit ordering via separate encoders sufficed |
| 167 | Metal 4 barriers | Metal 3 APIs used; Metal 4 not supported by objc2-metal |
| 281 | Bandwidth benchmarks | KB 101 used instead for specific numbers |
| 259-265 (remaining) | Occupancy range | General context, no profiling performed |

### MISLEADING (3 findings)
| KB ID | Finding | What Went Wrong |
|-------|---------|----------------|
| 232 | Triple buffering with semaphore(3) | Implementation uses single buffering (semaphore=1); triple buffering impractical with shared mutable particle buffers without 3x memory |
| 265 (partial) | Use function constants to avoid divergence | Recommended in research.md but never implemented; all kernels use runtime branching |
| 268 (partial) | SoA preferred for partial reads | True in general, but the update kernel reads ALL fields for every particle anyway (pos, vel, lifetime, color, size), so SoA's "partial read" advantage is minimal for the hottest kernel |

---

## Recommendations for Future KB Usage

1. **Validate KB recommendations against actual memory constraints** before including them in spec recommendations. The triple-buffering recommendation (KB 232) was mechanically applied without considering that shared mutable buffers make it impossible at scale.

2. **Distinguish "cite for context" from "cite as design driver"** in research.md. Having 28+ KB IDs creates an illusion of deep utilization when only 4 were truly design-critical.

3. **Track which KB findings are actually consulted during implementation**, not just cited during research. The grid.metal design note is an excellent example -- it shows KB 283 and KB 114 being actively reasoned about, not just cited.

4. **Add negative citations**: "KB X was considered but rejected because Y" is more valuable than omission. The grid.metal shader does this well; other files do not.

5. **Profile-dependent findings (KB 259, 265, 134) should include verification steps** in the task plan. None of the occupancy or divergence findings were verified with Metal System Trace or shader profiler, making their "verified" status in research.md questionable.
