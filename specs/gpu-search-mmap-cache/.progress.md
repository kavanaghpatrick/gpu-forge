---
spec: gpu-search-mmap-cache
basePath: specs/gpu-search-mmap-cache
phase: research
task: 0/0
updated: 2026-02-17T23:45:00Z
---

# Progress: gpu-search-mmap-cache

## Original Goal

Eliminate 30-second filesystem walk on subsequent launches by saving raw chunks_buffer to disk after first walk, then using mmap(file) + makeBuffer(bytesNoCopy:) to create file-backed Metal GPU buffer on next launch. GPU reads page-fault data from SSD on demand via unified memory. Target: <10ms warm start from cached path index. Cache file is exact GPU buffer byte layout (4KB chunks, newline-delimited paths). Save to ~/.cache/gpu-search-ui/path-index.bin with header (magic, version, chunk_count, total_paths, timestamp). On launch: check cache freshness, mmap if valid, otherwise walk+save. Background: re-walk and update cache async. KB evidence: #224 (mmap+bytesNoCopy file-backed buffer), #1349 (MTLIO streaming for GPU text search), #223 (makeBuffer bytesNoCopy page-aligned), #1387 (directory traversal is the bottleneck at 62us/file).

## Completed Tasks

- [x] 1.1 Add libc dependency and chunk_cache module scaffold
- [x] 1.2 Implement save_chunk_cache() with atomic write

## Current Task

Awaiting next task

## Learnings

- **metal crate API confirmed:** v0.33 exposes `new_buffer_with_bytes_no_copy()` method. Wraps existing memory without copying. Requires page-aligned pointer and deallocator block (from KB #1288, #1358).
- **Apple Silicon page alignment:** 16KB page boundary required (vm_page_size). `mmap()` returns page-aligned memory automatically on ARM64 — no custom alignment code needed.
- **Performance validated:** M4 Pro benchmark shows mmap + bytesNoCopy loads 10K paths in 80ms vs 300ms for standard read() (KB #1651). Subsequent accesses <1ms via page cache.
- **Best practice pattern:** Persistent indexes benefit from mmap + bytesNoCopy because they're accessed repeatedly. Page faults on first access acceptable for app startup (KB #1645).
- **Deallocator requirement:** Must implement deallocator block to call munmap() when GPU buffer released. IndexSnapshot RAII pattern (proven in gpu-content-index) manages lifetime correctly.
- **File format strategy:** Extend existing PathCache binary format to include raw chunks_buffer bytes. Keep header for version + timestamp invalidation. Atomic temp-rename write pattern.
- **GPU kernel unchanged:** Existing path_search_kernel + search_paths() work with any buffer source — no shader or search code changes needed.
- **No Metal 3 required:** mmap + bytesNoCopy is Metal 2+ (all Apple Silicon ships Metal 3+, but pattern proven on older iOS too per KB #1358).
- **chunk_data dual use:** `search_paths()` reads `self.chunk_data` (Vec<u8>) for CPU path extraction AND `self.chunks_buffer` for GPU dispatch. On mmap load, chunks_buffer wraps mmap memory via bytesNoCopy; chunk_data can point to same mmap region via unsafe slice or read from `chunks_buffer.contents()`.
- **Integration point is index_thread():** Cache check happens in `index_thread()` (app.rs:387). If valid cache, send paths via batch_tx and skip walk. Search thread receives paths normally.
- **search_thread rebuilds engine per search:** `GpuContentSearch::new_for_paths()` called when paths change. For mmap cache, need to pass pre-built buffer instead of calling `load_paths()`.
- **Header size risk:** 32 bytes for root path is too small. macOS paths average 40-60 bytes. Need variable-length or 256-byte fixed field.
- **Metadata is trivially reconstructible:** For path search, all ChunkMetadata entries are uniform (file_index=0, sequential chunk_index, flags=1). No need to cache metadata — rebuild from chunk count in <1ms.
- **Two integration paths:** (a) Cache saves chunk_data, load_chunk_cache() returns (chunk_data, chunk_count) to index_thread, search_thread calls load_paths() as normal. (b) Cache saves chunk_data, load returns mmap buffer directly, search_thread skips load_paths() and uses buffer directly. Option (a) is simpler (~50 lines) but copies data. Option (b) is zero-copy but requires refactoring GpuContentSearch to accept external buffer (~100 lines).

- **metal 0.33 uses `block` crate (not block2) and `objc` (not objc2):** The `ConcreteBlock::new()` API from `block` 0.1.6 creates Objective-C blocks for the deallocator param. However, design chose `None` deallocator with RAII `MmapChunkData` for simplicity.
- **new_buffer_with_bytes_no_copy exact signature:** `(&self, bytes: *const c_void, length: NSUInteger, options: MTLResourceOptions, deallocator: Option<&Block<(*const c_void, NSUInteger), ()>>) -> Buffer`. Deallocator is optional.
- **MAP_PRIVATE not MAP_SHARED:** MAP_PRIVATE gives copy-on-write semantics. Combined with PROT_READ|PROT_WRITE, the GPU's StorageModeShared can write to the buffer (for atomics etc) without corrupting the cache file on disk. Critical safety property.
- **libc not a direct dependency:** `libc` is transitive via metal/objc but must be added explicitly to Cargo.toml for direct `libc::mmap`/`libc::munmap` calls.
- **PathCache::default_cache_dir() is private:** Currently `fn default_cache_dir()` not `pub`. Must change to `pub(crate)` for chunk_cache to reuse it.
- **save_chunk_cache split into public + _to_dir:** Public function calls `PathCache::default_cache_dir()` then delegates to `save_chunk_cache_to_dir(cache_dir, ...)`. The `_to_dir` variant is `fn` (crate-private) and can be used by unit tests with tempdir in task 1.4.
- **Padding formula:** `padded_len = (raw_len + PAGE_SIZE - 1) / PAGE_SIZE * PAGE_SIZE` rounds up to next 16KB boundary. Zero bytes written for pad.
- **Atomic write pattern:** Write to `chunks.cache.tmp.<pid>` then `std::fs::rename()`. PID suffix avoids collision if multiple instances run.
- **batch_tx channel type change required:** Currently `Sender<Vec<PathBuf>>`. Must change to enum `PathBatch { Paths(Vec<PathBuf>), CachedChunks(MmapChunkData) }` to carry mmap data from index_thread to search_thread.
- **Hybrid approach for chunk_data:** GPU buffer is zero-copy via bytesNoCopy, but `self.chunk_data` Vec gets a copy from mmap (~1ms for 4MB). This avoids unsafe lifetime coupling between MmapChunkData and GpuContentSearch, which would require either self-referential structs or raw pointer gymnastics. The GPU path is the performance-critical one.
- **search_thread holds MmapChunkData alive:** The MmapChunkData must outlive the GPU buffer. Since search_thread owns the engine, it must also hold the MmapChunkData in scope. Drop order: engine first (releases GPU buffer), then MmapChunkData (calls munmap).

## Blockers

- None currently

## Next

Task 1.3: Implement load_chunk_cache()

## Task Planning Learnings

- **15 tasks across 5 phases**: 5 POC, 4 integration, 3 testing, 2 quality, 2 PR lifecycle (plus verify checkpoints)
- **Total estimated code**: ~150 lines across 5 files. Small, focused feature.
- **Critical dependency chain**: chunk_cache.rs (save/load) must be complete and tested before app.rs integration. load_from_cache() in search.rs depends on MmapChunkData struct.
- **Bridge type change cascades**: Changing `batch_tx` from `Sender<Vec<PathBuf>>` to `Sender<PathBatch>` touches bridge.rs, app.rs (both index_thread and search_thread). Must update all send sites and receive handlers simultaneously.
- **root parameter threading**: search_thread needs `root: PathBuf` to call `save_chunk_cache()` on cold path. Currently not passed — must add to search_thread signature and capture in spawn closure.
- **GPU test gating**: Integration test for search result identity requires Metal device. Must guard with `Device::system_default()` check or `#[ignore]` for headless CI.
- **Drop order critical**: MmapChunkData must outlive GpuContentSearch (which holds the GPU buffer wrapping the mmap). `mmap_holder` in search_thread must be declared before `engine` or structured so engine drops first.
- **save_chunk_cache needs test variants**: Production save uses `PathCache::default_cache_dir()` (real home dir). Unit tests need `save_to_dir`/`load_from_dir` variants accepting custom cache path to use tempdir. Same pattern as PathCache's `save_to`/`load_from` private methods.
- **Quality commands**: `cargo check && cargo clippy && cargo test && cargo build --release` — all standard Cargo, no custom scripts.
