---
spec: gpu-filter-compact
basePath: specs/gpu-filter-compact
phase: tasks
task: 0/22
updated: 2026-02-20
---

# Progress: gpu-filter-compact

## Original Goal

GPU filter+compact kernel for numeric WHERE clauses — 10-25x over Polars on selective filters

## Completed Tasks

- [x] 1.1 Scaffold forge-filter crate
- [x] 1.2 Implement FilterKey trait, Predicate enum, FilterParams, FilterError
- [x] 1.3 Write filter.metal — all 4 GPU kernels
- [x] 1.4 Implement FilterBuffer<T> and FilterResult<T>
- [x] 1.5 Implement GpuFilter::new() and 3-dispatch filter pipeline
- [x] 1.6 POC smoke test — filter u32 > threshold
- [x] 1.7 [VERIFY] Quality checkpoint: build + test + clippy

- [x] 2.1 Add all 6 comparison operators for u32
- [x] 2.2 Add all 6 numeric types with convenience methods

- [x] 2.3 BETWEEN range predicate — f2b865b93
- [x] 2.4 [VERIFY] Quality checkpoint: build + all tests + clippy
- [x] 2.5 Index output mode (filter_indices, filter_with_indices)
- [x] 2.6 Unordered atomic scatter mode
- [x] 2.7 Compound predicates (AND/OR)
- [x] 2.8 [VERIFY] Quality checkpoint: full test suite + clippy

- [x] 3.1 Exhaustive type x operator matrix tests (42 combos)
- [x] 3.2 Edge case tests
- [x] 3.3 Large input tests (16M, 64M)

## Current Task

Awaiting next task

## Learnings

- alloc_filter_buffer(0) panics because Metal cannot allocate 0-byte buffers. Empty input must be handled before buffer allocation (filter_u32 returns early, filter() checks buf.len()==0). Tests use capacity=1 with len=0 for the FilterBuffer empty path.
- Decoupled lookback deadlocks on Apple Silicon -- scan.metal already uses 3-pass reduce-then-scan. Decoupled Fallback (b0nes164) shows promise but adds complexity.
- Existing 5-dispatch compact pipeline achieves 42-73 GB/s (phase1_foundation.json). Fused 3-dispatch should reach 150-200 GB/s.
- Polars filter baseline measured on this machine: 2780 Mrows/s @ 16M, 4489 @ 4M, 4117 @ 1M. GPU dispatch overhead dominates below 1M elements.
- forge-primitives already has scan.metal (3-pass SIMD prefix scan), compact_scan.metal (flags+scatter), filter_bench.metal (simd_sum count). All reusable.
- PsoCache supports function constants (FnConstant::Bool/U32) -- ideal for predicate type specialization at zero runtime cost.
- Atomic scatter (unordered) is 30-40% faster than scan-based ordered compact at high selectivity -- useful for aggregation queries.
- f64 has 1/32 throughput on M4 Pro -- filter f64 columns will be ~32x slower than f32. Document this limitation.
- forge-sort `SortBuffer<T>` pattern with zero-copy StorageModeShared is the proven API pattern to follow.
- forge-sort uses sealed SortKey trait to restrict types at compile time -- FilterKey should follow same pattern.
- forge-sort exposes both buffer API (zero-copy) and convenience slice API (copy in/out) -- forge-filter needs both.
- 42 type/operator combos (6 types x 7 predicates) is the correctness test matrix; compound predicates add more.
- Output buffer ownership is an open design question -- returning new FilterResult is simplest; `filter_into()` for advanced reuse.
- Multi-column compound predicates deferred to P2 to avoid scope creep on initial release.
- FilterBuffer<T> vs SortBuffer<T> interop needs design decision -- separate types with conversion preferred.
- scan_partials with 4 elements/thread only handles 1024 partials = 4M elements. For 16M need 16 elems/thread (4096 partials). For 64M+ need hierarchical scan (2 extra dispatches).
- forge-sort inlines metal_helpers.rs (PsoCache, FnConstant, alloc_buffer) rather than depending on forge-primitives. forge-filter follows same pattern for independent crates.io publishing.
- FilterParams uses uint lo_bits/hi_bits with as_type<T> reinterpretation in Metal -- avoids template metaprogramming, single struct for all 32-bit types.
- 64-bit types need separate FilterParams64 struct (lo/hi each split into two uint32 fields) due to Metal setBytes alignment constraints.
- Predicate re-evaluation in scatter kernel is free (1 compare hidden behind memory latency) and avoids 64MB flags buffer at 16M elements.
- Single encoder + multiple dispatchThreadgroups() gives implicit memory barriers between dispatches -- proven in forge-sort's 4-dispatch pipeline.
- At 50% selectivity, realistic speedup is ~8x over Polars (not 10x) due to 2x input read. 10x+ achieved at selectivities below ~25%, which is the typical SQL WHERE clause scenario.
- SIMD-aggregated atomics for unordered mode: 1 atomic per 32 threads = near-zero contention even at 16M elements.
- FilterBuffer<T> mirrors SortBuffer<T> exactly: Retained<ProtocolObject<dyn MTLBuffer>>, len, capacity, PhantomData. FilterResult<T> uses Option<Retained<...>> for values_buf/indices_buf to support index-only and value-only modes.
- Function constant index allocation: 0=PRED_TYPE, 1=IS_64BIT, 2=OUTPUT_IDX, 3=OUTPUT_VALS. Pre-compile 15 PSOs at init (7 predicates x 2 kernels + scan).
- Cargo.toml [[bench]] section requires the bench file to exist even for `cargo build` -- created stub benches/filter_benchmark.rs.
- Metal `constant void*` buffer binding with reinterpret_cast enables sharing a single buffer slot for both FilterParams and FilterParams64 structs, selected by is_64bit function constant at compile time.
- filter_atomic_scatter uses `simd_sum` + `simd_prefix_exclusive_sum` + per-SG-lane-0 `atomic_fetch_add` + `simd_broadcast_first` pattern: only 1 atomic per 32 threads, near-zero contention.
- For the atomic scatter kernel, guarding the atomic_fetch_add with `simd_total > 0u` avoids unnecessary atomic operations when a simdgroup has zero matches.
- objc2-metal 0.3.2 requires `MTLCommandBuffer` trait import for methods like computeCommandEncoder, commit, waitUntilCompleted, status, error. Unlike forge-sort which imports it, forge-filter was missing it.
- MTLCommandBufferStatus::Error comparison with cmd.status() requires swapping order (`MTLCommandBufferStatus::Error == cmd.status()`) due to generic type parameter context in Rust's PartialEq resolution.
- GpuFilter pre-compiles 15 PSOs at init: 7 predicates x (filter_predicate_scan + filter_scatter with IS_64BIT=false) + filter_scan_partials. 64-bit PSOs compiled lazily on first use.
- filter_scatter PSO needs all 4 function constants: PRED_TYPE(0), IS_64BIT(1), OUTPUT_IDX(2), OUTPUT_VALS(3). Pre-compiled with output_idx=false, output_vals=true for default filter() path.
- ensure_scratch_buffers takes elem_size parameter to correctly calculate tile_size and output buffer size for both 32-bit and 64-bit types.
- Scatter kernel for ordered output must process elements round-by-round (one element per thread at a time) with per-round TG-wide prefix sums, NOT per-thread batch prefix sums. The original batch approach gave correct counts but wrong output order (thread-interleaved, not index-ordered).
- Race condition in round-by-round scatter: simd_totals[] used for both cross-SG prefix scan and round total communication. Must use separate TG variable (round_total_tg) to avoid aliasing — otherwise next round's SG total write races with current round's total read after the barrier.
- Metal has NO `double` type — f64 comparison must be implemented via raw ulong bit pattern comparison (IEEE 754 ordering). Added `evaluate_predicate_f64()` with proper NaN handling and sign-aware ordering.
- Added DATA_TYPE function constant (index 4): 0=unsigned, 1=signed, 2=float/double. This selects `as_type<int/float>` for 32-bit or `as_type<long>/evaluate_predicate_f64` for 64-bit at PSO compile time.
- For i32/i64: `as_type<int/long>` reinterprets uint/ulong bits as signed, giving correct signed comparison semantics. The Rust `to_bits()` casts via `as u32`/`as u64` which preserves the two's complement bit pattern.
- NaN handling verified: f32 NaN excluded by Gt (NaN > x = false), included by Ne (NaN != x = true). IEEE 754 semantics work naturally via Metal's float comparison.
- filter_indices and filter_with_indices: No shader changes needed. dispatch_filter already passes OUTPUT_IDX/OUTPUT_VALS as function constants (indices 2, 3). PsoCache key includes all constants so variants are cached correctly. Scatter kernel already writes global element index to out_idx when OUTPUT_IDX=true.
- filter_unordered uses single dispatch of filter_atomic_scatter kernel. Needs all 5 function constants (PRED_TYPE, IS_64BIT, OUTPUT_IDX, OUTPUT_VALS, DATA_TYPE). Counter buffer (buf_count) zeroed via CPU ptr before dispatch, read back after waitUntilCompleted. No partials or scan dispatches needed.
- Matrix test macro approach: 6 test functions x 7 predicates = 42 combos total. Each uses deterministic ChaCha8Rng (seed 42), 100K elements, ~50% selectivity via median/percentile thresholds. Full content comparison (not just count) against CPU Predicate::evaluate() reference. All 42 combos pass in ~0.3s.
- Compound predicates V1 approach: `simplify()` detects `And([Ge(lo), Le(hi)])` -> `Between(lo, hi)` for GPU fast path. AND of simple predicates cascades through multiple GPU passes (filter output of filter). OR and nested compounds fall back to CPU-side `evaluate()`. All 4 compound tests pass: AND, OR, nested AND(OR(...), Ne), and AND-as-BETWEEN optimization.
- Hierarchical scan for >4096 tiles: scan_partials handles max 4096 partials (256 threads x 16 elems). For 64M u32 (15625 tiles) and 16M u64 (7813 tiles), need 2-level scan. Implementation: (1) scan each block of 4096 partials independently, (2) scan block totals, (3) add block prefixes via filter_add_block_offsets kernel. Uses buffer offset parameter to point scan_partials at different sections of the partials array.
- Metal setBuffer_offset_atIndex byte offset works for splitting a buffer into blocks: each scan_partials dispatch gets `offset = block_idx * 4096 * 4` bytes into the partials buffer, processing only that block's entries.
- Hierarchical scan timings (release): 16M u32 (simple scan) = 11.5ms, 64M u32 (hierarchical, 15625 tiles, 4 blocks) = 41.6ms, 16M u64 (hierarchical, 7813 tiles, 2 blocks) = 25.5ms. All counts verified against CPU reference.
- Borrow checker with encode_scan_partials: dispatch_filter's buffer references must be scoped into blocks before/after the encode_scan_partials(&mut self) call to avoid simultaneous &self and &mut self borrows.

### Verification: V2.8 [VERIFY] Quality checkpoint: full test suite + clippy
- Status: PASS
- Commands: cargo test -p forge-filter (0, 29 passed), cargo clippy -p forge-filter -- -D warnings (0)
- No fixes needed -- all commands passed clean on first run
- Test quality: 29 tests, zero mocks/stubs. All tests use real GPU hardware (GpuFilter::new + Metal compute pipeline). Tests cover: all 6 types, all operators, BETWEEN, index output, unordered scatter, compound predicates (AND/OR/nested), NaN, 16M scale.

### Verification: V1.7 [VERIFY] Quality checkpoint: build + test + clippy
- Status: PASS (after fixes)
- Commands: cargo build (0), cargo test (0, 4 passed), cargo clippy -D warnings (0)
- Fixes applied:
  - Renamed unused `capacity` field to `_capacity` in FilterResult<T> (dead_code warning)
  - Replaced `n.div_ceil(tile_size)` with `(n + tile_size - 1) / tile_size` at 2 call sites (incompatible_msrv: div_ceil requires Rust 1.73, MSRV is 1.70)
- Commit: dcee3b8dc chore(forge-filter): pass quality checkpoint
- Test quality: All 4 tests use real GPU hardware (GpuFilter::new + Metal compute), zero mocks, assertions compare GPU output against CPU reference vectors.

### Verification: V2.4 [VERIFY] Quality checkpoint: build + all tests + clippy
- Status: PASS
- Commands: cargo build -p forge-filter --release (0), cargo test -p forge-filter (0, 20 passed), cargo clippy -p forge-filter -- -D warnings (0)
- No fixes needed — all three commands passed clean on first run
- Test quality: 20 tests, 46 real assertions, zero mocks/stubs/fakes. All tests use real GPU hardware (GpuFilter::new + Metal compute pipeline). Tests cover: u32 (6 operators + between + edge cases), i32/f32/u64/i64/f64 types, NaN handling, 16M scale.

## Interview Responses

### Requirements Interview (from requirements.md)
- Primary users: Published crate users (external Rust developers via crates.io)
- Priority tradeoffs: Prioritize API completeness (full predicate set and compound filters)
- Success criteria: 10x+ over Polars on numeric filters at 16M+ rows with measured benchmarks

## Blockers

- None currently

## Next

Task 3.4: [VERIFY] Quality checkpoint: full test suite + clippy + check

## Task Planning Insights

- 22 tasks total: 7 POC, 8 core features, 4 testing, 5 polish+publish, 3 PR lifecycle
- Critical dependency: hierarchical scan must be implemented before 64M test (task 3.3). scan_partials handles max 4096 partials = 16.7M elements (32-bit). 64M needs 15625 tiles = requires L1/L2 hierarchical scan (5 dispatches instead of 3).
- Compound predicates (task 2.7) are the most complex single task -- multi-pass mask composition for general AND/OR, with BETWEEN optimization for common Ge+Le pattern.
- forge-sort metal_helpers.rs can be copied verbatim -- PsoCache, FnConstant, alloc_buffer, init_device_and_queue are identical.
- Quality checkpoints at tasks 1.7, 2.4, 2.8, 3.4, 4.3 catch regressions early.
- POC (tasks 1.1-1.6) proves the 3-dispatch pipeline end-to-end for u32 Gt only. All other types/predicates deferred to Phase 2.
- workspace Cargo.toml at `metal-forge-compute/Cargo.toml` currently has 3 members -- adding forge-filter as 4th member.
- Benchmark task 4.4 may reveal 10x is only achievable at <25% selectivity -- design already accounts for this (8x at 50% is acceptable, 10x+ at typical SQL WHERE selectivity).
