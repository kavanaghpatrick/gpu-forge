---
spec: gpu-compute-experiments
basePath: ./specs/gpu-compute-experiments
phase: tasks
task: 5/52
updated: 2026-02-17
---

# Progress: gpu-compute-experiments

## Original Goal

Build a suite of 16 GPU compute benchmark experiments on Apple Silicon M4 Pro (273 GB/s, 20 GPU cores) to measure real-world speedups across key primitive categories: sort, scan, reduce, histogram, matmul, relational operations, and consumer app workloads. Each experiment compares GPU Metal vs CPU baseline. Results determine which layers of metal-forge-compute to prioritize.

## Completed Tasks

- [x] 1.1 Create workspace and forge-primitives crate skeleton
- [x] 1.2 Metal build.rs + reduce shader + types.h
- [x] 1.3 Create forge-bench crate with CLI, config, stats, data_gen
- [x] 1.4 Experiment trait + reduce experiment + CPU baseline
- [x] 1.6 Harness measurement loop + table output
- [x] 2.5 Histogram kernel + experiment
- [x] 2.1 Prefix scan kernel + experiment
- [x] 2.2 Stream compaction kernel + experiment
- [x] 2.7 Output: CSV, summary, roofline
- [x] 2.4 Radix sort kernel + experiment

## Current Task

Awaiting next task

## Learnings (task 2.4)

- Radix sort: reduce-then-scan approach (NOT OneSweep which deadlocks on Apple Silicon)
- DIGIT-MAJOR histogram layout: histogram[digit * num_tg + tg_id] is critical. TG-major layout (histogram[tg_id * 16 + digit]) places ALL of TG0's elements before TG1's regardless of digit value, breaking sort at TG boundaries
- Stable scatter: must NOT use atomic_fetch_add (non-deterministic). Use shared memory digit array + sequential prefix count for deterministic stable local offsets
- SortParams needs num_threadgroups field for digit-major indexing in shaders (repurposed a _pad field)
- cargo clean -p forge-primitives needed when adding new .metal shader files (build.rs rerun-if-changed doesn't detect new files)
- Borrow checker fix: clone Retained<ProtocolObject<dyn MTLBuffer>> pointers at top of run_gpu() to avoid mutable/immutable borrow conflicts with PsoCache
- 8-pass radix sort with ping-pong: even passes read keys_a->write keys_b, odd passes reverse. After 8 passes (even), result in keys_a
- Per-pass pipeline: zero histogram -> radix_histogram -> scan_local -> scan_partials -> scan_add_offsets -> radix_scatter
- Performance: 1M elements sorted at 31.4x GPU speedup (GPU ~11.7ms vs CPU ~366ms)
- Debug methodology: tested progressively larger inputs (8, 16, 256, 1000, 100K, 1M) to isolate bugs at TG boundaries

## Learnings (task 2.2)

- Stream compaction = 5-dispatch pipeline in 1 command buffer: compact_flags -> scan_local -> scan_partials -> scan_add_offsets -> compact_scatter
- compact_flags: simple predicate kernel (value > threshold), writes 0/1 flags
- compact_scatter: if flags[gid]==1, output[scan[gid]] = input[gid] -- scatter using prefix scan indices
- Total output count = scan[N-1] + flags[N-1] (last exclusive scan value + last flag)
- Threshold = u32::MAX / 2 gives ~50% selectivity on uniform random data
- Reused all 3 scan kernels (scan_local, scan_partials, scan_add_offsets) from scan.metal -- no duplication
- For 1M elements: 1954 scan TGs > 512 MAX_GPU_PARTIALS, so CPU partials path used (same as scan experiment)
- Performance: compact@1M = 6.6x speedup (GPU ~3.5ms vs CPU ~22.9ms) -- STRONG verdict
- Validation: sort both GPU and CPU results, compare element-by-element for exact set equality
- rayon par_filter_gt for CPU baseline: simple par_iter().filter().collect()

## Learnings (task 2.1)

- Blelloch exclusive prefix scan: up-sweep (reduce tree) + down-sweep (distribute) in shared memory
- 3-pass reduce-then-scan: scan_local (per-TG) -> scan_partials (single TG or CPU) -> scan_add_offsets
- 256 threads * 2 elements = 512 elements per threadgroup; for 1M elements = 1954 TGs
- 1954 partials exceeds MAX_GPU_PARTIALS (512), so CPU sequential scan used for partials at 1M+
- GPU partials path (scan_partials kernel) used for <= 512 partials (e.g. 100K elements = 196 TGs)
- PsoCache::get_or_create takes &mut self -- cannot hold multiple PSO refs simultaneously
- Fix: get PSO in separate scope per-pass so mutable borrow drops before next call
- MTLComputeCommandEncoder trait must be imported for setComputePipelineState/setBuffer_offset_atIndex
- Data clamped to avoid u32 overflow: 1M * max_val(4000) ~= 4B fits in u32 prefix sum
- Performance: scan@1M = ~9-18x speedup (GPU ~0.4ms vs CPU ~7ms), 10-20 GB/s effective bandwidth

## Learnings (task 2.5)

- histogram_256 kernel: threadgroup atomic_uint[256] = 1KB shared memory, well within 32KB limit
- Pattern: init local_hist -> accumulate (value % num_bins) -> merge to global via atomic_fetch_add
- Must zero output histogram buffer before each GPU run (same pattern as reduce output zeroing)
- 256-bin histogram at 1M: GPU=0.24ms, CPU=10.15ms, 43x speedup -- excellent for shared-memory approach
- 65536-bin histogram deferred: would need 256KB threadgroup memory (exceeds 32KB limit), requires tiled multi-pass
- sequential CPU baseline (simple loop + increment) is sufficient; no need for rayon parallelism on histogram

## Learnings

- Existing gpu-query shaders (filter.metal, compact.metal, aggregate.metal) are production-quality and reusable
- Inference pipeline achieved 462 tok/s decode on M4 Pro, validating Metal dispatch infrastructure
- Decoupled lookback and OneSweep DEADLOCK on Apple Silicon - use reduce-then-scan instead
- Single command buffer batching was key for inference perf (91â†’1 cmdbuf)
- Page-aligned buffers enable zero-copy via makeBuffer(bytesNoCopy:)
- Task planning: 52 tasks across 8 phases (POC, Foundation, Query Ops, Consumer, Run All, Testing, Quality, PR)
- Task planning: metal-forge-compute/ dir exists but only has an ai/ subdir -- all files are new creation
- Task planning: gpu-query/shaders/ has 11 .metal files with established patterns (aggregate.metal 3-level reduction, compact.metal atomic counter, filter.metal function constants)
- Task planning: build.rs pattern is identical across gpu-query and particle-system -- xcrun metal -c -> .air, xcrun metallib -> .metallib
- Task planning: Critical dependency chain: scan -> compact AND sort -> groupby -> pipeline. Must build scan first
- Task planning: Radix sort highest complexity (8 passes x 3 dispatches), sort experiment depends on scan kernels for histogram prefix sum
- Task planning: 65536-bin histogram may need tiling fallback if threadgroup memory > 32KB
- Task planning: No existing Cargo workspace at metal-forge-compute level -- must create from scratch
- Task planning: DuckDB experiment has CLI subprocess approach (not C API) -- simpler but ~50ms startup overhead

## Learnings (task 1.1)

- MTLDevice trait must be imported explicitly in pso_cache.rs for newComputePipelineStateWithFunction_error
- MetalContext library is Optional since no shaders compiled yet in task 1.1
- PsoCache simplified vs gpu-query: no function constants, just function name keyed

## Learnings (task 1.2)

- build.rs pattern copied directly from gpu-query/build.rs -- handles both stub (empty shaders dir) and real .metal files
- reduce.metal: 4 kernels (sum_u32, sum_f32, min_u32, max_u32) all use 3-level reduction: simd -> threadgroup shared mem -> global
- f32 sum uses CAS loop (atomic_compare_exchange_weak) since Metal has no atomic_float add
- min/max use per-threadgroup partials array (CPU does final reduction) since no atomic min/max on Metal
- types.h #include "types.h" works because build.rs passes -I shaders/ to xcrun metal

## Learnings (task 1.3)

- forge-bench skeleton already existed with stub Cargo.toml and main.rs from task 1.1
- clap 4 derive works cleanly for ForgeArgs: positional experiments + --sizes/--runs/--warmup/--profile/--json-file/--csv-file
- IQR outlier detection in stats.rs: filter values outside [Q1-1.5*IQR, Q3+1.5*IQR] before computing final stats
- DataGenerator uses StdRng::seed_from_u64 for deterministic benchmark data generation

## Learnings (task 1.4)

- forge-bench needs objc2 + objc2-metal direct deps for Metal types (Retained, MTLBuffer, MTLCommandQueue trait)
- MTLCommandQueue, MTLCommandBuffer, MTLCommandEncoder traits must be imported explicitly for method resolution
- Experiment trait uses &MetalContext (shared) + &mut self (exclusive buffers) pattern
- ReduceExperiment stores input/output/params buffers as Option<Retained<...>> -- set in setup()
- Must zero atomic output buffer before each GPU run (otherwise accumulates across runs)
- PsoCache stored per-experiment for kernel PSO reuse across runs
- cpu_baselines::rayon_reduce uses u64 accumulator to avoid u32 overflow in sum

## Learnings (task 1.6)

- Harness pattern: setup -> validate (run both once) -> warmup (GPU only) -> measured loop (GPU + CPU) -> compute_stats
- DataPoint struct holds experiment name, size, gpu_stats, cpu_stats, speedup, and arbitrary metrics HashMap
- comfy-table with Cell color (Green/Cyan/Yellow/Red) for speedup tiers: >=5x, >=2x, >=1x, <1x
- JSON output wraps results with hardware header (chip name + bandwidth) and RFC3339 timestamp via chrono
- indicatif spinner with progress callback pattern works well for harness status updates
- run_experiment takes a progress_cb: Option<&dyn Fn(&str)> for decoupled progress reporting
- Reduce validation warning: GPU atomic u32 sum overflows vs CPU u64 sum -- pre-existing issue in reduce experiment, not harness bug
- BenchConfig struct (sizes, runs, warmup) decouples CLI parsing from harness logic

## Learnings (task 2.7)

- CSV module: simple std::io::Write with header + data rows, no external crate needed
- Summary verdicts: DOMINANT (>10x), STRONG (>5x), SOLID (2-5x), MARGINAL (1-2x), SLOWER (<1x)
- ASCII roofline uses Unicode block characters (full block + light shade) for bar chart
- --csv-file flag already existed in cli.rs from task 1.3; just needed the write_csv implementation
- Wired summary + roofline to always print; CSV/JSON are opt-in via flags

## Blockers

- None currently

## Next

Task 2.3: Quality checkpoint -- scan/compact/sort correctness
