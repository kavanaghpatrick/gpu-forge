---
spec: gpu-compute-experiments
basePath: ./specs/gpu-compute-experiments
phase: tasks
task: 5/52
updated: 2026-02-17
---

# Progress: gpu-compute-experiments

## Original Goal

Build a suite of 16 GPU compute benchmark experiments on Apple Silicon M4 Pro (273 GB/s, 20 GPU cores) to measure real-world speedups across key primitive categories: sort, scan, reduce, histogram, matmul, relational operations, and consumer app workloads. Each experiment compares GPU Metal vs CPU baseline. Results determine which layers of metal-forge-compute to prioritize.

## Completed Tasks

- [x] 1.1 Create workspace and forge-primitives crate skeleton
- [x] 1.2 Metal build.rs + reduce shader + types.h
- [x] 1.3 Create forge-bench crate with CLI, config, stats, data_gen
- [x] 1.4 Experiment trait + reduce experiment + CPU baseline
- [x] 1.6 Harness measurement loop + table output
- [x] 2.5 Histogram kernel + experiment

## Current Task

Awaiting next task

## Learnings (task 2.5)

- histogram_256 kernel: threadgroup atomic_uint[256] = 1KB shared memory, well within 32KB limit
- Pattern: init local_hist -> accumulate (value % num_bins) -> merge to global via atomic_fetch_add
- Must zero output histogram buffer before each GPU run (same pattern as reduce output zeroing)
- 256-bin histogram at 1M: GPU=0.24ms, CPU=10.15ms, 43x speedup -- excellent for shared-memory approach
- 65536-bin histogram deferred: would need 256KB threadgroup memory (exceeds 32KB limit), requires tiled multi-pass
- sequential CPU baseline (simple loop + increment) is sufficient; no need for rayon parallelism on histogram

## Learnings

- Existing gpu-query shaders (filter.metal, compact.metal, aggregate.metal) are production-quality and reusable
- Inference pipeline achieved 462 tok/s decode on M4 Pro, validating Metal dispatch infrastructure
- Decoupled lookback and OneSweep DEADLOCK on Apple Silicon - use reduce-then-scan instead
- Single command buffer batching was key for inference perf (91â†’1 cmdbuf)
- Page-aligned buffers enable zero-copy via makeBuffer(bytesNoCopy:)
- Task planning: 52 tasks across 8 phases (POC, Foundation, Query Ops, Consumer, Run All, Testing, Quality, PR)
- Task planning: metal-forge-compute/ dir exists but only has an ai/ subdir -- all files are new creation
- Task planning: gpu-query/shaders/ has 11 .metal files with established patterns (aggregate.metal 3-level reduction, compact.metal atomic counter, filter.metal function constants)
- Task planning: build.rs pattern is identical across gpu-query and particle-system -- xcrun metal -c -> .air, xcrun metallib -> .metallib
- Task planning: Critical dependency chain: scan -> compact AND sort -> groupby -> pipeline. Must build scan first
- Task planning: Radix sort highest complexity (8 passes x 3 dispatches), sort experiment depends on scan kernels for histogram prefix sum
- Task planning: 65536-bin histogram may need tiling fallback if threadgroup memory > 32KB
- Task planning: No existing Cargo workspace at metal-forge-compute level -- must create from scratch
- Task planning: DuckDB experiment has CLI subprocess approach (not C API) -- simpler but ~50ms startup overhead

## Learnings (task 1.1)

- MTLDevice trait must be imported explicitly in pso_cache.rs for newComputePipelineStateWithFunction_error
- MetalContext library is Optional since no shaders compiled yet in task 1.1
- PsoCache simplified vs gpu-query: no function constants, just function name keyed

## Learnings (task 1.2)

- build.rs pattern copied directly from gpu-query/build.rs -- handles both stub (empty shaders dir) and real .metal files
- reduce.metal: 4 kernels (sum_u32, sum_f32, min_u32, max_u32) all use 3-level reduction: simd -> threadgroup shared mem -> global
- f32 sum uses CAS loop (atomic_compare_exchange_weak) since Metal has no atomic_float add
- min/max use per-threadgroup partials array (CPU does final reduction) since no atomic min/max on Metal
- types.h #include "types.h" works because build.rs passes -I shaders/ to xcrun metal

## Learnings (task 1.3)

- forge-bench skeleton already existed with stub Cargo.toml and main.rs from task 1.1
- clap 4 derive works cleanly for ForgeArgs: positional experiments + --sizes/--runs/--warmup/--profile/--json-file/--csv-file
- IQR outlier detection in stats.rs: filter values outside [Q1-1.5*IQR, Q3+1.5*IQR] before computing final stats
- DataGenerator uses StdRng::seed_from_u64 for deterministic benchmark data generation

## Learnings (task 1.4)

- forge-bench needs objc2 + objc2-metal direct deps for Metal types (Retained, MTLBuffer, MTLCommandQueue trait)
- MTLCommandQueue, MTLCommandBuffer, MTLCommandEncoder traits must be imported explicitly for method resolution
- Experiment trait uses &MetalContext (shared) + &mut self (exclusive buffers) pattern
- ReduceExperiment stores input/output/params buffers as Option<Retained<...>> -- set in setup()
- Must zero atomic output buffer before each GPU run (otherwise accumulates across runs)
- PsoCache stored per-experiment for kernel PSO reuse across runs
- cpu_baselines::rayon_reduce uses u64 accumulator to avoid u32 overflow in sum

## Learnings (task 1.6)

- Harness pattern: setup -> validate (run both once) -> warmup (GPU only) -> measured loop (GPU + CPU) -> compute_stats
- DataPoint struct holds experiment name, size, gpu_stats, cpu_stats, speedup, and arbitrary metrics HashMap
- comfy-table with Cell color (Green/Cyan/Yellow/Red) for speedup tiers: >=5x, >=2x, >=1x, <1x
- JSON output wraps results with hardware header (chip name + bandwidth) and RFC3339 timestamp via chrono
- indicatif spinner with progress callback pattern works well for harness status updates
- run_experiment takes a progress_cb: Option<&dyn Fn(&str)> for decoupled progress reporting
- Reduce validation warning: GPU atomic u32 sum overflows vs CPU u64 sum -- pre-existing issue in reduce experiment, not harness bug
- BenchConfig struct (sizes, runs, warmup) decouples CLI parsing from harness logic

## Blockers

- None currently

## Next

Task 2.6: Quality checkpoint -- all 5 foundation primitives
