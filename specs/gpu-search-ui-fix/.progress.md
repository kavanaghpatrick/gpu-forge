# gpu-search-ui-fix

## Original Goal
Fix 3 critical bugs in gpu-search-ui filename search: (1) GPU context extraction truncates paths at 64 bytes, (2) paths stored without absolute prefix, (3) index rebuilds from scratch every launch. Additional: add comprehensive automated tests.

## Current Phase
execution (quick mode - all artifacts generated)

## Reality Check (BEFORE)

**Goal type**: Fix
**Reproduction command**: `cd /Users/patrickkavanagh/gpu-search-ui && cargo test --release test_profile_gpu_vs_cpu -- --nocapture --ignored`
**Exit code**: 0 (tests pass but performance data shows the bug)
**Error output**:
```
GPU vs CPU Time Breakdown (1M files, 102.9 MB)
Query (max_results)          TotalMs      Ratio Throughput  Matches
friendship (100)               3.08   GPU~100%      33.4 GB/s      100
friendship (50K)              10.45    CPU~70%       9.8 GB/s    50000
file (100)                     2.14   GPU~100%      48.0 GB/s      100
file (50K)                     7.90    CPU~73%      13.0 GB/s    50000
a (100)                        1.58   GPU~100%      65.2 GB/s      100
a (50K)                        7.63    CPU~79%      13.5 GB/s    50000
```

**All existing tests**: 9 passed, 0 failed (test_path_search.rs), 2 passed (test_integration.rs)

**Key issues to resolve**:
- Bug 1: CPU context extraction is 70-79% of total search time at 50K results (7.3ms of 10.4ms)
- Bug 2: Needs verification — code review suggests paths ARE preserved, but needs targeted test
- Bug 3: index_thread walks FS on every launch — no persistence

## Completed Tasks
- [x] 1.1 Add GPU-side newline scanning to turbo_search_kernel
- [x] 1.2 Update search_paths() to use GPU-provided context offsets

## Current Task
Awaiting next task

## Learnings
- turbo_search_kernel now uses `(device const uchar*)data` to scan backward/forward for newlines from match position
- context_start is offset-within-chunk of line start; context_len is min(line_end - line_start, MAX_CONTEXT)
- column field preserved as raw byte offset for backward compatibility
- turbo_search_kernel writes raw byte offsets; CPU scans backward/forward to newlines (70-79% overhead at scale)
- content_search_kernel already does GPU-side newline scanning (lines 178-201) — proves the approach works in MSL
- load_paths() uses to_string_lossy() which preserves absolute prefix; bug 2 may be a context extraction issue not a storage issue
- chunk_data is contiguous Vec<u8> — backward scan from any abs_offset works correctly across chunk boundaries
- PathCache needs no new crates — std::fs + custom binary format sufficient
- Device buffer reads in MSL are fast on unified memory — no penalty for scattered newline scans
- MAX_CONTEXT is 512 bytes in shader.rs — sufficient for macOS paths (max ~1024)
- Existing test suite covers 5-path to 1M-path scales but lacks chunk boundary and unicode tests
- search_paths() now uses GPU-provided context_start/context_len directly — no CPU newline scanning
- abs_start = chunk_idx * CHUNK_SIZE + m.context_start; abs_end = (abs_start + context_len).min(data.len())
- All 9 path_search tests pass including 1M-path scale test — GPU offsets work correctly end-to-end
