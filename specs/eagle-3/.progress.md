---
spec: eagle-3
basePath: specs/eagle-3
phase: design
task: 0/0
updated: 2026-02-17
---

# Progress: eagle-3

## Original Goal

Implement EAGLE-3 speculative decoding for Mistral-7B GPU inference on Apple Silicon M4 Pro. Currently at 42.6 tok/s decode, target 100+ tok/s (2.43x speedup). EAGLE-3 adds a lightweight prediction head (~144MB Q4_0) attached to internal transformer layers that predicts draft tokens WITHOUT needing a separate draft model. The head reads hidden states from layers 0, 16, and 31 (low/mid/high) and produces draft logit distributions. Multiple draft tokens (N=6) generated via autoregressive chain through the head, then verified in one batched forward_prompt_logits call through full Mistral-7B. Reuses existing speculative decode infrastructure (verification, rollback, greedy acceptance).

## Completed Tasks

- [x] Research phase (via foreman PM agent) - EAGLE-3 architecture, bandwidth analysis, risk assessment
- [x] Requirements phase (via foreman PM agent) - User stories US-5..US-8, FR-8..FR-15, NFR-8..NFR-14
- [x] Design phase (via foreman Tech agent) - Component design, data flow, file structure, implementation steps
- [x] UX/DX design (via foreman UX agent) - CLI interface, error handling, debug modes
- [x] QA strategy (via foreman QA agent) - 4-tier test strategy, tolerance table, CI integration
- [x] 1.1 Add EagleCaptureBuffers and hidden state tap to GpuForwardPass
- [x] 1.2 Create concat_buffers Metal shader
- [x] 1.3 Create eagle_head.rs with EagleHead struct and random-weight init
- [x] 1.4 Implement forward_draft_token GPU pipeline in EagleHead

## Current Task

Awaiting next task

## Learnings

- EAGLE-3 multi-layer feature fusion (layers 0, N/2, N-1) captures richer features than top-layer-only
- Draft head is ~1.9% of target model params (~144MB Q4_0 for Mistral-7B)
- v1 uses chain drafting (linear, no tree) to reuse existing forward_prompt_logits verification
- SafeTensors is the standard weight format for EAGLE heads on HuggingFace
- At 60% acceptance rate, projected ~237 effective tok/s (massively exceeds 100 target)
- All existing Metal kernels can be reused for EAGLE head compute
- Task planning: buffer_copy insert point is after line 495 in forward_token (after FFN down+residual), hidden_a contains post-layer hidden state
- Task planning: encode_buffer_copy() helper already exists at line 2658, takes (encoder, src, dst, count_f32_elements)
- Task planning: EagleHead needs its own PsoCache and scratch buffers -- cannot share with GpuForwardPass due to borrow conflicts
- Task planning: build.rs auto-discovers .metal files, but cargo clean -p metal-attention-kernels needed to trigger recompilation
- Task planning: GpuWeightStore exposes embed(), lm_head(), lm_head_q6k(), lm_head_q8() -- EagleHead should borrow these from target
- Task planning: GpuKVCache supports both CPU-side (append_kv) and GPU-side (encode_kv_append) append -- eagle head uses GPU-side
- Task planning: forward_prompt_logits returns Vec<Vec<f32>> -- one logit vector per position, directly usable for verification
- Task planning: existing SpecStats struct in speculative.rs can be reused as-is for EAGLE chain stats
- Task planning: POC uses F32 FC weights (not Q4_0) to simplify initial implementation -- matvec_f32_v2 kernel used
- Task planning: Eagle KV cache is tiny (1 layer, max 64 positions) -- GpuKVCache::new(device, 64, kv_dim)
- Task planning: embed_lookup pattern from gpu_forward_pass.rs line 1441 can be replicated for prev_token embedding in eagle head

## Blockers

- None currently

## Learnings (task 1.3)

- MTLBuffer contents() returns NonNull<c_void>, must use .contents().as_ptr() as *mut f32 (not direct cast)
- Private storage buffers (alloc_buffer_private) for scratch, shared (alloc_buffer) for CPU-readable argmax result
- Norm weights initialized to ~1.0 (not random) to avoid NaN during RMSNorm
- Added `mod eagle_head;` (private) to lib.rs for compilation; task 1.7 will make it `pub mod`

## Learnings (task 1.4)

- EagleHead needs `device: &'static GpuDevice` field for command buffer creation (not available in task 1.3 struct)
- Added `embed_scratch` (shared buffer for CPU embed lookup) and `final_norm` weight buffer for output projection norm
- Added `rope_theta` and `rms_norm_eps` fields (defaulting to 10000.0 and 1e-5 for POC)
- No `matvec_f32_accumulate` kernel exists; O proj + residual done as separate matvec_f32_v2 + residual_add_inplace dispatches
- Added `residual_add_inplace`, `matvec_q6_k`, `matvec_q8_0` to PSO prewarm (needed for decoder layer and lm_head variants)
- forward_draft_token pipeline: concat3 -> fc_fuse -> concat2 -> fc_concat -> copy -> decoder(rmsnorm->QKV->RoPE->KV->attn->O+res->FFN) -> final_norm -> lm_head -> argmax
- All encode_* helpers duplicated from GpuForwardPass pattern into EagleHead impl (cannot share methods across structs)

## Next

Task 1.5: Quality checkpoint (cargo build + cargo clippy)
