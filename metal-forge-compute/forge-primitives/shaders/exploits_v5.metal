// exploits_v5.metal -- Wave 5: Hardware probing & SIMD-as-CPU experiments.
//
// Two tracks:
//   1. Hardware probing: Discover register file size, ILP width, dual-issue
//      capability, and cache hierarchy via microbenchmarks
//   2. SIMD-as-CPU: Treat each SIMD lane as an independent processor —
//      32-stage pipelines, bitonic sort in registers, divergent task dispatch

#include <metal_stdlib>
#include "types.h"
using namespace metal;

// ============================================================================
// TRACK 1: HARDWARE PROBING
// ============================================================================

// ============================================================================
// Experiment 1: Register Pressure Cliff
//
// Apple Silicon GPUs have a fixed register file per core. When a kernel uses
// too many registers, occupancy drops (fewer threads can run simultaneously).
// At some threshold, performance falls off a cliff.
//
// We vary the number of live float registers per thread: 8, 16, 32, 64, 128.
// Each kernel keeps N floats alive and does dependent FMAs across all of them.
// The performance cliff reveals the register file size.
// ============================================================================

#define DEFINE_REGPRESSURE_KERNEL(N) \
kernel void exploit_regpressure_##N( \
    device const float* input  [[buffer(0)]], \
    device float*       output [[buffer(1)]], \
    constant ExploitParams& params [[buffer(2)]], \
    uint tid [[thread_position_in_grid]]) \
{ \
    if (tid >= params.element_count) return; \
    float r[N]; \
    for (int i = 0; i < N; i++) { \
        r[i] = input[(tid + uint(i) * 997u) % params.element_count]; \
    } \
    for (uint p = 0; p < params.num_passes; p++) { \
        for (int i = 0; i < N; i++) { \
            r[i] = fma(r[i], r[(i + 1) & (N - 1)], r[(i + 3) & (N - 1)]); \
        } \
    } \
    float sum = 0.0f; \
    for (int i = 0; i < N; i++) sum += r[i]; \
    output[tid] = sum; \
}

DEFINE_REGPRESSURE_KERNEL(8)
DEFINE_REGPRESSURE_KERNEL(16)
DEFINE_REGPRESSURE_KERNEL(32)
DEFINE_REGPRESSURE_KERNEL(64)
DEFINE_REGPRESSURE_KERNEL(128)

// ============================================================================
// Experiment 2: Instruction-Level Parallelism (ILP) Width
//
// Modern GPUs hide latency with both thread-level parallelism AND ILP.
// Independent instructions can fill pipeline bubbles.
//
// We run 1, 2, 4, or 8 INDEPENDENT FMA chains. Each chain is a serial
// dependency (a = fma(a, a, c)), but chains are independent of each other.
// If the GPU can issue K FMAs per cycle, throughput scales linearly up to K,
// then saturates. The saturation point reveals the ALU pipeline depth.
// ============================================================================

kernel void exploit_ilp_1(
    device const float* input  [[buffer(0)]],
    device float*       output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;
    float a = input[tid];
    for (uint p = 0; p < params.num_passes; p++) {
        a = fma(a, a, 1.0f);
    }
    output[tid] = a;
}

kernel void exploit_ilp_2(
    device const float* input  [[buffer(0)]],
    device float*       output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;
    float a = input[tid];
    float b = a * 1.1f;
    for (uint p = 0; p < params.num_passes; p++) {
        a = fma(a, a, 1.0f);
        b = fma(b, b, 1.0f);
    }
    output[tid] = a + b;
}

kernel void exploit_ilp_4(
    device const float* input  [[buffer(0)]],
    device float*       output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;
    float a = input[tid], b = a * 1.1f, c = a * 1.2f, d = a * 1.3f;
    for (uint p = 0; p < params.num_passes; p++) {
        a = fma(a, a, 1.0f);
        b = fma(b, b, 1.0f);
        c = fma(c, c, 1.0f);
        d = fma(d, d, 1.0f);
    }
    output[tid] = a + b + c + d;
}

kernel void exploit_ilp_8(
    device const float* input  [[buffer(0)]],
    device float*       output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;
    float v = input[tid];
    float a = v, b = v*1.1f, c = v*1.2f, d = v*1.3f;
    float e = v*1.4f, f = v*1.5f, g = v*1.6f, h = v*1.7f;
    for (uint p = 0; p < params.num_passes; p++) {
        a = fma(a, a, 1.0f); b = fma(b, b, 1.0f);
        c = fma(c, c, 1.0f); d = fma(d, d, 1.0f);
        e = fma(e, e, 1.0f); f = fma(f, f, 1.0f);
        g = fma(g, g, 1.0f); h = fma(h, h, 1.0f);
    }
    output[tid] = a + b + c + d + e + f + g + h;
}

// ============================================================================
// Experiment 3: FP32 + INT32 Dual-Issue Probe
//
// Some GPUs (like NVIDIA Turing+) have separate FP32 and INT32 pipelines
// that execute simultaneously. If Apple Silicon has this, interleaving
// FP and INT ops should give ~2x throughput vs pure FP or pure INT.
//
// Three variants:
//   - Pure FP32: back-to-back FMA
//   - Pure INT32: back-to-back integer multiply-add
//   - Interleaved: alternating FP32 and INT32 ops
// ============================================================================

kernel void exploit_fp32_only(
    device const float* input  [[buffer(0)]],
    device float*       output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;
    float a = input[tid], b = a * 1.1f;
    for (uint p = 0; p < params.num_passes; p++) {
        a = fma(a, b, 1.0f);
        b = fma(b, a, 1.0f);
    }
    output[tid] = a + b;
}

kernel void exploit_int32_only(
    device const float* input  [[buffer(0)]],
    device float*       output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;
    uint a = as_type<uint>(input[tid]);
    uint b = a ^ 0x12345678u;
    for (uint p = 0; p < params.num_passes; p++) {
        a = a * 0x9E3779B9u + b;
        b = b * 0x517CC1B7u + a;
    }
    output[tid] = as_type<float>(a ^ b);
}

kernel void exploit_dual_issue(
    device const float* input  [[buffer(0)]],
    device float*       output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;
    float fa = input[tid], fb = fa * 1.1f;
    uint  ia = as_type<uint>(fa), ib = ia ^ 0x12345678u;
    for (uint p = 0; p < params.num_passes; p++) {
        fa = fma(fa, fb, 1.0f);          // FP32
        ia = ia * 0x9E3779B9u + ib;      // INT32
        fb = fma(fb, fa, 1.0f);          // FP32
        ib = ib * 0x517CC1B7u + ia;      // INT32
    }
    output[tid] = fa + fb + as_type<float>(ia ^ ib);
}

// ============================================================================
// Experiment 4: Cache Hierarchy Probe (Pointer Chasing)
//
// Pointer chasing measures LATENCY, not bandwidth. Each load depends on
// the previous load's result, so prefetching is impossible.
//
// At different working set sizes, latency jumps reveal cache boundaries:
//   - < ~8KB: L1 cache (~4 cycles)
//   - ~8KB - ~24MB: SLC (~30 cycles)
//   - > ~24MB: DRAM (~100+ cycles)
//
// The Rust host creates a random permutation (Sattolo cycle) of indices
// within the working set and dispatches the same kernel with different
// buffer sizes to probe each cache level.
// ============================================================================

kernel void exploit_cache_probe(
    device const uint* chain  [[buffer(0)]],
    device uint*       output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;

    // Start at a unique position to spread access across cache sets
    uint idx = tid % params.mode;  // mode = working set size (elements)

    // Follow the pointer chain — EACH load depends on the previous
    for (uint step = 0; step < params.num_passes; step++) {
        idx = chain[idx];
    }

    output[tid] = idx;
}


// ============================================================================
// TRACK 2: SIMD-AS-CPU (Each Lane = Independent Processor)
// ============================================================================

// ============================================================================
// Experiment 5: SIMD 32-Tap FIR Filter (Pipeline in Registers)
//
// Each SIMD group implements a 32-tap FIR filter ENTIRELY in registers.
// Thread i holds sample[i]. Samples rotate through all 32 lanes via
// simd_shuffle_rotate_down, and each lane accumulates its filtered output.
//
// This is ZERO-MEMORY convolution: no threadgroup memory, no barriers,
// no global memory reads beyond the initial load. Data flows between
// lanes at register speed (~1 cycle per shuffle).
//
// Compare: SIMD shuffle FIR  vs  Memory-based FIR (32 reads per output)
// ============================================================================

kernel void exploit_simd_fir(
    device const float* input  [[buffer(0)]],
    device float*       output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid  [[thread_position_in_grid]],
    uint stid [[thread_index_in_simdgroup]])
{
    if (tid >= params.element_count) return;

    float my_sample = input[tid];
    float result = 0.0f;

    // Each lane applies a decaying filter weight
    // Total: 32 multiply-accumulates per output sample, all in registers
    float rotated = my_sample;
    for (uint tap = 0; tap < 32; tap++) {
        float coeff = 1.0f / float(tap + 1);
        result += rotated * coeff;
        rotated = simd_shuffle_rotate_down(rotated, 1);
    }

    output[tid] = result;
}

kernel void exploit_memory_fir(
    device const float* input  [[buffer(0)]],
    device float*       output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;

    // Same 32-tap filter but reading neighbors from global memory
    float result = 0.0f;
    for (uint tap = 0; tap < 32; tap++) {
        float coeff = 1.0f / float(tap + 1);
        uint idx = tid + tap;
        if (idx < params.element_count) {
            result += input[idx] * coeff;
        }
    }

    output[tid] = result;
}

// Threadgroup-memory version: load neighborhood into shared, then filter
kernel void exploit_tgmem_fir(
    device const float* input  [[buffer(0)]],
    device float*       output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid  [[thread_position_in_grid]],
    uint ltid [[thread_position_in_threadgroup]])
{
    if (tid >= params.element_count) return;

    // Load data + halo into threadgroup memory
    threadgroup float shared[256 + 32];
    shared[ltid] = input[tid];
    if (ltid < 32) {
        uint halo_idx = tid + 256;
        shared[256 + ltid] = (halo_idx < params.element_count) ? input[halo_idx] : 0.0f;
    }
    threadgroup_barrier(mem_flags::mem_threadgroup);

    // 32-tap FIR from shared memory
    float result = 0.0f;
    for (uint tap = 0; tap < 32; tap++) {
        float coeff = 1.0f / float(tap + 1);
        result += shared[ltid + tap] * coeff;
    }

    output[tid] = result;
}

// ============================================================================
// Experiment 6: Bitonic Sort in SIMD Registers
//
// Sort 32 elements using ONLY simd_shuffle_xor — no threadgroup memory,
// no barriers, no global memory access after the initial load.
//
// A bitonic sorting network for 32 elements has 5 stages with a total of
// 15 compare-and-swap steps. Each step uses simd_shuffle_xor(val, distance)
// to fetch the partner's value, then keeps min or max based on position.
//
// This is the fastest possible sort for 32 elements on any GPU.
// ============================================================================

kernel void exploit_simd_sort32(
    device const float* input  [[buffer(0)]],
    device float*       output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid  [[thread_position_in_grid]],
    uint stid [[thread_index_in_simdgroup]])
{
    uint sg_id = tid / 32;
    if (sg_id * 32 >= params.element_count) return;

    float val = input[sg_id * 32 + stid];

    // Bitonic sort: 5 stages, 15 compare-and-swap steps
    // Stage k: merge 2^(k+1)-element bitonic sequences
    for (int k = 0; k < 5; k++) {
        for (int j = k; j >= 0; j--) {
            uint d = 1u << uint(j);
            float partner = simd_shuffle_xor(val, d);

            // Ascending if in lower half of 2^(k+1) block
            bool ascending = ((stid >> uint(k + 1)) & 1u) == 0u;
            bool is_lower  = (stid & d) == 0u;

            if (ascending == is_lower) {
                val = min(val, partner);
            } else {
                val = max(val, partner);
            }
        }
    }

    output[sg_id * 32 + stid] = val;
}

// Baseline: Same bitonic sort but using threadgroup memory for exchanges
kernel void exploit_tg_sort32(
    device const float* input  [[buffer(0)]],
    device float*       output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid  [[thread_position_in_grid]],
    uint ltid [[thread_position_in_threadgroup]],
    uint tg_id [[threadgroup_position_in_grid]])
{
    threadgroup float shared[256];

    uint global_idx = tg_id * 256 + ltid;
    if (global_idx >= params.element_count) return;

    // Each group of 32 sorts independently
    uint group_in_tg = ltid / 32;
    uint lane = ltid % 32;

    float val = input[global_idx];
    shared[ltid] = val;
    threadgroup_barrier(mem_flags::mem_threadgroup);

    for (int k = 0; k < 5; k++) {
        for (int j = k; j >= 0; j--) {
            uint d = 1u << uint(j);
            uint partner_lane = lane ^ d;
            float partner = shared[group_in_tg * 32 + partner_lane];

            bool ascending = ((lane >> uint(k + 1)) & 1u) == 0u;
            bool is_lower  = (lane & d) == 0u;

            if (ascending == is_lower) {
                val = min(val, partner);
            } else {
                val = max(val, partner);
            }

            shared[ltid] = val;
            threadgroup_barrier(mem_flags::mem_threadgroup);
        }
    }

    output[global_idx] = val;
}

// ============================================================================
// Experiment 7: Divergent Task Dispatch (32 Independent CPUs)
//
// The ultimate test of "SIMD lanes as independent processors."
// Each group of 8 lanes executes a STRUCTURALLY DIFFERENT computation:
//   - Lanes  0-7:  Cryptographic hash (integer multiply-xor chain)
//   - Lanes  8-15: Polynomial evaluation (FMA chain)
//   - Lanes 16-23: Bit manipulation (popcount, clz, extract_bits)
//   - Lanes 24-31: Transcendental math (sin, cos)
//
// On NVIDIA: these 4 code paths serialize → 4x slowdown vs uniform.
// On Apple Silicon: claimed zero divergence cost → should match uniform.
//
// We also include a SIMD FSM (finite state machine) where each lane
// independently transitions through states based on input — the most
// extreme form of "each lane is a CPU."
// ============================================================================

kernel void exploit_divergent_tasks(
    device const float* input  [[buffer(0)]],
    device float*       output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid  [[thread_position_in_grid]],
    uint stid [[thread_index_in_simdgroup]])
{
    if (tid >= params.element_count) return;

    float val = input[tid];
    float result;
    uint group = stid / 8;  // 4 groups of 8 lanes

    if (group == 0) {
        // Hash: integer multiply-xor chain
        uint h = as_type<uint>(val);
        for (uint i = 0; i < params.num_passes; i++) {
            h *= 0x9E3779B9u;
            h ^= h >> 16;
            h *= 0x85EBCA6Bu;
            h ^= h >> 13;
        }
        result = as_type<float>(h);
    } else if (group == 1) {
        // Polynomial: Horner's method FMA chain
        float x = val;
        for (uint i = 0; i < params.num_passes; i++) {
            x = fma(x, x, fma(x, 0.5f, -0.3f));
        }
        result = x;
    } else if (group == 2) {
        // Bit manipulation: popcount + clz + shifts
        uint b = as_type<uint>(val);
        for (uint i = 0; i < params.num_passes; i++) {
            b = (popcount(b) << 4) ^ (clz(b) << 2) ^ (b >> 1);
            b = b * 0x01000193u;  // FNV prime
        }
        result = as_type<float>(b);
    } else {
        // Transcendental: sin + cos
        float x = val * 0.001f;
        for (uint i = 0; i < params.num_passes; i++) {
            x = sin(x) * 0.99f + cos(x * 0.7f) * 0.01f;
        }
        result = x;
    }

    output[tid] = result;
}

// Uniform baseline: ALL lanes do the hash computation (no divergence)
kernel void exploit_uniform_hash(
    device const float* input  [[buffer(0)]],
    device float*       output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;

    uint h = as_type<uint>(input[tid]);
    for (uint i = 0; i < params.num_passes; i++) {
        h *= 0x9E3779B9u;
        h ^= h >> 16;
        h *= 0x85EBCA6Bu;
        h ^= h >> 13;
    }
    output[tid] = as_type<float>(h);
}

// Uniform baseline: ALL lanes do polynomial (no divergence)
kernel void exploit_uniform_poly(
    device const float* input  [[buffer(0)]],
    device float*       output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;

    float x = input[tid];
    for (uint i = 0; i < params.num_passes; i++) {
        x = fma(x, x, fma(x, 0.5f, -0.3f));
    }
    output[tid] = x;
}

// Uniform baseline: ALL lanes do bit manipulation (no divergence)
kernel void exploit_uniform_bits(
    device const float* input  [[buffer(0)]],
    device float*       output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;

    uint b = as_type<uint>(input[tid]);
    for (uint i = 0; i < params.num_passes; i++) {
        b = (popcount(b) << 4) ^ (clz(b) << 2) ^ (b >> 1);
        b = b * 0x01000193u;
    }
    output[tid] = as_type<float>(b);
}

// Uniform baseline: ALL lanes do transcendental (no divergence)
kernel void exploit_uniform_trig(
    device const float* input  [[buffer(0)]],
    device float*       output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;

    float x = input[tid] * 0.001f;
    for (uint i = 0; i < params.num_passes; i++) {
        x = sin(x) * 0.99f + cos(x * 0.7f) * 0.01f;
    }
    output[tid] = x;
}

// ============================================================================
// Bonus: SIMD Finite State Machine
//
// Each lane is an independent finite state machine processing a different
// input stream. States transition based on input symbols. This is the
// ultimate "lane as CPU" experiment — 32 independent programs running
// in a single SIMD group.
// ============================================================================

kernel void exploit_simd_fsm(
    device const uint* input  [[buffer(0)]],
    device uint*       output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid  [[thread_position_in_grid]],
    uint stid [[thread_index_in_simdgroup]])
{
    if (tid >= params.element_count) return;

    // Each lane starts in a different state
    uint state = stid;

    // Process input symbols — each lane reads from a different stream offset
    for (uint step = 0; step < params.num_passes; step++) {
        uint sym = input[(tid * params.num_passes + step) % params.element_count];
        uint nibble = sym & 0xFu;

        // State transition — MAXIMALLY divergent: every state range has
        // different logic, and each lane has a different state
        if (state < 8u) {
            state = (nibble + state) & 0x1Fu;
        } else if (state < 16u) {
            state = (state * nibble + 1u) & 0x1Fu;
        } else if (state < 24u) {
            state = popcount(state ^ sym) & 0x1Fu;
        } else {
            state = ((state << 1) ^ nibble) & 0x1Fu;
        }
    }

    output[tid] = state;
}
