// exploits_v7.metal — Wave 7: GPU OS Primitives
//
// 4 experiment groups, 9 kernels:
// 1. Lock-Free Hash Table: insert, lookup, mixed (3 kernels)
// 2. MPMC Queue: throughput (1 kernel)
// 3. Work-Stealing Scheduler: init + process (2 kernels)
// 4. Tick Chain: persistent state processing (1 kernel)
// 5. SIMD FSM Runtime: independent + coupled (2 kernels)
//
// KB foundations:
//   Finding 3240: stride-1 access mandatory (30x cliff at stride-32)
//   Finding 3239: per-SIMD atomic aggregation (2.26x relief)
//   Finding 3231: SLC retains 16MB at 465 GB/s
//   Finding 3229: cross-TG device atomics 100% reliable

#include "types.h"

// ─── CONSTANTS ──────────────────────────────────────────────────

constant uint EMPTY_KEY = 0xFFFFFFFF;
constant uint MAX_PROBE = 64;

// ─── HELPER ─────────────────────────────────────────────────────

inline uint gpu_hash(uint key) {
    key ^= key >> 16;
    key *= 0x45d9f3b;
    key ^= key >> 16;
    return key;
}

// MurmurHash3 finalizer — better avalanche than gpu_hash.
// Each input bit affects every output bit (full avalanche).
inline uint murmur3_hash(uint key) {
    key ^= key >> 16;
    key *= 0x85ebca6b;
    key ^= key >> 13;
    key *= 0xc2b2ae35;
    key ^= key >> 16;
    return key;
}


// ═══════════════════════════════════════════════════════════════
// 1. LOCK-FREE HASH TABLE
// ═══════════════════════════════════════════════════════════════
//
// Open-addressing, linear probing (stride-1 per KB finding 3240).
// Atomic CAS on key slots, relaxed ordering (MSL limitation).
// Capacity must be power-of-2 for bitmask indexing.

/// Insert keys into lock-free hash table.
kernel void gpuos_ht_insert(
    device atomic_uint* keys         [[buffer(0)]],
    device uint* values              [[buffer(1)]],
    device const uint* input_keys    [[buffer(2)]],
    device const uint* input_values  [[buffer(3)]],
    constant GpuOsParams& params     [[buffer(4)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.num_ops) return;

    uint key = input_keys[tid];
    uint val = input_values[tid];
    if (key == EMPTY_KEY) key = 0xFFFFFFFE;

    uint slot = gpu_hash(key) & (params.capacity - 1);

    for (uint probe = 0; probe < MAX_PROBE; probe++) {
        uint expected = EMPTY_KEY;
        bool ok = atomic_compare_exchange_weak_explicit(
            &keys[slot], &expected, key,
            memory_order_relaxed, memory_order_relaxed);

        if (ok || expected == key) {
            values[slot] = val;
            return;
        }
        slot = (slot + 1) & (params.capacity - 1);
    }
}

/// Lookup keys in lock-free hash table.
kernel void gpuos_ht_lookup(
    device const atomic_uint* keys   [[buffer(0)]],
    device const uint* values        [[buffer(1)]],
    device const uint* query_keys    [[buffer(2)]],
    device uint* output              [[buffer(3)]],
    constant GpuOsParams& params     [[buffer(4)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.num_ops) return;

    uint key = query_keys[tid];
    if (key == EMPTY_KEY) key = 0xFFFFFFFE;

    uint slot = gpu_hash(key) & (params.capacity - 1);

    for (uint probe = 0; probe < MAX_PROBE; probe++) {
        uint slot_key = atomic_load_explicit(&keys[slot], memory_order_relaxed);

        if (slot_key == key) {
            output[tid] = values[slot];
            return;
        }
        if (slot_key == EMPTY_KEY) {
            output[tid] = EMPTY_KEY;
            return;
        }
        slot = (slot + 1) & (params.capacity - 1);
    }
    output[tid] = EMPTY_KEY;
}

/// Mixed insert/lookup. Even threads insert, odd threads lookup.
kernel void gpuos_ht_mixed(
    device atomic_uint* keys         [[buffer(0)]],
    device uint* values              [[buffer(1)]],
    device const uint* input_keys    [[buffer(2)]],
    device uint* output              [[buffer(3)]],
    constant GpuOsParams& params     [[buffer(4)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.num_ops) return;

    uint key = input_keys[tid];
    if (key == EMPTY_KEY) key = 0xFFFFFFFE;
    uint slot = gpu_hash(key) & (params.capacity - 1);

    if (tid & 1) {
        // Odd: lookup
        for (uint probe = 0; probe < MAX_PROBE; probe++) {
            uint slot_key = atomic_load_explicit(&keys[slot], memory_order_relaxed);
            if (slot_key == key) { output[tid] = values[slot]; return; }
            if (slot_key == EMPTY_KEY) { output[tid] = EMPTY_KEY; return; }
            slot = (slot + 1) & (params.capacity - 1);
        }
        output[tid] = EMPTY_KEY;
    } else {
        // Even: insert
        for (uint probe = 0; probe < MAX_PROBE; probe++) {
            uint expected = EMPTY_KEY;
            bool ok = atomic_compare_exchange_weak_explicit(
                &keys[slot], &expected, key,
                memory_order_relaxed, memory_order_relaxed);
            if (ok || expected == key) {
                values[slot] = tid;
                output[tid] = 1;
                return;
            }
            slot = (slot + 1) & (params.capacity - 1);
        }
        output[tid] = 0;
    }
}


// ═══════════════════════════════════════════════════════════════
// 1b. LOCK-FREE HASH TABLE V2 — Optimized
// ═══════════════════════════════════════════════════════════════
//
// V2: 1 key/thread (same TLP as v1) with two targeted optimizations:
//   1. MurmurHash3 finalizer: better avalanche → fewer collisions → shorter probes
//   2. Non-atomic lookup: plain device reads after insert completes
//
// ILP-4 (4 keys/thread) was tried and HURT performance — reduced TLP by 4x,
// which matters more than ILP for memory-latency-bound workloads (KB 3216).
// Dispatch size: N threads (same as v1).

/// V2 Insert: 1 key/thread, MurmurHash3 for better slot distribution.
/// Same CAS logic as v1 — only change is hash quality.
kernel void gpuos_ht_insert_v2(
    device atomic_uint* keys         [[buffer(0)]],
    device uint* values              [[buffer(1)]],
    device const uint* input_keys    [[buffer(2)]],
    device const uint* input_values  [[buffer(3)]],
    constant GpuOsParams& params     [[buffer(4)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.num_ops) return;

    uint key = input_keys[tid];
    uint val = input_values[tid];
    if (key == EMPTY_KEY) key = 0xFFFFFFFE;

    uint slot = murmur3_hash(key) & (params.capacity - 1);

    for (uint probe = 0; probe < MAX_PROBE; probe++) {
        uint expected = EMPTY_KEY;
        bool ok = atomic_compare_exchange_weak_explicit(
            &keys[slot], &expected, key,
            memory_order_relaxed, memory_order_relaxed);

        if (ok || expected == key) {
            values[slot] = val;
            return;
        }
        slot = (slot + 1) & (params.capacity - 1);
    }
}

/// V2 Lookup: non-atomic reads + MurmurHash3.
/// After insert command buffer completes, table is coherent — no atomics needed.
/// Plain `device const uint*` instead of `device const atomic_uint*`.
kernel void gpuos_ht_lookup_v2(
    device const uint* keys          [[buffer(0)]],  // NOT atomic — read-only
    device const uint* values        [[buffer(1)]],
    device const uint* query_keys    [[buffer(2)]],
    device uint* output              [[buffer(3)]],
    constant GpuOsParams& params     [[buffer(4)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.num_ops) return;

    uint key = query_keys[tid];
    if (key == EMPTY_KEY) key = 0xFFFFFFFE;

    uint slot = murmur3_hash(key) & (params.capacity - 1);

    for (uint probe = 0; probe < MAX_PROBE; probe++) {
        uint slot_key = keys[slot];  // Plain load — no atomic needed

        if (slot_key == key) {
            output[tid] = values[slot];
            return;
        }
        if (slot_key == EMPTY_KEY) {
            output[tid] = EMPTY_KEY;
            return;
        }
        slot = (slot + 1) & (params.capacity - 1);
    }
    output[tid] = EMPTY_KEY;
}

/// V2 Mixed: 1 key/thread, MurmurHash3, even=insert odd=lookup.
/// Mixed uses atomic_load for lookups since inserts happen concurrently.
kernel void gpuos_ht_mixed_v2(
    device atomic_uint* keys         [[buffer(0)]],
    device uint* values              [[buffer(1)]],
    device const uint* input_keys    [[buffer(2)]],
    device uint* output              [[buffer(3)]],
    constant GpuOsParams& params     [[buffer(4)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.num_ops) return;

    uint key = input_keys[tid];
    if (key == EMPTY_KEY) key = 0xFFFFFFFE;
    uint slot = murmur3_hash(key) & (params.capacity - 1);

    if (tid & 1) {
        // Odd: lookup
        for (uint probe = 0; probe < MAX_PROBE; probe++) {
            uint slot_key = atomic_load_explicit(&keys[slot], memory_order_relaxed);
            if (slot_key == key) { output[tid] = values[slot]; return; }
            if (slot_key == EMPTY_KEY) { output[tid] = EMPTY_KEY; return; }
            slot = (slot + 1) & (params.capacity - 1);
        }
        output[tid] = EMPTY_KEY;
    } else {
        // Even: insert
        for (uint probe = 0; probe < MAX_PROBE; probe++) {
            uint expected = EMPTY_KEY;
            bool ok = atomic_compare_exchange_weak_explicit(
                &keys[slot], &expected, key,
                memory_order_relaxed, memory_order_relaxed);
            if (ok || expected == key) {
                values[slot] = tid;
                output[tid] = 1;
                return;
            }
            slot = (slot + 1) & (params.capacity - 1);
        }
        output[tid] = 0;
    }
}


// ═══════════════════════════════════════════════════════════════
// 2. MPMC QUEUE
// ═══════════════════════════════════════════════════════════════
//
// Lock-free ring buffer. head_tail[0] = head, head_tail[1] = tail.
// Each thread does num_queues cycles of enqueue+dequeue.
// Per-SIMD aggregation of successful ops (KB finding 3239).

kernel void gpuos_queue_throughput(
    device atomic_uint* head_tail    [[buffer(0)]],
    device uint* ring_data           [[buffer(1)]],
    device atomic_uint* result_count [[buffer(2)]],
    constant GpuOsParams& params     [[buffer(3)]],
    uint tid [[thread_position_in_grid]],
    uint simd_lane [[thread_index_in_simdgroup]])
{
    if (tid >= params.num_ops) return;

    uint cap_mask = params.capacity - 1;
    uint ops = 0;

    for (uint cycle = 0; cycle < params.num_queues; cycle++) {
        // Enqueue
        uint tail = atomic_fetch_add_explicit(&head_tail[1], 1, memory_order_relaxed);
        ring_data[tail & cap_mask] = tid ^ cycle;

        // Dequeue
        uint head = atomic_fetch_add_explicit(&head_tail[0], 1, memory_order_relaxed);
        uint val = ring_data[head & cap_mask];
        if (val != 0) ops++;
    }

    // Per-SIMD aggregation (KB finding 3239: 2.26x atomic relief)
    uint total = simd_sum(ops);
    if (simd_lane == 0) {
        atomic_fetch_add_explicit(&result_count[0], total, memory_order_relaxed);
    }
}


// ═══════════════════════════════════════════════════════════════
// 3. WORK-STEALING SCHEDULER
// ═══════════════════════════════════════════════════════════════
//
// Per-TG work queues in device memory.
// Queue layout: each TG i owns queue_data[i * queue_stride .. (i+1) * queue_stride]
//   [0] = head (atomic dequeue pointer)
//   [1] = tail (item count, set during init)
//   [2..capacity+2) = task items
// Cross-TG stealing via device atomics (KB finding 3229: 100% reliable).

/// Initialize per-TG work queues.
/// mode 0/1: balanced distribution, mode 2: TG 0 gets 90% of tasks.
kernel void gpuos_ws_init(
    device atomic_uint* queues       [[buffer(0)]],
    device atomic_uint* steal_counts [[buffer(1)]],
    constant GpuOsParams& params     [[buffer(2)]],
    uint tg_idx [[threadgroup_position_in_grid]],
    uint local_id [[thread_position_in_threadgroup]])
{
    if (local_id != 0) return;

    uint queue_stride = 2 + params.capacity;
    uint base = tg_idx * queue_stride;
    uint num_tgs = params.num_queues;

    // Reset steal counter
    atomic_store_explicit(&steal_counts[tg_idx], 0, memory_order_relaxed);

    uint tasks;
    if (params.mode == 2 && num_tgs > 1) {
        // Imbalanced: TG 0 gets 90%
        uint total = params.num_ops;
        tasks = (tg_idx == 0) ? (total * 9) / 10 : total / (10 * (num_tgs - 1));
    } else {
        tasks = params.num_ops / num_tgs;
    }
    if (tasks > params.capacity) tasks = params.capacity;

    atomic_store_explicit(&queues[base + 0], 0, memory_order_relaxed);     // head = 0
    atomic_store_explicit(&queues[base + 1], tasks, memory_order_relaxed); // tail = tasks

    for (uint i = 0; i < tasks; i++) {
        atomic_store_explicit(&queues[base + 2 + i], tg_idx * 10000 + i, memory_order_relaxed);
    }
}

/// Process tasks from local queue, then steal from others.
/// mode 0: local only, mode 1: steal enabled, mode 2: imbalanced + steal.
kernel void gpuos_ws_process(
    device atomic_uint* queues       [[buffer(0)]],
    device atomic_uint* steal_counts [[buffer(1)]],
    device atomic_uint* total_done   [[buffer(2)]],
    constant GpuOsParams& params     [[buffer(3)]],
    uint tid [[thread_position_in_grid]],
    uint tg_idx [[threadgroup_position_in_grid]],
    uint local_id [[thread_position_in_threadgroup]],
    uint simd_lane [[thread_index_in_simdgroup]])
{
    uint queue_stride = 2 + params.capacity;
    uint num_tgs = params.num_queues;
    uint done = 0;

    // Phase 1: Drain local queue
    uint my_base = tg_idx * queue_stride;
    for (;;) {
        uint head = atomic_fetch_add_explicit(
            &queues[my_base], 1, memory_order_relaxed);
        uint tail = atomic_load_explicit(
            &queues[my_base + 1], memory_order_relaxed);

        if (head >= tail) break;

        uint task = atomic_load_explicit(
            &queues[my_base + 2 + head], memory_order_relaxed);
        uint h = gpu_hash(task);
        if (h == 0xDEAD) {
            atomic_store_explicit(&queues[my_base + 2], h, memory_order_relaxed);
        }
        done++;
    }

    // Phase 2: Work stealing
    if (params.mode > 0) {
        uint seed = tid ^ 0x9E3779B9u;
        for (uint attempt = 0; attempt < 32; attempt++) {
            seed = gpu_hash(seed);
            uint victim = seed % num_tgs;
            if (victim == tg_idx) continue;

            uint v_base = victim * queue_stride;
            uint head = atomic_fetch_add_explicit(
                &queues[v_base], 1, memory_order_relaxed);
            uint tail = atomic_load_explicit(
                &queues[v_base + 1], memory_order_relaxed);

            if (head >= tail) continue;

            uint task = atomic_load_explicit(
                &queues[v_base + 2 + head], memory_order_relaxed);
            uint h = gpu_hash(task);
            if (h == 0xDEAD) {
                atomic_store_explicit(&queues[v_base + 2], h, memory_order_relaxed);
            }
            done++;
            atomic_fetch_add_explicit(&steal_counts[tg_idx], 1, memory_order_relaxed);
        }
    }

    // Per-SIMD aggregation
    uint total = simd_sum(done);
    if (simd_lane == 0) {
        atomic_fetch_add_explicit(&total_done[0], total, memory_order_relaxed);
    }
}


// ═══════════════════════════════════════════════════════════════
// 4. TICK CHAIN — Persistent State Processing
// ═══════════════════════════════════════════════════════════════
//
// Each encoder dispatch applies one "tick" to state buffer.
// Multiple encoders in one command buffer = simulated persistent kernel.
// State persists in SLC between ticks for sizes < 16MB (KB finding 3231).

kernel void gpuos_tick_process(
    device uint* state               [[buffer(0)]],
    device atomic_uint* tick_counter  [[buffer(1)]],
    constant GpuOsParams& params     [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.capacity) return;

    uint s = state[tid];

    if (params.mode == 0) {
        // Simple increment
        s += 1;
    } else if (params.mode == 1) {
        // Hash-based state machine (8 states)
        uint next_state = (s >> 28) & 0x7;
        uint data = s & 0x0FFFFFFF;
        switch (next_state) {
            case 0: data ^= 0x12345678; next_state = 1; break;
            case 1: data += 0x9E3779B9; next_state = 2; break;
            case 2: data ^= (data >> 16); next_state = 3; break;
            case 3: data *= 0x45d9f3b; next_state = 4; break;
            case 4: data ^= (data >> 13); next_state = 5; break;
            case 5: data += tid; next_state = 6; break;
            case 6: data ^= 0xDEADBEEF; next_state = 7; break;
            case 7: data += 1; next_state = 0; break;
        }
        s = (next_state << 28) | (data & 0x0FFFFFFF);
    } else {
        // Reduction-accumulate: hash current state
        s = gpu_hash(s ^ tid);
    }

    state[tid] = s;

    if (tid == 0) {
        atomic_fetch_add_explicit(&tick_counter[0], 1, memory_order_relaxed);
    }
}


// ═══════════════════════════════════════════════════════════════
// 5. SIMD FSM RUNTIME
// ═══════════════════════════════════════════════════════════════
//
// Each thread runs an independent finite state machine.
// 32 FSMs per SIMD group = 32 independent programs.
// 16 states, 4 input symbols. Transition table in constant memory.

constant uint FSM_NUM_STATES = 16;
constant uint FSM_NUM_INPUTS = 4;

// Transition table: FSM_TABLE[state * 4 + input] = next_state
constant uint FSM_TABLE[64] = {
    1,  3,  5,  0,    // state 0
    2,  0,  4,  6,    // state 1
    7,  1,  3,  5,    // state 2
    4,  8,  0,  2,    // state 3
    9,  5,  1,  7,    // state 4
    6, 10,  2,  4,    // state 5
   11,  7,  3,  9,    // state 6
    8, 12,  4,  6,    // state 7
   13,  9,  5, 11,    // state 8
   10, 14,  6,  8,    // state 9
   15, 11,  7, 13,    // state 10
   12,  0,  8, 10,    // state 11
    1, 13,  9, 15,    // state 12
   14,  2, 10, 12,    // state 13
    3, 15, 11,  1,    // state 14
    0,  4, 12, 14,    // state 15
};

/// Independent FSM: each thread runs num_ops transitions.
kernel void gpuos_fsm_independent(
    device const uint* input_stream  [[buffer(0)]],
    device uint* output_states       [[buffer(1)]],
    device atomic_uint* total_trans  [[buffer(2)]],
    constant GpuOsParams& params     [[buffer(3)]],
    uint tid [[thread_position_in_grid]],
    uint simd_lane [[thread_index_in_simdgroup]])
{
    if (tid >= params.capacity) return;

    uint state = tid & 0xF;
    uint transitions = 0;

    for (uint step = 0; step < params.num_ops; step++) {
        uint input_idx = (tid * params.num_ops + step) % params.capacity;
        uint input = input_stream[input_idx] & 0x3;
        state = FSM_TABLE[state * FSM_NUM_INPUTS + input];
        transitions++;
    }

    output_states[tid] = state;

    uint total = simd_sum(transitions);
    if (simd_lane == 0) {
        atomic_fetch_add_explicit(&total_trans[0], total, memory_order_relaxed);
    }
}

/// Coupled FSM: reads neighbor TG's lane-0 state every 4 steps.
/// Cross-TG via device memory (KB finding 3229: 100% reliable).
/// Eventual consistency — no inter-TG barrier, accept stale reads.
kernel void gpuos_fsm_coupled(
    device uint* states_global       [[buffer(0)]],
    device const uint* input_stream  [[buffer(1)]],
    device uint* output_states       [[buffer(2)]],
    device atomic_uint* total_trans  [[buffer(3)]],
    constant GpuOsParams& params     [[buffer(4)]],
    uint tid [[thread_position_in_grid]],
    uint tg_idx [[threadgroup_position_in_grid]],
    uint local_id [[thread_position_in_threadgroup]],
    uint simd_lane [[thread_index_in_simdgroup]])
{
    if (tid >= params.capacity) return;

    uint state = tid & 0xF;
    uint transitions = 0;
    uint num_tgs = params.num_queues;

    for (uint step = 0; step < params.num_ops; step++) {
        uint input_idx = (tid * params.num_ops + step) % params.capacity;
        uint input = input_stream[input_idx] & 0x3;

        state = FSM_TABLE[state * FSM_NUM_INPUTS + input];
        transitions++;

        // Publish local state (lane 0 per TG)
        if (local_id == 0) {
            states_global[tg_idx] = state;
        }

        // Read neighbor every 4 steps (amortize cross-TG overhead)
        if ((step & 3) == 3 && num_tgs > 1) {
            uint neighbor_tg = (tg_idx + 1) % num_tgs;
            uint neighbor_state = states_global[neighbor_tg];
            input = (input ^ neighbor_state) & 0x3;
            state = FSM_TABLE[state * FSM_NUM_INPUTS + input];
            transitions++;
        }
    }

    output_states[tid] = state;

    uint total = simd_sum(transitions);
    if (simd_lane == 0) {
        atomic_fetch_add_explicit(&total_trans[0], total, memory_order_relaxed);
    }
}
