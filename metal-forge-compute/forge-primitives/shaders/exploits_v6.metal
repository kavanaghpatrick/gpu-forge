// exploits_v6.metal — Wave 6: Micro-architecture Deep Probes
//
// 7 experiment groups, 11 kernels:
// 1. SLC Persistence: bandwidth test across dispatches
// 2. Dispatch Overhead: trivial kernel for overhead isolation
// 3. Memory-Compute Overlap: mem-only, compute-only, interleaved
// 4. Bank Conflicts: threadgroup memory stride sweep
// 5. simdgroup_matrix TOPS: F32 and F16 matrix multiply throughput
// 6. Atomic Throughput: contention sweep across target counts
// 7. Cross-TG Communication: producer-consumer within single dispatch

#include "types.h"

// ─── 1. SLC PERSISTENCE ────────────────────────────────────────

/// Write known pattern to buffer (warms SLC for sizes that fit).
kernel void exploit_slc_write(
    device uint* data            [[buffer(0)]],
    constant ExploitParams& params [[buffer(1)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;
    data[tid] = tid ^ 0xDEADBEEFu;
}

/// Bandwidth read: each thread loads a float4 (16 bytes), reduces via simd_sum.
/// Total bandwidth = element_count * 4 bytes / gpu_time.
kernel void exploit_slc_read_bw(
    device const float4* input   [[buffer(0)]],
    device float*        output  [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]],
    uint simd_lane [[thread_index_in_simdgroup]])
{
    uint n4 = params.element_count / 4;
    if (tid >= n4) return;

    float4 v = input[tid];
    float sum = v.x + v.y + v.z + v.w;

    // Reduce to avoid polluting bandwidth with output writes
    sum = simd_sum(sum);
    if (simd_lane == 0) {
        output[tid / 32] = sum;
    }
}


// ─── 2. DISPATCH OVERHEAD ──────────────────────────────────────

/// Trivial kernel: ~zero compute. Measured time = pure dispatch overhead.
kernel void exploit_dispatch_trivial(
    device uint* output          [[buffer(0)]],
    constant ExploitParams& params [[buffer(1)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid == 0) output[0] = params.element_count;
}


// ─── 3. MEMORY-COMPUTE OVERLAP ─────────────────────────────────

/// Memory-only: strided reads to defeat caching, sum results.
kernel void exploit_mem_only(
    device const float* input    [[buffer(0)]],
    device float*       output   [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;

    float sum = 0.0f;
    uint idx = tid;
    for (uint p = 0; p < params.num_passes; p++) {
        sum += input[idx];
        idx = (idx + 997u) % params.element_count;
    }
    output[tid] = sum;
}

/// Compute-only: pure FMA chains after initial load.
kernel void exploit_compute_only(
    device const float* input    [[buffer(0)]],
    device float*       output   [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;

    float v = input[tid];
    for (uint p = 0; p < params.num_passes; p++) {
        v = fma(v, 1.00001f, 0.00001f);
        v = fma(v, 1.00001f, 0.00001f);
        v = fma(v, 1.00001f, 0.00001f);
        v = fma(v, 1.00001f, 0.00001f);
    }
    output[tid] = v;
}

/// Interleaved: memory load + compute in alternating fashion.
/// If GPU overlaps mem+compute: time ≈ max(mem, compute), not sum.
kernel void exploit_mem_compute_interleaved(
    device const float* input    [[buffer(0)]],
    device float*       output   [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;

    float sum = 0.0f;
    float v = input[tid];
    uint idx = tid;
    for (uint p = 0; p < params.num_passes; p++) {
        // Memory load (potentially overlapped with compute below)
        idx = (idx + 997u) % params.element_count;
        float loaded = input[idx];
        // Compute (independent of load, can execute in parallel)
        v = fma(v, 1.00001f, 0.00001f);
        v = fma(v, 1.00001f, 0.00001f);
        v = fma(v, 1.00001f, 0.00001f);
        v = fma(v, 1.00001f, 0.00001f);
        sum += loaded;
    }
    output[tid] = sum + v;
}


// ─── 4. BANK CONFLICTS ────────────────────────────────────────

/// Threadgroup memory access at parameterized stride.
/// If bank conflicts exist, certain strides (e.g., 32) will show slowdowns.
/// params.mode = stride (1, 2, 4, 8, 16, 32)
kernel void exploit_bank_test(
    device float*        output  [[buffer(0)]],
    constant ExploitParams& params [[buffer(1)]],
    uint tid [[thread_position_in_grid]],
    uint lid [[thread_position_in_threadgroup]],
    uint simd_lane [[thread_index_in_simdgroup]],
    uint simd_id [[simdgroup_index_in_threadgroup]])
{
    // 32KB TG memory — each SIMD group gets a 1024-float region
    threadgroup float smem[8192];

    uint stride = params.mode;
    uint base = simd_id * 1024u;

    float sum = 0.0f;
    for (uint p = 0; p < params.num_passes; p++) {
        uint addr = base + ((simd_lane * stride) & 1023u);
        smem[addr] = float(simd_lane + p);
        threadgroup_barrier(mem_flags::mem_threadgroup);
        sum += smem[addr];
        threadgroup_barrier(mem_flags::mem_threadgroup);
    }

    if (tid < params.element_count) output[tid] = sum;
}


// ─── 5. SIMDGROUP_MATRIX THROUGHPUT ───────────────────────────

/// F32 simdgroup_matrix multiply-accumulate throughput test.
/// Each simdgroup_multiply_accumulate = 512 FMAs = 1024 FLOPs.
kernel void exploit_simd_matmul_f32(
    device float*        output  [[buffer(0)]],
    constant ExploitParams& params [[buffer(1)]],
    uint tid [[thread_position_in_grid]],
    uint simd_id [[simdgroup_index_in_threadgroup]])
{
    simdgroup_float8x8 a, b, c;

    a = simdgroup_float8x8(float(tid & 0xFFu) * 0.001f);
    b = simdgroup_float8x8(float((tid >> 8) & 0xFFu) * 0.001f + 0.5f);
    c = simdgroup_float8x8(0.0f);

    for (uint p = 0; p < params.num_passes; p++) {
        simdgroup_multiply_accumulate(c, a, b, c);
    }

    // Store one tile to prevent dead-code elimination
    if (simd_id == 0 && (tid & 31u) == 0) {
        simdgroup_store(c, output, 8);
    }
}

/// F16 simdgroup_matrix multiply-accumulate throughput test.
/// Expect ~2x throughput vs F32 if dedicated half-precision matrix units exist.
kernel void exploit_simd_matmul_f16(
    device half*         output  [[buffer(0)]],
    constant ExploitParams& params [[buffer(1)]],
    uint tid [[thread_position_in_grid]],
    uint simd_id [[simdgroup_index_in_threadgroup]])
{
    simdgroup_half8x8 a, b, c;

    a = simdgroup_half8x8(half(tid & 0xFFu) * 0.001h);
    b = simdgroup_half8x8(half((tid >> 8) & 0xFFu) * 0.001h + 0.5h);
    c = simdgroup_half8x8(0.0h);

    for (uint p = 0; p < params.num_passes; p++) {
        simdgroup_multiply_accumulate(c, a, b, c);
    }

    if (simd_id == 0 && (tid & 31u) == 0) {
        simdgroup_store(c, output, 8);
    }
}


// ─── 6. ATOMIC THROUGHPUT ─────────────────────────────────────

/// Atomic throughput sweep: params.mode = number of target locations.
/// 1 target = max contention. 32768 targets = near-zero contention.
/// Measures: atomic ops/sec as f(contention level).
kernel void exploit_atomic_sweep(
    device atomic_uint*  counters [[buffer(0)]],
    constant ExploitParams& params [[buffer(1)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;

    uint num_targets = max(params.mode, 1u);
    uint target = tid % num_targets;

    for (uint p = 0; p < params.num_passes; p++) {
        atomic_fetch_add_explicit(&counters[target], 1u, memory_order_relaxed);
    }
}


// ─── 7. CROSS-TG COMMUNICATION ───────────────────────────────

/// Producer-consumer within a single dispatch.
/// Even TGs = producers (write data + set flag).
/// Odd TGs = consumers (poll flag + read data).
/// Tests: can threadgroups communicate via device memory atomics?
///
/// results[0] = total matched elements (correctness)
/// results[1] = total spin iterations across all consumers (latency)
/// results[2] = number of consumer TGs that completed (coverage)
/// results[3] = number of consumer TGs that timed out (failures)
kernel void exploit_cross_tg(
    device uint*         data    [[buffer(0)]],
    device atomic_uint*  flags   [[buffer(1)]],
    device atomic_uint*  results [[buffer(2)]],
    constant ExploitParams& params [[buffer(3)]],
    uint tgid [[threadgroup_position_in_grid]],
    uint lid  [[thread_position_in_threadgroup]],
    uint tid  [[thread_position_in_grid]])
{
    uint pair_id = tgid / 2;
    uint slot_base = pair_id * 256u;

    if ((tgid & 1u) == 0u) {
        // ─── PRODUCER ───
        // Write data pattern
        if (lid < 256u) {
            data[slot_base + lid] = pair_id * 1000u + lid;
        }
        // Ensure all writes visible before flag
        threadgroup_barrier(mem_flags::mem_device);

        if (lid == 0) {
            atomic_store_explicit(&flags[pair_id], 1u, memory_order_relaxed);
        }
    } else {
        // ─── CONSUMER ───
        // Thread 0 polls the flag
        threadgroup uint shared_spins;
        threadgroup uint shared_flag;
        if (lid == 0) {
            uint spins = 0;
            while (atomic_load_explicit(&flags[pair_id], memory_order_relaxed) == 0u) {
                spins++;
                if (spins > 10000000u) break; // ~10M iterations timeout
            }
            shared_spins = spins;
            shared_flag = atomic_load_explicit(&flags[pair_id], memory_order_relaxed);
        }
        threadgroup_barrier(mem_flags::mem_threadgroup);

        if (lid < 256u) {
            uint val = data[slot_base + lid];
            uint expected = pair_id * 1000u + lid;
            uint match_val = (val == expected) ? 1u : 0u;

            // SIMD reduce match count
            uint simd_lane = tid & 31u;
            uint local_sum = simd_sum(match_val);
            if (simd_lane == 0) {
                atomic_fetch_add_explicit(&results[0], local_sum, memory_order_relaxed);
            }
        }

        if (lid == 0) {
            atomic_fetch_add_explicit(&results[1], shared_spins, memory_order_relaxed);
            if (shared_flag != 0u) {
                atomic_fetch_add_explicit(&results[2], 1u, memory_order_relaxed);
            } else {
                atomic_fetch_add_explicit(&results[3], 1u, memory_order_relaxed);
            }
        }
    }
}
