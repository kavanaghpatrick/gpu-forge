// exploits_v2.metal -- Deeper exploration of Apple Silicon GPU undocumented behaviors.
//
// Experiments:
// 1. FP16 Triple Advantage: >2x from throughput + register + cache compounding
// 2. Occupancy Sweep: same kernel, different TG sizes -- lower occupancy wins
// 3. setBytes LUT: constant address space vs device buffer for lookup tables
// 4. TG Memory Gradient: 0→32KB threadgroup memory, mapping the Dynamic Caching curve
// 5. Encoder Reuse: set-once-dispatch-many vs re-encode-every-dispatch

#include <metal_stdlib>
#include "types.h"
using namespace metal;

// ============================================================================
// Experiment 1: FP16 Triple Advantage
//
// FP16 provides >2x speedup from three compounding effects:
//   (a) 2x ALU throughput (hardware)
//   (b) Lower register dependency penalty (0.56 vs 0.84 cycles)
//   (c) Half-sized values = 2x register cache capacity
//
// FP16→FP32 conversions are FREE on Apple GPU.
// We use enough ALU iterations (32) to make this compute-bound, not BW-bound.
// ============================================================================

kernel void exploit_fp16_compute(
    device const half4*  input  [[buffer(0)]],
    device half4*        output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;

    half4 v = input[tid];

    // 32 multiply-add iterations -- enough to be compute-bound
    for (int i = 0; i < 32; i++) {
        v = v * half4(1.001h) + half4(0.5h);
    }

    output[tid] = v;
}

kernel void exploit_fp32_compute(
    device const float4* input  [[buffer(0)]],
    device float4*       output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;

    float4 v = input[tid];

    for (int i = 0; i < 32; i++) {
        v = v * float4(1.001f) + float4(0.5f);
    }

    output[tid] = v;
}

// ============================================================================
// Experiment 2: Occupancy Sweep
//
// On NVIDIA: more occupancy = better (hides latency).
// On Apple with Dynamic Caching: fewer threads per core = more pool per thread
//   = larger L1 cache = better for bandwidth-bound kernels.
//
// Same kernel dispatched with TG sizes 32, 64, 128, 256, 512, 1024 from Rust.
// ============================================================================

kernel void exploit_occupancy_bw(
    device const float4* input  [[buffer(0)]],
    device float*        output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid  [[thread_position_in_grid]],
    uint stid [[thread_index_in_simdgroup]],
    uint sgid [[simdgroup_index_in_threadgroup]],
    uint tg_id [[threadgroup_position_in_grid]],
    uint tg_size [[threads_per_threadgroup]])
{
    // Each thread loads 4 float4 (16 floats) to create register/L1 pressure
    float acc = 0.0;
    uint base = tid * 4;
    for (uint i = 0; i < 4; i++) {
        uint idx = base + i;
        if (idx < params.element_count) {
            float4 v = input[idx];
            acc += v.x + v.y + v.z + v.w;
        }
    }

    // SIMD reduction (no threadgroup memory)
    acc += simd_shuffle_xor(acc, 1);
    acc += simd_shuffle_xor(acc, 2);
    acc += simd_shuffle_xor(acc, 4);
    acc += simd_shuffle_xor(acc, 8);
    acc += simd_shuffle_xor(acc, 16);

    if (stid == 0) {
        uint simdgroups_per_tg = tg_size / 32;
        uint out_idx = tg_id * simdgroups_per_tg + sgid;
        output[out_idx] = acc;
    }
}

// ============================================================================
// Experiment 3: setBytes LUT -- Constant vs Device Address Space
//
// setBytes pushes up to 4KB into the command stream. The shader compiler
// preloads constant-address-space data into dedicated uniform registers.
// Apple GPU L1 is only 8KB -- setBytes' 4KB limit is exactly half L1.
//
// Compare: 256-entry u32 lookup table via constant space (setBytes)
//     vs   same table in device buffer (setBuffer).
// Random access pattern to stress the cache difference.
// ============================================================================

kernel void exploit_setbytes_lut(
    device const uint*  input  [[buffer(0)]],
    device uint*        output [[buffer(1)]],
    constant uint*      lut    [[buffer(2)]],  // via setBytes: constant address space
    constant ExploitParams& params [[buffer(3)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;

    uint key = input[tid];

    // 8 dependent lookups per element to amplify cache effect
    uint val = lut[key & 0xFF];
    val = lut[(val ^ key) & 0xFF];
    val = lut[(val ^ (key >> 8)) & 0xFF];
    val = lut[(val ^ (key >> 16)) & 0xFF];
    val = lut[(val ^ (key >> 24)) & 0xFF];
    val = lut[(val ^ key) & 0xFF];
    val = lut[(val ^ (key >> 4)) & 0xFF];
    val = lut[(val ^ (key >> 12)) & 0xFF];

    output[tid] = val;
}

kernel void exploit_buffer_lut(
    device const uint*  input  [[buffer(0)]],
    device uint*        output [[buffer(1)]],
    device const uint*  lut    [[buffer(2)]],  // via setBuffer: device address space
    constant ExploitParams& params [[buffer(3)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;

    uint key = input[tid];

    // Same 8 dependent lookups
    uint val = lut[key & 0xFF];
    val = lut[(val ^ key) & 0xFF];
    val = lut[(val ^ (key >> 8)) & 0xFF];
    val = lut[(val ^ (key >> 16)) & 0xFF];
    val = lut[(val ^ (key >> 24)) & 0xFF];
    val = lut[(val ^ key) & 0xFF];
    val = lut[(val ^ (key >> 4)) & 0xFF];
    val = lut[(val ^ (key >> 12)) & 0xFF];

    output[tid] = val;
}

// ============================================================================
// Experiment 4: Threadgroup Memory Gradient
//
// Dynamic Caching ~208KB pool: sweep TG memory from 0 to 32KB.
// Each variant allocates different amount, all do same work.
// Map the performance curve to find the sweet spot.
//
// We use macros to generate variants to avoid code duplication.
// ============================================================================

// Helper: reduce float4 input with N-KB threadgroup memory allocated
// (even if not all used for computation, it's allocated and steals from pool)

#define DEFINE_TG_GRADIENT_KERNEL(NAME, TG_FLOATS)                                    \
kernel void NAME(                                                                      \
    device const float4* input  [[buffer(0)]],                                         \
    device float*        output [[buffer(1)]],                                         \
    constant ExploitParams& params [[buffer(2)]],                                      \
    uint tid  [[thread_position_in_grid]],                                             \
    uint ltid [[thread_position_in_threadgroup]],                                      \
    uint stid [[thread_index_in_simdgroup]],                                           \
    uint sgid [[simdgroup_index_in_threadgroup]],                                      \
    uint tg_id [[threadgroup_position_in_grid]])                                       \
{                                                                                      \
    threadgroup float tg_mem[TG_FLOATS];                                               \
                                                                                       \
    /* Load 4 float4 per thread */                                                     \
    float acc = 0.0;                                                                   \
    uint base = tid * 4;                                                               \
    for (uint i = 0; i < 4; i++) {                                                     \
        uint idx = base + i;                                                           \
        if (idx < params.element_count) {                                              \
            float4 v = input[idx];                                                     \
            acc += v.x + v.y + v.z + v.w;                                              \
        }                                                                              \
    }                                                                                  \
                                                                                       \
    /* Force TG memory usage so compiler doesn't optimize it away */                   \
    tg_mem[ltid] = acc;                                                                \
    threadgroup_barrier(mem_flags::mem_threadgroup);                                    \
    acc = tg_mem[ltid]; /* read back to prevent elimination */                         \
                                                                                       \
    /* SIMD reduction */                                                               \
    acc += simd_shuffle_xor(acc, 1);                                                   \
    acc += simd_shuffle_xor(acc, 2);                                                   \
    acc += simd_shuffle_xor(acc, 4);                                                   \
    acc += simd_shuffle_xor(acc, 8);                                                   \
    acc += simd_shuffle_xor(acc, 16);                                                  \
                                                                                       \
    if (stid == 0) {                                                                   \
        uint out_idx = tg_id * 8 + sgid;                                               \
        output[out_idx] = acc;                                                         \
    }                                                                                  \
}

// Generate 7 variants: 256B, 1KB, 2KB, 4KB, 8KB, 16KB, 32KB
// (TG_FLOATS = KB * 256, since float = 4 bytes, so 256 floats = 1KB)
DEFINE_TG_GRADIENT_KERNEL(exploit_tg_256b_reduce,   64)   // 256 bytes
DEFINE_TG_GRADIENT_KERNEL(exploit_tg_1kb_reduce,    256)  // 1 KB
DEFINE_TG_GRADIENT_KERNEL(exploit_tg_2kb_reduce,    512)  // 2 KB
DEFINE_TG_GRADIENT_KERNEL(exploit_tg_4kb_reduce,    1024) // 4 KB
DEFINE_TG_GRADIENT_KERNEL(exploit_tg_8kb_reduce,    2048) // 8 KB
DEFINE_TG_GRADIENT_KERNEL(exploit_tg_16kb_reduce,   4096) // 16 KB
DEFINE_TG_GRADIENT_KERNEL(exploit_tg_32kb_reduce,   8192) // 32 KB

// ============================================================================
// Experiment 5: Encoder Reuse -- set-once-dispatch-many
//
// Simple bandwidth-bound kernel used to measure encoder overhead.
// Rust side will compare:
//   (A) 1 encoder, set state once, dispatch N times, end once
//   (B) N encoders, each sets state + dispatches + ends
// ============================================================================

kernel void exploit_encoder_work(
    device const float4* input  [[buffer(0)]],
    device float4*       output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;
    output[tid] = input[tid] * 2.0f + 1.0f;
}
