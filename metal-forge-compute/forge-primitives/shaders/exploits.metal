// exploits.metal -- Undocumented Apple Silicon GPU exploit experiments.
//
// These kernels test unintended performance behaviors:
// 1. SIMD shuffle vs threadgroup memory (4x bandwidth difference)
// 2. SLC cache residency across multi-pass compute
// 3. Dynamic Caching pool monopolization (zero-TG vs TG-heavy)
// 4. Indirect conditional dispatch (GPU-side branching)

#include <metal_stdlib>
#include "types.h"
using namespace metal;

// ============================================================================
// Experiment 1: SIMD Shuffle vs Threadgroup Memory Bandwidth
//
// Apple GPU SIMD shuffle: 256 bytes/cycle (undocumented)
// Threadgroup memory:      64 bytes/cycle (documented)
// Expected result: shuffle version ~4x faster for inter-thread data exchange.
//
// Both kernels perform the same logical operation: each thread accumulates
// values from all 32 threads in its simdgroup via a butterfly reduction
// pattern, then writes the per-simdgroup result.
// ============================================================================

/// Accumulate across simdgroup using ONLY simd_shuffle_xor (no threadgroup).
/// This exploits the 256 byte/cycle SIMD shuffle fabric.
kernel void exploit_simd_shuffle_reduce(
    device const float4* input  [[buffer(0)]],
    device float*         output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid  [[thread_position_in_grid]],
    uint stid [[thread_index_in_simdgroup]],
    uint sgid [[simdgroup_index_in_threadgroup]],
    uint tg_id [[threadgroup_position_in_grid]])
{
    uint idx = tid;
    float4 val = (idx < params.element_count) ? input[idx] : float4(0.0);

    // Horizontal sum of float4
    float sum = val.x + val.y + val.z + val.w;

    // Butterfly reduction via simd_shuffle_xor -- ALL communication
    // happens through the SIMD register file, not threadgroup memory.
    // This path uses 256 bytes/cycle shuffle fabric.
    sum += simd_shuffle_xor(sum, 1);
    sum += simd_shuffle_xor(sum, 2);
    sum += simd_shuffle_xor(sum, 4);
    sum += simd_shuffle_xor(sum, 8);
    sum += simd_shuffle_xor(sum, 16);

    // Lane 0 of each simdgroup writes result
    if (stid == 0) {
        uint out_idx = tg_id * 8 + sgid; // 8 simdgroups per TG (256/32)
        if (out_idx < (params.element_count + 31) / 32) {
            output[out_idx] = sum;
        }
    }
}

/// Same reduction but using threadgroup memory for inter-thread exchange.
/// This forces data through the 64 byte/cycle threadgroup memory path.
kernel void exploit_threadgroup_reduce(
    device const float4* input  [[buffer(0)]],
    device float*         output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid  [[thread_position_in_grid]],
    uint ltid [[thread_position_in_threadgroup]],
    uint sgid [[simdgroup_index_in_threadgroup]],
    uint stid [[thread_index_in_simdgroup]],
    uint tg_id [[threadgroup_position_in_grid]])
{
    // Force threadgroup memory usage: shared array for inter-simdgroup exchange
    threadgroup float tg_data[256];

    uint idx = tid;
    float4 val = (idx < params.element_count) ? input[idx] : float4(0.0);
    float sum = val.x + val.y + val.z + val.w;

    // Write to threadgroup memory
    tg_data[ltid] = sum;
    threadgroup_barrier(mem_flags::mem_threadgroup);

    // Tree reduction in threadgroup memory (forces TG memory bandwidth)
    for (uint stride = 128; stride > 0; stride >>= 1) {
        if (ltid < stride) {
            tg_data[ltid] += tg_data[ltid + stride];
        }
        threadgroup_barrier(mem_flags::mem_threadgroup);
    }

    // Thread 0 writes result
    if (ltid == 0) {
        output[tg_id] = tg_data[0];
    }
}

// ============================================================================
// Experiment 2: SLC Cache Residency (GPU-Inclusive Property)
//
// Apple SLC is GPU-inclusive: all GPU data is backed by SLC.
// CPU activity does NOT evict GPU data (GPU-inclusive, CPU-exclusive).
// Multi-pass algorithms on SLC-sized working sets get cache hits on pass 2+.
//
// M4 Pro SLC: ~24 MB, M4 Max: ~48 MB
// Working set: 16 MB (fits SLC) vs 128 MB (exceeds SLC)
//
// We run N passes over the same data. Pass 1 loads from DRAM.
// Pass 2+ should hit SLC for the smaller working set (massive speedup).
// ============================================================================

/// Multi-pass iterative computation -- reads and writes same buffer N times.
/// Each pass: output[i] = input[i] * 1.00001 + 0.5 (lightweight ALU, bandwidth-bound)
kernel void exploit_slc_multipass(
    device float4* data [[buffer(0)]],
    constant ExploitParams& params [[buffer(1)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;

    float4 val = data[tid];
    // Lightweight transform to keep it bandwidth-bound, not ALU-bound
    val = val * 1.00001f + 0.5f;
    data[tid] = val;
}

// ============================================================================
// Experiment 3: Dynamic Caching Pool Monopolization
//
// M3/M4 Dynamic Caching: ~208 KB shared pool per GPU core.
// Pool is shared between: registers, threadgroup memory, tile cache, buffer L1.
// Using ZERO threadgroup memory lets registers + buffer L1 claim the full pool.
//
// Test: same computation, one kernel with threadgroup memory (wastes pool),
// one without (monopolizes pool for registers/L1).
// ============================================================================

/// Zero-threadgroup kernel: all 208KB available for registers + buffer L1.
/// Processes 8 elements per thread to increase register pressure and L1 reuse.
kernel void exploit_zero_tg_reduce(
    device const float4* input  [[buffer(0)]],
    device float*         output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid  [[thread_position_in_grid]],
    uint stid [[thread_index_in_simdgroup]],
    uint sgid [[simdgroup_index_in_threadgroup]],
    uint tg_id [[threadgroup_position_in_grid]])
{
    // Each thread loads 8 float4 values (32 floats) -- high register pressure
    float acc = 0.0;
    uint base = tid * 8;
    for (uint i = 0; i < 8; i++) {
        uint idx = base + i;
        if (idx < params.element_count) {
            float4 v = input[idx];
            acc += v.x + v.y + v.z + v.w;
        }
    }

    // SIMD reduction only -- no threadgroup memory touched
    acc += simd_shuffle_xor(acc, 1);
    acc += simd_shuffle_xor(acc, 2);
    acc += simd_shuffle_xor(acc, 4);
    acc += simd_shuffle_xor(acc, 8);
    acc += simd_shuffle_xor(acc, 16);

    if (stid == 0) {
        uint out_idx = tg_id * 8 + sgid;
        if (out_idx < ((params.element_count + 255) / 256)) {
            output[out_idx] = acc;
        }
    }
}

/// Threadgroup-heavy kernel: deliberately wastes ~16 KB of TG memory per TG,
/// reducing the Dynamic Caching pool available for registers and buffer L1.
kernel void exploit_heavy_tg_reduce(
    device const float4* input  [[buffer(0)]],
    device float*         output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid  [[thread_position_in_grid]],
    uint ltid [[thread_position_in_threadgroup]],
    uint tg_id [[threadgroup_position_in_grid]])
{
    // Allocate 16 KB of threadgroup memory (4096 floats)
    // This forces Dynamic Caching to reserve TG pool, reducing L1/register budget.
    threadgroup float tg_waste[4096];

    // Each thread loads 8 float4 values (same workload as zero-TG version)
    float acc = 0.0;
    uint base = tid * 8;
    for (uint i = 0; i < 8; i++) {
        uint idx = base + i;
        if (idx < params.element_count) {
            float4 v = input[idx];
            acc += v.x + v.y + v.z + v.w;
        }
    }

    // Force TG memory usage (write partial, barrier, read back)
    tg_waste[ltid] = acc;
    threadgroup_barrier(mem_flags::mem_threadgroup);

    // Tree reduction through threadgroup memory
    for (uint stride = 128; stride > 0; stride >>= 1) {
        if (ltid < stride) {
            tg_waste[ltid] += tg_waste[ltid + stride];
        }
        threadgroup_barrier(mem_flags::mem_threadgroup);
    }

    if (ltid == 0) {
        output[tg_id] = tg_waste[0];
    }
}

// ============================================================================
// Experiment 4: Indirect Conditional Dispatch (GPU-Side Branching)
//
// Write (0,0,0) to an indirect dispatch buffer from a prior compute pass
// to skip subsequent dispatches WITHOUT CPU involvement.
// Compare: CPU-side conditional (2x waitUntilCompleted) vs GPU-side (1x wait).
// ============================================================================

/// Decision kernel: writes threadgroup counts to indirect dispatch buffer.
/// If data meets condition, writes (N,1,1). Otherwise writes (0,0,0) to skip.
kernel void exploit_indirect_decision(
    device const uint*  input  [[buffer(0)]],
    device uint*        indirect_args [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    // Only thread 0 makes the decision
    if (tid != 0) return;

    // Check if first element exceeds threshold (mode field used as threshold)
    uint threshold = params.mode;
    uint val = input[0];

    if (val > threshold) {
        // Dispatch the work: ceil(element_count / 256) threadgroups
        indirect_args[0] = (params.element_count + 255) / 256;
        indirect_args[1] = 1;
        indirect_args[2] = 1;
    } else {
        // Skip: zero threadgroups = no work dispatched
        indirect_args[0] = 0;
        indirect_args[1] = 0;
        indirect_args[2] = 0;
    }
}

/// Work kernel dispatched conditionally via indirect buffer.
kernel void exploit_indirect_work(
    device const float4* input  [[buffer(0)]],
    device float4*       output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;
    float4 val = input[tid];
    output[tid] = val * 2.0f + 1.0f;
}
