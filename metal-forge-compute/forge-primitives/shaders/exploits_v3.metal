// exploits_v3.metal -- Wave 3: Practical kernel design patterns.
//
// Experiments:
// 1. Atomic Contention Scaling: How atomic throughput degrades with contention
// 2. Memory Stride Bandwidth: Sequential vs strided access patterns
// 3. Branch Divergence Cost: Uniform vs divergent control flow

#include <metal_stdlib>
#include "types.h"
using namespace metal;

// ============================================================================
// Experiment 1: Atomic Contention Scaling
//
// Measures atomic_fetch_add throughput at different contention levels:
//   (A) 1 target (all threads contend on single location) -- worst case
//   (B) 32 targets (per-simdgroup -- threads within simdgroup contend)
//   (C) 256 targets (per-threadgroup -- no cross-TG contention)
//   (D) N targets (per-thread private accumulate + final atomic) -- best case
//
// mode=0: single target, mode=1: 32 targets, mode=2: 256 targets, mode=3: N targets
// ============================================================================

kernel void exploit_atomic_single(
    device const uint*  input   [[buffer(0)]],
    device atomic_uint* output  [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;

    uint val = input[tid];
    // 4 atomic adds per thread to amplify contention effect
    atomic_fetch_add_explicit(&output[0], val & 0xFF, memory_order_relaxed);
    atomic_fetch_add_explicit(&output[0], (val >> 8) & 0xFF, memory_order_relaxed);
    atomic_fetch_add_explicit(&output[0], (val >> 16) & 0xFF, memory_order_relaxed);
    atomic_fetch_add_explicit(&output[0], (val >> 24) & 0xFF, memory_order_relaxed);
}

kernel void exploit_atomic_per_simdgroup(
    device const uint*  input   [[buffer(0)]],
    device atomic_uint* output  [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid  [[thread_position_in_grid]],
    uint stid [[thread_index_in_simdgroup]])
{
    if (tid >= params.element_count) return;

    uint val = input[tid];
    // Accumulate locally in register, then one atomic per simdgroup lane
    uint local = (val & 0xFF) + ((val >> 8) & 0xFF) + ((val >> 16) & 0xFF) + ((val >> 24) & 0xFF);
    // Reduce within simdgroup first
    local = simd_sum(local);
    if (stid == 0) {
        uint sg_idx = tid / 32;  // one target per simdgroup
        atomic_fetch_add_explicit(&output[sg_idx & 0x1F], local, memory_order_relaxed);
    }
}

kernel void exploit_atomic_per_tg(
    device const uint*  input   [[buffer(0)]],
    device atomic_uint* output  [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid   [[thread_position_in_grid]],
    uint stid  [[thread_index_in_simdgroup]],
    uint sgid  [[simdgroup_index_in_threadgroup]],
    uint tg_id [[threadgroup_position_in_grid]])
{
    if (tid >= params.element_count) return;

    uint val = input[tid];
    uint local = (val & 0xFF) + ((val >> 8) & 0xFF) + ((val >> 16) & 0xFF) + ((val >> 24) & 0xFF);

    // SIMD reduce then one atomic per simdgroup leader
    local = simd_sum(local);
    if (stid == 0) {
        // 256 distinct targets -- no cross-TG contention
        uint target = (tg_id * 8 + sgid) & 0xFF;
        atomic_fetch_add_explicit(&output[target], local, memory_order_relaxed);
    }
}

kernel void exploit_atomic_private(
    device const uint*  input   [[buffer(0)]],
    device atomic_uint* output  [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid   [[thread_position_in_grid]],
    uint stid  [[thread_index_in_simdgroup]],
    uint sgid  [[simdgroup_index_in_threadgroup]],
    uint tg_id [[threadgroup_position_in_grid]])
{
    if (tid >= params.element_count) return;

    // Each thread processes 4 elements privately
    uint local = 0;
    uint base = tid;
    uint val = input[base];
    local += (val & 0xFF) + ((val >> 8) & 0xFF) + ((val >> 16) & 0xFF) + ((val >> 24) & 0xFF);

    // SIMD reduce + one atomic per TG
    local = simd_sum(local);
    if (stid == 0) {
        // Unique target per TG -- zero contention
        uint target = tg_id;
        atomic_fetch_add_explicit(&output[target & 0xFFFF], local, memory_order_relaxed);
    }
}

// ============================================================================
// Experiment 2: Memory Stride Bandwidth
//
// Measures effective bandwidth at different access strides.
// Apple GPU L1 is 8KB, cache line is 128 bytes.
// Sequential access (stride=1) should be optimal.
// Strided access wastes cache lines proportionally.
//
// stride=1: input[tid], stride=2: input[tid*2], stride=4: input[tid*4], etc.
// Each thread loads 1 float4 at its strided position.
// ============================================================================

// Macro to generate stride variants
#define DEFINE_STRIDE_KERNEL(NAME, STRIDE)                                     \
kernel void NAME(                                                              \
    device const float4* input  [[buffer(0)]],                                 \
    device float*        output [[buffer(1)]],                                 \
    constant ExploitParams& params [[buffer(2)]],                              \
    uint tid  [[thread_position_in_grid]],                                     \
    uint stid [[thread_index_in_simdgroup]],                                   \
    uint sgid [[simdgroup_index_in_threadgroup]],                              \
    uint tg_id [[threadgroup_position_in_grid]])                               \
{                                                                              \
    uint idx = tid * STRIDE;                                                   \
    if (idx >= params.element_count) return;                                   \
                                                                               \
    float4 v = input[idx];                                                     \
    float acc = v.x + v.y + v.z + v.w;                                         \
                                                                               \
    /* SIMD reduce to prevent dead code elimination */                         \
    acc = simd_sum(acc);                                                       \
    if (stid == 0) {                                                           \
        uint out_idx = tg_id * 8 + sgid;                                       \
        output[out_idx] = acc;                                                 \
    }                                                                          \
}

DEFINE_STRIDE_KERNEL(exploit_stride_1,  1)
DEFINE_STRIDE_KERNEL(exploit_stride_2,  2)
DEFINE_STRIDE_KERNEL(exploit_stride_4,  4)
DEFINE_STRIDE_KERNEL(exploit_stride_8,  8)
DEFINE_STRIDE_KERNEL(exploit_stride_16, 16)
DEFINE_STRIDE_KERNEL(exploit_stride_32, 32)

// ============================================================================
// Experiment 3: Branch Divergence Cost
//
// Apple GPU uses SIMD execution (32-wide). When threads in a SIMD take
// different branches, both paths execute (thread masking).
//
// Compare:
//   (A) Uniform: all threads take same branch (input all < threshold)
//   (B) Alternating: even threads one branch, odd threads another (50% diverge)
//   (C) Random: random branch per thread (worst case)
//   (D) Full diverge: each lane takes a different code path depth
//
// All variants do same total work per thread to isolate branching cost.
// ============================================================================

kernel void exploit_branch_uniform(
    device const float4* input  [[buffer(0)]],
    device float4*       output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;

    float4 v = input[tid];

    // All threads take path A (threshold = 2.0, values are 0.0-0.99)
    if (v.x < 2.0f) {
        // Path A: 8 multiply-adds
        for (int i = 0; i < 8; i++) {
            v = v * 1.001f + 0.5f;
        }
    } else {
        // Path B: 8 different multiply-adds (same work)
        for (int i = 0; i < 8; i++) {
            v = v * 0.999f + 0.5f;
        }
    }

    output[tid] = v;
}

kernel void exploit_branch_alternating(
    device const float4* input  [[buffer(0)]],
    device float4*       output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;

    float4 v = input[tid];

    // Even threads take A, odd threads take B -- 50% divergence within each SIMD
    if ((tid & 1) == 0) {
        for (int i = 0; i < 8; i++) {
            v = v * 1.001f + 0.5f;
        }
    } else {
        for (int i = 0; i < 8; i++) {
            v = v * 0.999f + 0.5f;
        }
    }

    output[tid] = v;
}

kernel void exploit_branch_random(
    device const uint*   keys   [[buffer(0)]],
    device float4*       output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;

    // Random branch based on input data -- worst case divergence
    uint key = keys[tid];
    float4 v = float4(float(key & 0xFF) * 0.01f);

    if (key & 1) {
        for (int i = 0; i < 8; i++) {
            v = v * 1.001f + 0.5f;
        }
    } else {
        for (int i = 0; i < 8; i++) {
            v = v * 0.999f + 0.5f;
        }
    }

    output[tid] = v;
}

kernel void exploit_branch_deep_diverge(
    device const uint*   keys   [[buffer(0)]],
    device float4*       output [[buffer(1)]],
    constant ExploitParams& params [[buffer(2)]],
    uint tid [[thread_position_in_grid]])
{
    if (tid >= params.element_count) return;

    // Each SIMD lane takes a different depth of iteration
    // Lane 0: 1 iteration, lane 1: 2, ..., lane 31: 32
    // This maximizes divergence within each SIMD
    uint key = keys[tid];
    float4 v = float4(float(key & 0xFF) * 0.01f);

    uint lane = tid & 31;
    uint iters = lane + 1;  // 1-32 iterations per lane

    for (uint i = 0; i < iters; i++) {
        v = v * 1.001f + 0.5f;
    }

    // Ensure mean work matches other variants: avg = 16.5 iterations
    // Add constant work to normalize total: 32 - iters extra iterations
    // Actually keep it unequal to measure the divergence penalty directly
    output[tid] = v;
}
