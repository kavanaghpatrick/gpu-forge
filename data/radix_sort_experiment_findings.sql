-- Radix Sort Experiment Findings (exp7-exp26)
-- Generalized Apple Silicon GPU compute principles from 20 experiments
-- All single-line INSERTs for sqlite3 compatibility

-- UNIFIED MEMORY (skill_id=2): SLC Behavior and Working Set Model

INSERT INTO findings (skill_id, topic, claim, evidence, source_url, source_title, source_type, confidence, tags, notes, gpu_generation, temporal_status) VALUES (2, 'slc bandwidth model', 'Apple Silicon SLC bandwidth follows a step function: 469 GB/s for working sets <=24MB, ~300 GB/s at 32MB, and 245 GB/s (DRAM speed) at >=48MB on M4 Pro. The transition zone is 24-48MB with approximately linear interpolation: B(ws) = 469 - (469-245) * clamp((ws - 24) / 24, 0, 1) GB/s.', 'Measured across 9 exploit experiments and 20 radix sort experiments. SLC cliff consistently appears at 24MB regardless of access pattern. At 4M elements (16MB working set), radix sort achieves 7662 Mk/s vs 5431 at 16M (64MB) -- a 1.41x ratio matching the bandwidth ratio discounted by fixed overhead.', 'metal-gpu-experiments/src/experiments/', 'Radix sort experiments exp7-exp26', 'empirical_test', 'verified', 'slc,bandwidth,working-set,memory-hierarchy,m4-pro,cache-cliff', 'Mathematical model: B_eff(ws_MB) = 469 - 224 * clamp((ws_MB - 24) / 24, 0, 1) GB/s. This predicts throughput for any bandwidth-limited kernel given its working set size.', 'm4', 'current');

INSERT INTO findings (skill_id, topic, claim, evidence, source_url, source_title, source_type, confidence, tags, notes, gpu_generation, temporal_status) VALUES (2, 'natural slc residency', 'GPU kernels achieve SLC-speed bandwidth automatically when concurrent_TGs * working_set_per_TG <= SLC_capacity. The GPU scheduler limits concurrent threadgroups to ~80 on M4 Pro (10 cores * ~8 TGs/core), so any kernel where each TG touches <=300KB of data achieves SLC residency without explicit batching.', 'MSD radix sort inner passes: 256 TGs each processing a 250KB bucket. Only ~80 run concurrently -> 80 * 250KB = 20MB < 24MB SLC. Measured: fused_v4 inner sort achieves ~200+ GB/s effective bandwidth at 16M elements. Explicit SLC batching (Investigation W+) provided ZERO benefit -- the GPU natural scheduling already achieves optimal cache residency.', 'metal-gpu-experiments/src/exp17_hybrid.rs', 'Radix sort Investigation W vs W+', 'empirical_test', 'verified', 'slc,occupancy,scheduling,threadgroup,cache-residency,natural-batching', 'Formula: SLC_residency iff min(dispatched_TGs, max_concurrent_TGs) * ws_per_TG <= SLC_capacity. For M4 Pro: max_concurrent_TGs ~ 80, SLC_capacity = 24MB, so ws_per_TG <= 300KB guarantees SLC speed. Corollary: explicit batching is unnecessary when per-TG working set is small.', 'm4', 'current');

INSERT INTO findings (skill_id, topic, claim, evidence, source_url, source_title, source_type, confidence, tags, notes, gpu_generation, temporal_status) VALUES (2, 'scatter bandwidth penalty', 'Apple Silicon hardware write-combining reduces the 256-bin radix scatter penalty to ~2.2x versus sequential writes. Effective scatter write bandwidth is ~100-110 GB/s versus ~245 GB/s sequential. For truly random access patterns the penalty is 6-8x. Software TG-memory-based scatter reordering provides ZERO benefit because the hardware already performs equivalent coalescing.', 'Measured in exp16 per-pass breakdown: full pass 1.18ms (108 GB/s R+W combined), scatter-only ~0.85ms (73 GB/s write). Sequential read bandwidth 199 GB/s. TG memory reorder experiments (exp16 v2, v4): identical or slower throughput vs direct scatter.', 'metal-gpu-experiments/src/exp16_8bit.rs', 'Radix sort scatter bandwidth analysis', 'empirical_test', 'verified', 'scatter,write-combining,bandwidth,radix-sort,coalescing', 'Model: B_scatter(bins) ~ B_sequential / (1 + log2(bins)/8). For 256 bins: 245/2.2 ~ 110 GB/s. Critical insight: NEVER implement TG memory reorder on Apple Silicon -- it adds overhead with zero benefit.', 'm4', 'current');

-- GPU-PERF (skill_id=6): Throughput Models and Optimization Hierarchy

INSERT INTO findings (skill_id, topic, claim, evidence, source_url, source_title, source_type, confidence, tags, notes, gpu_generation, temporal_status) VALUES (6, 'radix sort throughput model', 'Radix sort throughput on Apple Silicon follows: T = N / (sum_pass_time_i + dispatch_overhead). Each pass reads and writes N elements: pass_time = N * elem_size * 2 / B_eff(ws). For MSD+3-inner: T = N / (N*8/B_scatter + N*24/B_inner + 4*D) where B_scatter depends on access pattern, B_inner on per-bucket working set vs SLC, and D is per-dispatch overhead (~1us within encoder, ~100us per command buffer).', 'Validated across 6 sizes (1M-32M) and 20+ experiments. Predictions match measured throughput within 10%. Key: 16M @ 5431 Mk/s (predicted: 5200-5600), 4M @ 7662 Mk/s (predicted: 7200-8000, SLC inner), 32M @ 4752 Mk/s (predicted: 4500-5000, DRAM inner). Model correctly predicts the crossover at ~20M where inner passes transition from SLC to DRAM bandwidth.', 'metal-gpu-experiments/src/exp17_hybrid.rs', 'Radix sort experiments exp7-exp26', 'empirical_test', 'verified', 'radix-sort,throughput-model,bandwidth,performance-prediction', 'Full model: T(N) = N / (T_msd(N) + T_inner(N) + T_overhead). T_msd = N*4/B_read + N*4/B_scatter_write. T_inner = 3 * N*4*2 / B_eff(N*4*2/256). T_overhead = num_dispatches * 1us. Enables predicting performance for any input size without benchmarking.', 'universal', 'current');

INSERT INTO findings (skill_id, topic, claim, evidence, source_url, source_title, source_type, confidence, tags, notes, gpu_generation, temporal_status) VALUES (6, 'dispatch overhead model', 'Metal compute dispatch overhead on Apple Silicon: ~1us per dispatchThreadgroups within a single encoder (implicit barrier), ~0.8us per encoder creation/end, ~97us per command buffer commit+waitUntilCompleted. Reducing dispatches from 7 to 4 improved 16M radix sort from 4176 to 5431 Mk/s (30%). At small sizes (<=1M), dispatch overhead dominates: 4 dispatches * ~100us = 0.4ms of a 0.59ms total.', 'Measured in exp12 (encoder reuse experiment) and radix sort progression: Investigation S (7 dispatches) = 4176 Mk/s, Investigation T (5) = 4969 Mk/s, Investigation W (4) = 5431 Mk/s. The per-dispatch overhead explains the 1M anomaly: 1688 Mk/s with 68% of time in dispatch overhead.', 'metal-gpu-experiments/src/exp17_hybrid.rs', 'Radix sort dispatch optimization', 'empirical_test', 'verified', 'dispatch-overhead,command-buffer,encoder,latency,performance', 'Rule of thumb: if per-dispatch data < 4MB, dispatch overhead exceeds 1% of total time. For latency-sensitive paths, minimize dispatches aggressively. Fusing kernel stages into one dispatch pays off when data is small.', 'm4', 'current');

INSERT INTO findings (skill_id, topic, claim, evidence, source_url, source_title, source_type, confidence, tags, notes, gpu_generation, temporal_status) VALUES (6, 'optimization priority hierarchy', 'For bandwidth-limited GPU kernels on Apple Silicon, optimization impact follows this strict hierarchy: (1) Reduce data movement passes 2x-3x impact, (2) Eliminate dispatches 5-30% per dispatch removed, (3) Algorithmic changes like atomic scatter vs decoupled lookback 10-20%, (4) SLC residency for working sets near 24MB boundary up to 1.9x, (5) Micro-optimizations like SIMD shuffles and TG memory tricks <7% each. Spending time on items 4-5 before exhausting items 1-3 is always wrong.', 'Radix sort progression: flat 4-pass (3009 Mk/s) -> MSD+3-inner (3456) -> fused inner (4176) -> atomic MSD (4969) -> fused_v4 (5431). Meanwhile: TG memory reorder = 0%, SIMD shuffle optimization = 0-7%, occupancy sweep = 3-5%, setBytes vs buffer = 0%, dynamic cache tuning = <1%.', 'metal-gpu-experiments/', 'Experiments exp7-exp26 and exploit experiments', 'empirical_test', 'verified', 'optimization,priority,bandwidth-limited,performance-hierarchy', 'Meta-finding: Apple Silicon is so well-optimized at the hardware level that NVIDIA-era micro-optimization tricks provide negligible benefit. Focus on algorithmic data movement reduction.', 'universal', 'current');

INSERT INTO findings (skill_id, topic, claim, evidence, source_url, source_title, source_type, confidence, tags, notes, gpu_generation, temporal_status) VALUES (6, 'radix sort throughput ceilings', 'Measured radix sort throughput ceilings on M4 Pro at 16M uint32: flat 4-pass 8-bit = 3009 Mk/s (scatter-limited), MSD+3-inner fused_v4 = 5431 Mk/s (SLC-assisted), theoretical sequential R+W limit ~ 30000 Mk/s (245 GB/s / 8B per element per pass). Size-dependent peaks: 4M = 7662 Mk/s (full SLC), 8M = 6315 Mk/s, 16M = 5431 Mk/s, 32M = 4752 Mk/s.', 'Comprehensive benchmarks across exp16 (flat 4-pass) and exp17 (MSD+inner). The 3009->5431 improvement (1.80x) came from: eliminating 1 scatter pass via MSD, SLC-speed inner passes from natural residency, and fewer dispatches.', 'metal-gpu-experiments/', 'Radix sort exp16 and exp17', 'empirical_test', 'verified', 'radix-sort,throughput,ceiling,m4-pro,benchmark', 'Ceiling prediction: T_max(N) = N / (N*8/B_scatter + 3*N*8/B_inner(N) + 4us). At 16M: B_scatter~110, B_inner~350 GB/s -> T_max ~ 5800. Measured 5431 = 94% of predicted ceiling.', 'm4', 'current');

INSERT INTO findings (skill_id, topic, claim, evidence, source_url, source_title, source_type, confidence, tags, notes, gpu_generation, temporal_status) VALUES (6, 'atomic scatter vs decoupled lookback', 'Atomic scatter (atomic_fetch_add for global positioning) outperforms decoupled lookback by 15-20% for MSD radix sort on Apple Silicon. Eliminates: tile_status buffer (4MB at 16M), zero_status dispatch, lookback spin-wait, and 2x device-scope fences. Atomic contention on 256 counters is negligible because inter-TG atomics on disjoint counters have no contention (each on different cache line).', 'Investigation T (atomic MSD): 4969 Mk/s vs Investigation S (lookback MSD): 4176 Mk/s. Atomic approach: initialize counters[d] = exclusive_prefix[d], then atomic_fetch_add returns exact global scatter position.', 'metal-gpu-experiments/src/exp17_hybrid.rs', 'Investigation T vs S', 'empirical_test', 'verified', 'atomic,scatter,decoupled-lookback,radix-sort,optimization', 'When to prefer atomic: bin count small (<=256) and tiles independent. When to prefer lookback: streaming output needing global prefix (e.g., stream compaction).', 'm4', 'current');

INSERT INTO findings (skill_id, topic, claim, evidence, source_url, source_title, source_type, confidence, tags, notes, gpu_generation, temporal_status) VALUES (6, 'size-dependent performance regimes', 'GPU compute on Apple Silicon exhibits three performance regimes: (1) Overhead-dominated N<2M where dispatch overhead >10% of total time, (2) SLC-sweet-spot 2M<N<~20M where working set fits in SLC achieving peak throughput, (3) DRAM-limited N>~20M where working set exceeds SLC. For MSD+inner sort: 1M=1688 Mk/s (overhead), 4M=7662 (SLC peak), 16M=5431 (partial DRAM), 32M=4752 (full DRAM).', 'Investigation W size sweep: 1M, 2M, 4M, 8M, 16M, 32M. The 4M sweet spot (16MB) matches SLC capacity on M4 Pro. The SLC boundary depends on per-bucket size: bucket=N/256, SLC-resident when 80 concurrent buckets * bucket_size < 24MB.', 'metal-gpu-experiments/src/exp17_hybrid.rs', 'Investigation W size sweep', 'empirical_test', 'verified', 'performance-regimes,size-scaling,slc,overhead,bandwidth', 'Design implication: for latency-sensitive apps, keep working sets in 2M-8M element range. For throughput-sensitive batch processing, larger sizes amortize dispatch overhead but sacrifice per-element bandwidth.', 'm4', 'current');

INSERT INTO findings (skill_id, topic, claim, evidence, source_url, source_title, source_type, confidence, tags, notes, gpu_generation, temporal_status) VALUES (6, 'experiment progression methodology', 'Systematic GPU kernel optimization follows: (1) Build correct baseline, (2) Profile per-phase timing to identify dominant cost, (3) Reduce passes via algorithmic change, (4) Reduce dispatches via kernel fusion, (5) Reduce synchronization overhead, (6) Verify across sizes. Dead-end experiments are valuable: exp18-26 explored WLMS, presort, local sort, SLC sort, batched, fence-free, 3-pass -- all slower than MSD+fused inner, definitively closing alternative paths.', '20+ experiments building on previous findings. Key progression: exp7 (first sort) -> exp10 (SIMD rank 2.83x) -> exp12 (coherence solved) -> exp15 (onesweep, compiler bug found) -> exp16 (8-bit 3009 Mk/s) -> exp17 (hybrid 5431 Mk/s).', 'metal-gpu-experiments/', 'Radix sort experiment series', 'empirical_test', 'verified', 'methodology,optimization-progression,benchmarking,systematic', 'Key lesson: try the simplest approach first (atomic scatter) before complex ones (decoupled lookback, WLMS). Investment in dead ends was not wasted -- they confirmed MSD+fused inner is optimal.', 'universal', 'current');

-- METAL-COMPUTE (skill_id=3): Pipeline Architecture Patterns

INSERT INTO findings (skill_id, topic, claim, evidence, source_url, source_title, source_type, confidence, tags, notes, gpu_generation, temporal_status) VALUES (3, 'single encoder pso switching', 'Single compute command encoder with PSO switching is the optimal Metal pipeline pattern for multi-kernel workflows. Sequential dispatchThreadgroups() calls within one encoder have implicit memory barriers. Switching PSO via setComputePipelineState() costs ~1us. This eliminates encoder creation overhead (~0.8us each) and command buffer boundaries (~97us each).', 'Radix sort optimized from 4 encoders (exp8) to 1 encoder (exp14+). Investigation W uses 4 dispatches in 1 encoder: histogram->prep->scatter->fused_inner, all with implicit barriers. The 7->4 dispatch reduction was worth 1255 Mk/s (4176->5431).', 'metal-gpu-experiments/src/exp17_hybrid.rs', 'Metal pipeline optimization', 'empirical_test', 'verified', 'command-encoder,pso,pipeline,dispatch,single-encoder,optimization', 'Pattern: enc = cmd.computeCommandEncoder(); enc.setPSO(a); enc.dispatch(); enc.setPSO(b); enc.dispatch(); enc.endEncoding(); -- implicit barriers between dispatches, no explicit sync needed.', 'universal', 'current');

INSERT INTO findings (skill_id, topic, claim, evidence, source_url, source_title, source_type, confidence, tags, notes, gpu_generation, temporal_status) VALUES (3, 'fused multi-pass kernel pattern', 'Fusing multiple compute passes into a single dispatch with threadgroup_barrier(mem_flags::mem_device) between passes exploits L2/SLC temporal locality. Data written by pass N remains cache-resident when read by pass N+1 in the same dispatch. This improved radix sort inner passes from 3 separate dispatches (124 GB/s effective) to 1 fused dispatch (~220 GB/s effective).', 'Investigation S: fused 3-pass inner sort in single kernel. Each TG processes one bucket: pass0 (bits 0:7) -> barrier(mem_device) -> pass1 (bits 8:15) -> barrier(mem_device) -> pass2 (bits 16:23). Same device memory reused across passes hits L2/SLC instead of DRAM.', 'metal-gpu-experiments/shaders/exp17_hybrid.metal', 'Fused inner sort kernel', 'empirical_test', 'verified', 'fused-kernel,multi-pass,cache-locality,threadgroup-barrier,l2-cache', 'Applicability: any pipeline where same data is read->processed->written multiple times with different parameters. The barrier ensures writes are globally visible before next pass reads them.', 'universal', 'current');

-- MSL-KERNELS (skill_id=4): Correctness and Compiler Issues

INSERT INTO findings (skill_id, topic, claim, evidence, source_url, source_title, source_type, confidence, tags, notes, gpu_generation, temporal_status) VALUES (4, 'cross-tg coherence mechanisms', 'Cross-threadgroup memory coherence on Metal requires atomic_thread_fence(mem_flags::mem_device, memory_order_seq_cst, thread_scope_device). This is the ONLY reliable mechanism. Alternatives that DO NOT work: volatile device pointers (exp9), coherent(device) attribute alone (exp12 D), threadgroup_barrier(mem_device) which is intra-TG only, memory_order_relaxed fences which are no-ops. Multi-encoder barriers work but add ~0.8us overhead.', 'Systematic benchmark in exp12: A (multi-encoder)=correct+2.0ms, B (coherent+fence)=correct+2.3ms, C (fence only)=correct+1.5-2.8ms, D (coherent only)=FAILS 100%. Fence alone (C) is often fastest because coherent adds ~15% overhead with no correctness benefit when fence is present.', 'metal-gpu-experiments/src/exp12_coherence.rs', 'Cross-TG coherence benchmark', 'empirical_test', 'verified', 'cross-threadgroup,coherence,fence,atomic,memory-ordering,metal3.2', 'Requires -std=metal3.2 for thread_scope_device. Pattern: write -> atomic_store(flag,1,relaxed) -> fence(mem_device,seq_cst,device) on producer. fence(mem_device,seq_cst,device) -> atomic_load(flag,relaxed) -> read on consumer.', 'universal', 'current');

INSERT INTO findings (skill_id, topic, claim, evidence, source_url, source_title, source_type, confidence, tags, notes, gpu_generation, temporal_status) VALUES (4, 'metal compiler simd_shuffle_xor bug', 'CRITICAL Metal compiler bug: combining simd_shuffle_xor operations with threadgroup memory read/write operations in the same loop causes a 4000x slowdown on M4 Pro. Ballot-only loop: 1.4ms. Scatter-only loop: 1.3ms. Combined loop: 5400ms. NOT register pressure and NOT missing barriers. Workaround: use per-simdgroup atomic_fetch_add on TG memory for ranking instead of SIMD shuffle-based ranking.', 'Discovered in exp15 onesweep radix sort. Fused histogram+scatter kernel combining simd_shuffle_xor for ballot ranking with TG memory scatter writes. Each operation alone runs at full speed; combining them triggers the bug.', 'metal-gpu-experiments/src/exp15_onesweep.rs', 'Onesweep compiler bug', 'empirical_test', 'verified', 'compiler-bug,simd-shuffle-xor,threadgroup-memory,performance-cliff,workaround', 'Diagnostic: if kernel with SIMD shuffles + TG memory ops is 1000x+ slower than expected, split operations into separate loops or replace SIMD shuffles with TG memory atomics.', 'm4', 'current');

INSERT INTO findings (skill_id, topic, claim, evidence, source_url, source_title, source_type, confidence, tags, notes, gpu_generation, temporal_status) VALUES (4, 'simd_shuffle non-uniform lane index', 'simd_shuffle(value, non_uniform_lane_index) produces INCORRECT results on M4 Pro when the lane index varies per thread. The MSL specification does not guarantee correct behavior for non-uniform lane indices. Workaround: write values to threadgroup memory array and index into it. This achieves identical performance with correct results.', 'Discovered in exp10 SIMD rank optimization. simd_shuffle(digit, j) where j varies per thread produced silently incorrect sort output. Replacing with shared_digits[j] (TG memory read) fixed correctness with identical throughput (792 Mk/s).', 'metal-gpu-experiments/src/exp10_simd_rank.rs', 'SIMD rank optimization', 'empirical_test', 'verified', 'simd-shuffle,non-uniform,correctness-bug,threadgroup-memory,workaround', 'Rule: ONLY use simd_shuffle with uniform (same across all lanes) or compile-time-constant lane indices. For non-uniform access, use threadgroup memory.', 'm4', 'current');

-- GPU-SILICON (skill_id=1): Hardware Behavior

INSERT INTO findings (skill_id, topic, claim, evidence, source_url, source_title, source_type, confidence, tags, notes, gpu_generation, temporal_status) VALUES (1, 'write combining hardware', 'Apple Silicon GPU includes hardware write-combining that automatically coalesces scattered device memory writes, making software-based scatter reordering through threadgroup memory unnecessary. For 256-bin radix scatter, hardware achieves ~110 GB/s write bandwidth regardless of whether software TG-memory reorder is applied. Fundamental difference from NVIDIA GPUs where software coalescing through shared memory is essential.', 'Exp16: v1 direct scatter = 3009 Mk/s, v2 2048-tile+TG reorder = 2350 Mk/s SLOWER, v4 4096-tile+two-half reorder = 2896 Mk/s same. Write-combining buffers aggregate writes to same cache line before issuing memory transaction.', 'metal-gpu-experiments/src/exp16_8bit.rs', 'TG memory reorder experiments', 'empirical_test', 'verified', 'write-combining,scatter,coalescing,nvidia-comparison,hardware', 'This invalidates NVIDIA-focused GPU optimization literature. On Apple Silicon: NEVER add TG memory scatter reorder stages. Direct scatter to device memory is optimal.', 'm4', 'current');

INSERT INTO findings (skill_id, topic, claim, evidence, source_url, source_title, source_type, confidence, tags, notes, gpu_generation, temporal_status) VALUES (1, 'micro-optimization ineffectiveness', 'Apple Silicon GPU hardware is so well-optimized that traditional GPU micro-optimizations provide negligible benefit: occupancy tuning = 3-5% variation across TG 32-1024, setBytes vs device buffer = 0%, dynamic TG memory pool = <1%, SIMD shuffle vs TG memory = <=7% at 1M and 0% at 10M, encoder reuse = 0% overhead, TG memory allocation cost = 0 up to 32KB.', 'Systematically measured in 9 exploit experiments (forge-bench): simd_vs_tg, slc_residency, dynamic_cache, indirect_cond, fp16_advantage, occupancy_sweep, setbytes_lut, tg_gradient, encoder_reuse. Only FP16 vs FP32 showed significant difference (2.05x from 2x ALU throughput).', 'metal-gpu-experiments/src/experiments/', 'GPU exploit experiments', 'empirical_test', 'verified', 'micro-optimization,occupancy,setbytes,dynamic-cache,simd,encoder,tg-memory', 'Meta-principle: on Apple Silicon hardware handles what NVIDIA requires software to manage. Focus on algorithmic data movement reduction. Exception: FP16 provides genuine 2x ALU throughput.', 'm4', 'current');

-- SIMD-WAVE (skill_id=7): SIMD Optimization Patterns

INSERT INTO findings (skill_id, topic, claim, evidence, source_url, source_title, source_type, confidence, tags, notes, gpu_generation, temporal_status) VALUES (7, 'simd rank optimization for radix sort', 'SIMD-based rank computation provides 2.83x speedup over serial O(255) ranking in radix sort. Two-phase approach: (1) within-simdgroup scan of shared_digits[] in O(31) via sequential read, (2) cross-simdgroup histogram in O(num_simdgroups-1) via TG memory atomic increment. Replaces naive per-thread loop counting elements with same or lower digit value.', 'Exp10: serial ranking was 65% of total runtime. SIMD rank reduced per-pass time by 2.83x, achieving 792 Mk/s at 1M elements. rank(thread_i) = count_same_digit_in_lanes_before_i + count_lower_digit_across_all_lanes.', 'metal-gpu-experiments/src/exp10_simd_rank.rs', 'SIMD rank optimization', 'empirical_test', 'verified', 'simd,rank,radix-sort,prefix-sum,histogram,optimization', 'Algorithm: for thread with digit d: sg_rank = count in same SG with (digit<d) OR (digit==d AND lane<my_lane). Cross-SG offset = sum sg_histograms[0..my_sg-1][d]. O(32+num_SGs) per thread vs O(tile_size) serial.', 'universal', 'current');

-- GPU-CENTRIC-ARCH (skill_id=11): Architecture Patterns

INSERT INTO findings (skill_id, topic, claim, evidence, source_url, source_title, source_type, confidence, tags, notes, gpu_generation, temporal_status) VALUES (11, 'persistent vs non-persistent dispatch', 'Non-persistent dispatch (1 TG per tile, N/tile_size total TGs) outperforms persistent dispatch (fixed 64 TGs with atomic work-stealing) by 12% for radix sort on Apple Silicon. GPU hardware scheduler is more efficient at distributing tiles than software work-stealing via atomics.', 'Exp15: non-persistent (1 TG/tile, 8192 TGs at 16M) = 1910 Mk/s vs persistent (64 TGs + atomic counter) = 1700 Mk/s. Consistent 12% gap across sizes.', 'metal-gpu-experiments/src/exp15_onesweep.rs', 'Onesweep persistent vs non-persistent', 'empirical_test', 'verified', 'persistent-kernel,work-stealing,dispatch,scheduling,atomics', 'Exception: persistent kernels needed when cross-TG communication required during execution (decoupled lookback). For independent tiles, prefer non-persistent and let GPU scheduler handle it.', 'm4', 'current');

INSERT INTO findings (skill_id, topic, claim, evidence, source_url, source_title, source_type, confidence, tags, notes, gpu_generation, temporal_status) VALUES (11, 'msd bucket sort architecture', 'MSD (Most Significant Digit) bucket sort followed by fused per-bucket LSD passes is the optimal radix sort architecture for Apple Silicon when N > 2M. MSD scatters N elements into 256 buckets of ~N/256 each. Each bucket is independently sorted by a fused 3-pass LSD kernel in a single TG. Achieves natural SLC residency, reduces effective pass count, enables maximum parallelism with 256 independent TGs.', 'Progression from flat 4-pass (3009 Mk/s) to MSD+3-inner (5431 Mk/s) = 1.80x. The architecture exploits natural SLC residency: 256 buckets * 250KB/bucket = 64MB total but only ~80 TGs concurrent -> 80*250KB = 20MB < 24MB SLC.', 'metal-gpu-experiments/src/exp17_hybrid.rs', 'MSD+fused inner radix sort', 'empirical_test', 'verified', 'msd-sort,bucket-sort,architecture,radix-sort,slc-residency', 'Architecture: histogram(N)->prefix_sum(256)->atomic_scatter(N->256 buckets)->fused_v4(256 TGs each sorts one bucket with 3 LSD passes). Total: 4 dispatches, 1 encoder, 1 command buffer.', 'universal', 'current');
